{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This project will take your through design and hosting AI applications on Nutanix.</p> <p>Intended to be used by any Nutanix user to setup and run LLMs, LLM with RAG, Support GPT and other AI applications. </p> <p>This site follows Design &gt; Deploy model.</p>"},{"location":"#design-infra-k8s-and-ai-apps","title":"Design Infra, K8S and AI Apps","text":"<ul> <li> <p> AI Apps Design on Nutanix</p> <p>Start here to read about gathering requirements and considering general design considerations.</p> <p> Start designing</p> <p> Start sizing  </p> </li> </ul>"},{"location":"#deploy-infra","title":"Deploy Infra","text":"<ul> <li> <p> Set up Nutanix Cluster in an hour</p> <p>You can setup a Nutanix cluster in under an hour using Foundation to prepare your infrastructure.</p> <p> Setup a Nutanix cluster</p> <p> Deploy Prism Central</p> </li> </ul>"},{"location":"#deploy-k8s","title":"Deploy K8S","text":"<ul> <li> <p> Set up Nutanix Karbon Engine [ NKE ] in 10 minutes</p> <p>Setup a NKE K8S cluster in under an hour using Foundation to prepare your infrastructure.</p> <p> Setup Nutanix Karbon Engine</p> </li> <li> <p> Set up Nutanix Kubernetes Platform [ NKP ] in 30 minutes</p> <p>Setup a Nutanix NKP K8S cluster to deploy your workloads.</p> <p> Setup Nutanix Kubernetes Platform</p> </li> </ul> <ul> <li> <p> Set up NKP Advanced</p> <p>Customize what NKP components to deploy</p> <p> Setup Customized NKP </p> </li> <li> <p> Setup NKP Air-Gapped</p> <p>Setup NKP on air-gapped cluster</p> <p> Air-gapped Install</p> </li> </ul>"},{"location":"#deploy-ai-apps","title":"Deploy AI Apps","text":"<ul> <li> <p> Set up Nutanix Enterprise AI (NAI) in 60 minutes</p> <p>Install and host an LLM on Nutanix Enterprise AI (NAI) platform.</p> <p> Start</p> </li> <li> <p> Set up Air-gapped Nutanix Enterprise AI (NAI)</p> <p>Install and host an LLM on Nutanix Enterprise AI (NAI) platform.</p> <p> Start</p> </li> </ul> <ul> <li> <p> Set up LLM with RAG on Nutanix in 10 minutes - soon to be deprecated</p> <p>Install and host a LLM on Nutanix platform using this framework in this repo. Get up and running in minutes.</p> <p>This site will also take you through implementation using NKE clusters and GPT-in-a-Box v1 NVD Reference App.</p> <p> Start</p> </li> </ul>"},{"location":"#others","title":"Others","text":"<ul> <li> <p> It's just Markdown</p> <p>Everything is this site is written in .md file. When you see something in this repo, say something by submitting a PR. </p> <p>We review PR on a day-to-day basis.</p> <p> Contributing</p> </li> </ul> <p> Nutanix components used in this project</p> <p> Open Source Tools used in this projects</p> <p> Appendix</p> <p> Baremetal servers ... huh?</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are always welcome. Before contributing please search the issue tracker. Your issue may have already been discussed or fixed in <code>main</code>. To contribute, fork this repository, commit your changes, and send a Pull Request.</p>"},{"location":"contributing/#pull-requests","title":"Pull requests","text":""},{"location":"contributing/#approval-and-release-process","title":"Approval and release process","text":"<p>Pull requests approvals go through the following steps:</p> <ol> <li>A GitHub action may be triggered to lint and test.</li> <li>A maintainer reviews the changes. Any change requires at least one review.</li> <li>The pull request can be merged when at least one maintainer approves it.</li> </ol>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#nutanix-enterprise-ai","title":"Nutanix Enterprise AI","text":"# Topic ETA 1 Airgap Installation - NAI on NKP N/A 2 NAI on EKS N/A 3 NAI on GKE N/A 4 NAI with DataRobot integration N/A 5 NVIDIA NIM support N/A 6 Manual imports of custom models for NAI N/A 7 Performance testing N/A 8 NAI on OCP N/A"},{"location":"airgap_nai/","title":"Getting Started","text":"<p>In this part of the lab we will deploy LLM on GPU nodes.</p> <p>We will also deploy a Kubernetes cluster so far as per the NVD design requirements.</p> <p>Darksite NKP cluster: to host the dev LLM and ChatBot application - this will use GPU passed through to the kubernetes worker node.</p> <p>Deploy the kubernetes cluster with the following components:</p> <ul> <li>3 x Control plane nodes</li> <li>4 x Worker nodes </li> <li>1 x GPU node (with a minimum of 40GB of RAM and 16 vCPUs based on <code>llama3-8B</code> LLM model)</li> </ul> <p>We will deploy the GPT-in-a-Box v2 NVD Reference App - backed by <code>llama3-8B</code> model.</p> <p>The following is the flow of the NAI lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployHarborRegistry {\n        [*] --&gt; IncreaseJumphostResources\n        IncreaseJumphostResources --&gt; DeployHarbor\n        DeployHarbor --&gt; [*]\n    }\n\n    state DeployNKP {\n        [*] --&gt; CreateNkpMachineImage\n        CreateNkpMachineImage --&gt; CreateNkpSelfManagedCluster\n        CreateNkpSelfManagedCluster --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n\n    state NAIPreRequisites {\n        [*] --&gt; ReserveIPs\n        ReserveIPs --&gt; CreateFilesShare\n        CreateFilesShare --&gt; [*]\n    }\n\n    state DeployNAI {\n        [*] --&gt; BootStrapDevCluster\n        BootStrapDevCluster --&gt; MonitorResourcesDeployment\n        MonitorResourcesDeployment --&gt; [*]\n    }\n\n    state TestLLMApp {\n        [*] --&gt; TestQueryLLM\n        TestQueryLLM --&gt; TestChatApp\n        TestChatApp --&gt; [*]\n    }\n\n    [*] --&gt; DeployHarborRegistry\n    DeployHarborRegistry --&gt; DeployNKP\n    DeployNKP --&gt; NAIPreRequisites\n    NAIPreRequisites --&gt; DeployNAI\n    DeployNAI --&gt; TestLLMApp\n    TestLLMApp --&gt; [*]</code></pre>"},{"location":"airgap_nai/airgap_nai_deploy/","title":"Deploying Nutanix Enterprise AI (NAI) NVD Reference Application","text":"<p>Version 2.3.0</p> <p>This version of the NAI deployment is based on the Nutanix Enterprise AI (NAI) <code>v2.3.0</code> release.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNAI {\n        [*] --&gt; DeployNAIAdmin\n        DeployNAIAdmin --&gt;  InstallSSLCert\n        InstallSSLCert --&gt; DownloadModel\n        DownloadModel --&gt; CreateNAI\n        CreateNAI --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI \n    DeployNAI --&gt; TestNAI : next section\n    TestNAI --&gt; [*]</code></pre>"},{"location":"airgap_nai/airgap_nai_deploy/#prepare-for-nai-deployment","title":"Prepare for NAI Deployment","text":"<p>Changes in NAI <code>v2.4.0</code></p> <ul> <li>Istio Ingress gateway is replaced with Envoy Gateway</li> <li>Knative is removed from NAI </li> <li>Kserve has been upgraded to 0.15.0</li> </ul>"},{"location":"airgap_nai/airgap_nai_deploy/#enable-nkp-applications","title":"Enable NKP Applications","text":"<p>Enable these NKP Applications from NKP GUI.</p> <p>Note</p> <p>In this lab, we will be using the Management Cluster Workspace to deploy our Nutanix Enterprise AI (NAI)</p> <p>However, in a customer environment, it is recommended to use a separate workload NKP cluster.</p> <p>Info</p> <p>The helm charts for these applications are stores in NKP's Chart Museum The container images are stored in internal Harbor registry. These images got uploaded to Harbor at the time of install NKE in this section.</p> <ol> <li>In the NKP GUI, Go to Clusters</li> <li>Click on Management Cluster Workspace</li> <li>Go to Applications </li> <li> <p>Search and enable the following applications: follow this order to install dependencies for NAI application</p> <ul> <li>Kube-prometheus-stack: version <code>70.4.2</code> or later (pre-installed on NKP cluster)  </li> </ul> </li> <li> <p>Login to VSC on the jumphost VM, append the following environment variables to the <code>$HOME\\airgap-nai\\.env</code> file and save it</p> Template .envSample .env <pre><code>export ENVIRONMENT=nkp\nexport NAI_USER=_your_desired_nai_username\nexport NAI_TEMP_PASS=_your_desired_nai_password # At least 8 characters\n</code></pre> <pre><code>export ENVIRONMENT=nkp\nexport NAI_USER=admin\nexport NAI_TEMP_PASS=_XXXXXXXXX # At least 8 characters\n</code></pre> </li> <li> <p>IN VSC,go to Terminal  and run the following commands to source the environment variables</p> <pre><code>source $HOME/airgap-nai/.env\n</code></pre> </li> <li> <p>Enable Envoy Gateway <code>v1.5.0</code> using the following command</p> CommandOutput <pre><code>helm install envoy-gateway \\\n  oci://${REGISTRY_HOST}/gateway-helm \\\n  --version v1.5.0 \\\n  --set image.repository=$REGISTRY_HOST/envoyproxy/gateway \\\n  --set image.tag=v1.5.0 \\\n  -n envoy-gateway-system \\\n  --create-namespace\n</code></pre> <pre><code>Pulled: harbor.10.x.x.111.nip.io/nkp/gateway-helm:v1.5.0\nDigest: sha256:2435a9cfcf22043b5ea2cdfe1e5783ec81f1dc527bff3c46c80c3ecc3ed66915\nNAME: envoy-gateway\nLAST DEPLOYED: Fri Aug 29 06:08:29 2025\nNAMESPACE: envoy-gateway-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> </li> <li> <p>Check if Envoy Gateway resources are ready</p> CommandOutput <pre><code>kubectl wait --timeout=5m -n envoy-gateway-system deployment/envoy-gateway --for=condition=Available\n</code></pre> <pre><code>deployment.apps/envoy-gateway condition met\n</code></pre> </li> <li> <p>Run the Kserve CRD installation</p> <pre><code>helm install kserve-crd \\\n  oci://${REGISTRY_HOST}/kserve-crd \\\n  --version v0.15.0 \\\n  -n kserve \\\n  --create-namespace \n</code></pre> </li> <li> <p>Run the Kserve installation</p> CommandOutput <pre><code>helm install kserve \\\n  oci://${REGISTRY_HOST}/kserve \\\n  --version v0.15.0 \\\n  -n kserve \\\n  --set controller.image.repository=${REGISTRY_HOST}/kserve-controller \\\n  --set controller.image.tag=v0.15.0 \\\n  --set kserve.controller.deploymentMode=RawDeployment \\\n  --set kserve.controller.gateway.disableIngressCreation=true\n</code></pre> <pre><code>Pulled: harbor.apj-cxrules.win/nkp/kserve:v0.15.0\nDigest: sha256:dae9d3e35c96d318bf3f5eb15e303ed6c268129f988f0b0b4ad698dc414d3f40\nNAME: kserve\nLAST DEPLOYED: Tue Sep  2 01:56:48 2025\nNAMESPACE: kserve\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> Check helm deployment status <p>Check the status of the <code>nai</code> helm deployments using the following command:</p> <pre><code>helm list -n envoy-gateway-system\nhelm list -n kserve\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy/#deploy-nai","title":"Deploy NAI","text":"<ol> <li> <p>Source the environment variables (if not done so already)</p> <pre><code>source $HOME/airgap-nai/.env\n</code></pre> </li> <li> <p>In <code>VSCode</code> Explorer pane, browse to <code>$HOME/airgap-nai</code> folder</p> </li> <li> <p>Run the following command to create a helm values file:</p> CommandSample NAI <code>v2.4.0</code> nkp-values.yaml <pre><code>cat &lt;&lt; EOF &gt; ${ENVIRONMENT}-values.yaml\nnaiIepOperator:\n  iepOperatorImage:\n    image: ${REGISTRY_HOST}/nai-iep-operator\n    tag: ${NAI_API_VERSION}\n  modelProcessorImage:\n    image: ${REGISTRY_HOST}/nai-model-processor\n    tag: ${NAI_API_VERSION}\nnaiInferenceUi:\n  naiUiImage:\n    image: ${REGISTRY_HOST}/nai-inference-ui\n    tag: ${NAI_API_VERSION}\nnaiApi:\n  naiApiImage:\n    image: ${REGISTRY_HOST}/nai-api\n    tag: ${NAI_API_VERSION}\n  supportedKserveRuntimeImage: ${REGISTRY_HOST}/nai-kserve-huggingfaceserver\n  supportedKserveCPURuntimeImageTag: ${KSERVE_VERSION}\n  supportedKserveGPURuntimeImageTag: ${KSERVE_VERSION}-gpu\n  supportedKserveCustomModelServerRuntimeImage: ${REGISTRY_HOST}/nai-kserve-custom-model-server\n  supportedKserveCustomModelServerRuntimeImageTag: ${NAI_API_VERSION}\n  # Details of super admin (first user in the nai system)\n  superAdmin:\n    username: ${NAI_USER}\n    password: ${NAI_TEMP_PASS} # At least 8 characters\n    # email: admin@nutanix.com\n    # firstName: admin\n#${REGISTRY_HOST}/nai-kserve-huggingfaceserver:kserve-version\n  supportedTGIImage: ${REGISTRY_HOST}/nai-tgi\n  supportedTGIImageTag: ${NAI_TGI_RUNTIME_VERSION}\nnaiDatabase:\n  naiDbImage:\n    image: ${REGISTRY_HOST}/nai-postgres:${NAI_POSTGRESQL_VERSION}\nnaiMonitoring:\n  prometheus:\n    image: \n      registry: ${REGISTRY_HOST}\n      repository: prometheus/prometheus\n      tag: ${NAI_PROMETHEUS_VERSION}\nEOF\n</code></pre> <pre><code>naiIepOperator:\n  iepOperatorImage:\n    image: harbor.10.x.x.111.nip.io/nkp/nai-iep-operator\n    tag: v2.4.0\n  modelProcessorImage:\n    image: harbor.10.x.x.111.nip.io/nkp/nai-model-processor\n    tag: v2.4.0\nnaiInferenceUi:\n  naiUiImage:\n    image: harbor.10.x.x.111.nip.io/nkp/nai-inference-ui\n    tag: v2.4.0\nnaiApi:\n  naiApiImage:\n    image: harbor.10.x.x.111.nip.io/nkp/nai-api\n    tag: v2.4.0\n  supportedKserveRuntimeImage: harbor.10.x.x.111.nip.io/nkp/nai-kserve-huggingfaceserver\n  supportedKserveCPURuntimeImageTag: \"v0.15.2\"\n  supportedKserveGPURuntimeImageTag: \"v0.15.2-gpu\"\n  supportedKserveCustomModelServerRuntimeImage: harbor.10.x.x.111.nip.io/nkp/nai-kserve-custom-model-server\n  supportedKserveCustomModelServerRuntimeImageTag: \"v2.4.0\"\n# harbor.10.x.x.111.nip.io/nkp/nai-kserve-huggingfaceserver:kserve-version\n  supportedTGIImage: harbor.10.x.x.111.nip.io/nkp/nai-tgi\n  supportedTGIImageTag: \"3.3.4-b2485c9\"\n  # Details of super admin (first user in the nai system)\n  superAdmin:\n    username: admin         \n    password: _XXXXXXXXXX # At least 8 characters\n    # email: admin@nutanix.com\n    # firstName: admin\nnaiDatabase:\n  naiDbImage:\n    image: harbor.10.x.x.111.nip.io/nkp/nai-postgres:16.1-alpine\nnaiMonitoring:\n  prometheus:\n    image: \n      registry: harbor.10.x.x.111.nip.io/nkp\n      repository: prometheus/prometheus\n      tag: v2.54.0\n</code></pre> </li> <li> <p>In <code>VSCode</code>, Under <code>$HOME/airgap-nai</code> folder, click on New File  and create a file with the following name:</p> <pre><code>nai-deploy.sh\n</code></pre> <p>with the following content:</p> <pre><code>#!/usr/bin/env bash\n\nset -ex\nset -o pipefail\n\nhelm upgrade --install nai-core oci://${REGISTRY_HOST}/nai-core \\\n--version=${NAI_CORE_VERSION} -n nai-system --create-namespace \\\n--insecure-skip-tls-verify \\\n-f ${ENVIRONMENT}-values.yaml --wait\n</code></pre> </li> <li> <p>Run the following command to deploy NAI</p> CommandCommand output <pre><code>$HOME/airgap-nai/nai-deploy.sh\n</code></pre> <pre><code>$HOME/airgap-nai/nai-deploy.sh \n\nRelease \"nai-core\" does not exist. Installing it now.\nPulled: harbor.10.x.x.111.nip.i/nkp/nai-core:2.4.0\nDigest: sha256:283b8373ca76088d89fbf91482ef8530f6608f30085f84863e17edae77efe673\nNAME: nai-core\nLAST DEPLOYED: Tue Sep  2 01:31:15 2025\nNAMESPACE: nai-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> </li> <li> <p>Verify that the NAI Core Pods are running and healthy</p> CommandCommand output <pre><code>kubens nai-system\nkubectl get po,deploy\n</code></pre> <pre><code>$ kubens nai-system\n\u2714 Active namespace is \"nai-system\"\n\n$ kubectl get po,deploy\n\nNAME                                           READY   STATUS      RESTARTS   AGE\npod/nai-api-db-migrate-tt6rn-cp5fc             0/1     Completed   1          7m\npod/nai-api-fbc4f956d-h5vk2                    1/1     Running     0          7m\npod/nai-db-0                                   1/1     Running     0          7m\npod/nai-iep-model-controller-f84596945-ck5tk   1/1     Running     0          7m\npod/nai-ui-d5b546bfc-82x6l                     1/1     Running     0          7m\npod/prometheus-nai-0                           2/2     Running     0          7m\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nai-api                    1/1     1            1           7m\ndeployment.apps/nai-iep-model-controller   1/1     1            1           7m\ndeployment.apps/nai-ui                     1/1     1            1           7m\n</code></pre> </li> </ol> Uninstall NAI <code>v2.3.0</code> Dependencies <p>If you are upgrading NAI from <code>v2.3.0</code> to <code>v2.4.0</code>, uninstall the following:</p> <p>If Helm was used:</p> <p>Uninstall Istio<pre><code>helm uninstall istio-ingressgateway -n istio-system --wait --ignore-not-found\nhelm uninstall istiod -n istio-system --wait --ignore-not-found\nhelm uninstall istio-base -n istio-system --wait --ignore-not-found\n</code></pre> Uninstall Knative<pre><code>kubectl delete --ignore-not-found=true KnativeServing knative-serving -n knative-serving\nhelm uninstall knative-operator -n knative-serving --wait --ignore-not-found\nkubectl wait --for=delete pod --all -n knative-serving --timeout=300s\n</code></pre></p> <p>If NKP Application were used for installation:</p> <p>Go to NKP Cluster Dashboard &gt; Application &gt; Search and Uninstall the following:</p> <ol> <li>Istio</li> <li>Knative </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy/#install-ssl-certificate-and-gateway-elements","title":"Install SSL Certificate and Gateway Elements","text":"<p>In this section we will install SSL Certificate to access the NAI UI. This is required as the endpoint will only work with a ssl endpoint with a valid certificate.</p> <p>NAI UI is accessible using the Ingress Gateway.</p> <p>The following steps show how cert-manager can be used to generate a self signed certificate using the default selfsigned-issuer present in the cluster. </p> <p>If you are using Public Certificate Authority (CA) for NAI SSL Certificate</p> <p>If an organization generates certificates using a different mechanism then obtain the certificate + key and create a kubernetes secret manually using the following command:</p> <pre><code>kubectl -n istio-system create secret tls nai-cert --cert=path/to/nai.crt --key=path/to/nai.key\n</code></pre> <p>Skip the steps in this section to create a self-signed certificate resource.</p> <ol> <li> <p>Get the NAI UI ingress gateway host using the following command:</p> <pre><code>NAI_UI_ENDPOINT=$(kubectl get svc -n envoy-gateway-system -l \"gateway.envoyproxy.io/owning-gateway-name=nai-ingress-gateway,gateway.envoyproxy.io/owning-gateway-namespace=nai-system\" -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}' | grep -v '^$' || kubectl get svc -n envoy-gateway-system -l \"gateway.envoyproxy.io/owning-gateway-name=nai-ingress-gateway,gateway.envoyproxy.io/owning-gateway-namespace=nai-system\" -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')\n</code></pre> </li> <li> <p>Get the value of <code>NAI_UI_ENDPOINT</code> environment variable</p> CommandCommand output <pre><code>echo $NAI_UI_ENDPOINT\n</code></pre> <pre><code>10.x.x.216\n</code></pre> </li> <li> <p>We will use the command output e.g: <code>10.x.x.216</code> as the IP address for NAI as reserved in this section</p> </li> <li> <p>Construct the FQDN of NAI UI using nip.io and we will use this FQDN as the certificate's Common Name (CN).</p> Template URLSample URL <pre><code>nai.${NAI_UI_ENDPOINT}.nip.io\n</code></pre> <pre><code>nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>Create the ingress resource certificate using the following command:</p> <pre><code>cat &lt;&lt; EOF | k apply -f -\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: nai-cert\n  namespace: nai-system\nspec:\n  issuerRef:\n    name: selfsigned-issuer\n    kind: ClusterIssuer\n  secretName: nai-cert\n  commonName: nai.${NAI_UI_ENDPOINT}.nip.io\n  dnsNames:\n  - nai.${NAI_UI_ENDPOINT}.nip.io\n  ipAddresses:\n  - ${NAI_UI_ENDPOINT}\nEOF\n</code></pre> </li> <li> <p>Patch the Envoy gateway with the <code>nai-cert</code> certificate details</p> <pre><code>kubectl patch gateway nai-ingress-gateway -n nai-system --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/listeners/1/tls/certificateRefs/0/name\", \"value\": \"nai-cert\"}]'\n</code></pre> </li> <li> <p>Create EnvoyProxy</p> <pre><code>k apply -f -&lt;&lt;EOF\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyProxy\nmetadata:\n  name: envoy-service-config\n  namespace: nai-system\nspec:\n  provider:\n    type: Kubernetes\n    kubernetes:\n      envoyService:\n        type: LoadBalancer\nEOF\n</code></pre> </li> <li> <p>Patch the <code>nai-ingress-gateway</code> resource with the new <code>EnvoyProxy</code> details</p> <pre><code>kubectl patch gateway nai-ingress-gateway -n nai-system --type=merge \\\n-p '{\n    \"spec\": {\n        \"infrastructure\": {\n            \"parametersRef\": {\n                \"group\": \"gateway.envoyproxy.io\",\n                \"kind\": \"EnvoyProxy\",\n                \"name\": \"envoy-service-config\"\n            }\n        }\n    }\n}'\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy/#accessing-the-ui","title":"Accessing the UI","text":"<ol> <li> <p>In a browser, open the following URL to connect to the NAI UI</p> <pre><code>https://nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>Use the <code>${NAI_USER}</code> and <code>${NAI_TEMP_PASS}</code> values set in <code>${ENVIRONMENT}-values.yaml</code> files during <code>helm</code> installation of NAI <code>v.2.4.0</code></p> </li> <li> <p>Change the password for the <code>admin</code> user</p> </li> <li> <p>Login using <code>admin</code> user and password.</p> <p></p> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy/#download-model","title":"Download Model","text":"<p>We will download and user llama3 8B model which we sized for in the previous section.</p> <ol> <li>In the NAI GUI, go to Models</li> <li>Click on Import Model from Hugging Face</li> <li>Choose the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model</li> <li> <p>Input your Hugging Face token that was created in the previous section and click Import</p> </li> <li> <p>Provide the Model Instance Name as <code>Meta-Llama-3.1-8B-Instruct</code> and click Import</p> </li> <li> <p>Go to VSC Terminal to monitor the download</p> CommandCommand output <p>Get jobs in nai-admin namespace<pre><code>kubens nai-admin\n\nkubectl get jobs\n</code></pre> Validate creation of pods and PVC<pre><code>kubectl get po,pvc\n</code></pre> Verify download of model using pod logs<pre><code>kubectl logs -f _pod_associated_with_job\n</code></pre></p> <p>Get jobs in nai-admin namespace<pre><code>kubens nai-admin\n\n\u2714 Active namespace is \"nai-admin\"\n\nkubectl get jobs\n\nNAME                                       COMPLETIONS   DURATION   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-model-job   0/1           4m56s      4m56\n</code></pre> Validate creation of pods and PVC<pre><code>kubectl get po,pvc\n\nNAME                                             READY   STATUS    RESTARTS   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-model-job-9nmff   1/1     Running   0          4m49s\n\nNAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      VOLUMEATTRIBUTESCLASS   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-pvc-claim   Bound    pvc-a63d27a4-2541-4293-b680-514b8b890fe0   28Gi       RWX            nai-nfs-storage   &lt;unset&gt;                 2d\n</code></pre> Verify download of model using pod logs<pre><code>kubectl logs -f nai-c0d6ca61-1629-43d2-b57a-9f-model-job-9nmff \n\n/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.05 MB. The target location /data/model-files only has 0.00 MB free disk space.\nwarnings.warn(\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51.0k/51.0k [00:00&lt;00:00, 3.26MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.09M/9.09M [00:00&lt;00:00, 35.0MB/s]&lt;00:30, 150MB/s]\nmodel-00004-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.17G/1.17G [00:12&lt;00:00, 94.1MB/s]\nmodel-00001-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.98G/4.98G [04:23&lt;00:00, 18.9MB/s]\nmodel-00003-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.92G/4.92G [04:33&lt;00:00, 18.0MB/s]\nmodel-00002-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [04:47&lt;00:00, 17.4MB/s]\nFetching 16 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [05:42&lt;00:00, 21.43s/it]:33&lt;00:52, 9.33MB/s]\n## Successfully downloaded model_files|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [04:47&lt;00:00, 110MB/s] \n\nDeleting directory : /data/hf_cache\n</code></pre></p> </li> <li> <p>Optional - verify the events in the namespace for the pvc creation </p> CommandCommand output <pre><code>k get events | awk '{print $1, $3}'\n</code></pre> <pre><code>$ k get events | awk '{print $1, $3}'\n\n3m43s Scheduled\n3m43s SuccessfulAttachVolume\n3m36s Pulling\n3m29s Pulled\n3m29s Created\n3m29s Started\n3m43s SuccessfulCreate\n90s   Completed\n3m53s Provisioning\n3m53s ExternalProvisioning\n3m45s ProvisioningSucceeded\n3m53s PvcCreateSuccessful\n3m48s PvcNotBound\n3m43s ModelProcessorJobActive\n90s   ModelProcessorJobComplete\n</code></pre> </li> </ol> <p>The model is downloaded to the Nutanix Files <code>pvc</code> volume.</p> <p>After a successful model import, you will see it in Active status in the NAI UI under Models menu</p> <p></p>"},{"location":"airgap_nai/airgap_nai_deploy/#create-and-test-inference-endpoint","title":"Create and Test Inference Endpoint","text":"<p>In this section we will create an inference endpoint using the downloaded model.</p> <ol> <li>Navigate to Inference Endpoints menu and click on Create Endpoint button</li> <li> <p>Fill the following details:</p> <ul> <li>Endpoint Name: <code>llama-8b</code></li> <li>Model Instance Name: <code>Meta-LLaMA-8B-Instruct</code></li> <li>Use GPUs for running the models : <code>Checked</code></li> <li>No of GPUs (per instance):</li> <li>GPU Card: <code>NVIDIA-L40S</code> (or other available GPU)</li> <li>No of Instances: <code>1</code></li> <li>API Keys: Create a new API key or use an existing one</li> </ul> </li> <li> <p>Click on Create</p> </li> <li> <p>Monitor the <code>nai-admin</code> namespace to check if the services are coming up</p> CommandCommand output <pre><code>kubens nai-admin\nkubectl get po,deploy\n</code></pre> <pre><code>kubens nai-admin\nget po,deploy\nNAME                                                     READY   STATUS        RESTARTS   AGE\npod/llama8b-predictor-00001-deployment-9ffd786db-6wkzt   2/2     Running       0          71m\n\nNAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/llama8b-predictor-00001-deployment   1/1     1            0           3d17h\n</code></pre> </li> <li> <p>Check the events in the <code>nai-admin</code> namespace for resource usage to make sure all </p> CommandCommand output <pre><code>kubectl get events -n nai-admin --sort-by='.lastTimestamp' | awk '{print $1, $3, $5}'\n</code></pre> <pre><code>$ kubectl get events -n nai-admin --sort-by='.lastTimestamp' | awk '{print $1, $3, $5}'\n\n110s FinalizerUpdate Updated\n110s FinalizerUpdate Updated\n110s RevisionReady Revision\n110s ConfigurationReady Configuration\n110s LatestReadyUpdate LatestReadyRevisionName\n110s Created Created\n110s Created Created\n110s Created Created\n110s InferenceServiceReady InferenceService\n110s Created Created\n</code></pre> </li> <li> <p>Once the services are running, check the status of the inference service</p> CommandCommand output <pre><code>kubectl get isvc\n</code></pre> <pre><code>kubectl get isvc\n\nNAME      URL                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllama8b   http://llama8b.nai-admin.svc.cluster.local   True           100                              llama8b-predictor-00001   3d17h\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy/#troubleshooting-endpoint-isvc","title":"Troubleshooting Endpoint ISVC","text":"<p>TGI Imange and Self-signed Certificates</p> <p>Only follow this procedure if this <code>isvc</code> is not starting up.</p> <p>KNative Serving Image Tag Checking</p> <p>From testing, we have identified that KServe module is making sure that there are no container image tag discrepencies, by pulling image using SHA digest. This is done to avoid pulling images that are updated without updating the tag.</p> <p>We have avoided this behavior by patching the <code>config-deployment</code> config map in the <code>knative-serving</code> namespace to skip image tag checking. Check this Prepare for NAI Deployment sectionfor more details.</p> <pre><code>kubectl patch configmap  config-deployment -n knative-serving --type merge -p '{\"data\":{\"registries-skipping-tag-resolving\":\"${REGISTRY_HOST}\"}'\n</code></pre> <p>If this procedure was not followed, then the <code>isvc</code> will not start up.</p> <ol> <li> <p>If the <code>isvc</code> is not coming up, then explore the events in <code>nai-admin</code> namespace.</p> CommandCommand output <pre><code>kubens nai-admin\nkubectl get isvc\nkubectl get events  --sort-by='.lastTimestamp'\n</code></pre> <pre><code>$ kubectl get isvc\n\nNAME      URL                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllama8b   http://llama8b.nai-admin.svc.cluster.local   False\n\n$ kubectl get events --sort-by='.lastTimestamp'\n\nWarning   InternalError         revision/llama8b-predictor-00001   Unable to fetch image \"harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\": failed to resolve image to digest: \nGet \"https://harbor.10.x.x.111.nip.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority\n</code></pre> <p>The temporary workaround is to use the TGI images SHA signature from the container registry.</p> <p>This site will be updated with resolutions for the above issues in the future.</p> </li> <li> <p>Note the above TGI image SHA digest from the container registry.</p> CommandCommand output <pre><code>docker pull ${REGISTRY_HOST}/nutanix/nai-tgi:${NAI_TGI_RUNTIME_VERSION}\n</code></pre> <pre><code>docker pull harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\n\n2.3.1-825f39d: Pulling from nkp/nutanix/nai-tgi\nDigest: sha256:2df9fab2cf86ab54c2e42959f23e6cfc5f2822a014d7105369aa6ddd0de33006\nStatus: Image is up to date for harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\nharbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\n</code></pre> </li> <li> <p>The SHA digest will look like the following:</p> TGI image SHA digest will be different for different environments<pre><code>sha256:2df9fab2cf86ab54c2e42959f23e6cfc5f2822a014d7105369aa6ddd0de33006\n</code></pre> </li> <li> <p>Create a copy of the <code>isvc</code> manifest</p> <pre><code>kubectl get isvc llama8b -n nai-admin -o yaml &gt; llama8b.yaml\n</code></pre> </li> <li> <p>Edit the <code>isvc</code></p> <pre><code>kubectl edit isvc llama8b -n nai-admin\n</code></pre> </li> <li> <p>Search and replace the <code>image</code> tag with the SHA digest from the TGI image.</p> <p><pre><code>&lt;snip&gt;\n\nenv:\n- name: STORAGE_URI\n  value: pvc://nai-c34d8d58-d6f8-4cb4-94e4-28-pvc-claim/model-files\n  image: harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\n\n&lt;snip&gt;\n</code></pre> 7. After replacing the image's SHA digest, the image value should look as follows: </p> <pre><code>&lt;snip&gt;\n\nenv:\n- name: STORAGE_URI\n  value: pvc://nai-c34d8d58-d6f8-4cb4-94e4-28-pvc-claim/model-files\n  image: harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi@sha256:2df9fab2cf86ab54c2e42959f23e6cfc5f2822a014d7105369aa6ddd0de33006\n\n&lt;snip&gt;\n</code></pre> </li> <li> <p>Save the <code>isvc</code> configuration by writing the changes to the file and exiting the vi editor using <code>:wq!</code> key combination.</p> </li> <li> <p>Verify that the <code>isvc</code> is running</p> CommandCommand output <pre><code>kubens nai-admin\nkubectl get isvc\n</code></pre> <pre><code>$ kubectl get isvc\n\nNAME      URL                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllama8b   http://llama8b.nai-admin.svc.cluster.local   True           100                              llama8b-predictor-00001   3d17h\n</code></pre> </li> </ol> <p>This should resolve the issue the issue with the TGI image.</p> <p>Report Other Issues</p> <p>If you are facing any other issues, please report them here in the NAI LLM GitHub Repo Issues page.</p>"},{"location":"airgap_nai/airgap_nai_deploy_2.0/","title":"Deploying Nutanix Enterprise AI (NAI) NVD Reference Application","text":"<p>Version 2.0.0</p> <p>This version of the NAI deployment is based on the Nutanix Enterprise AI (NAI) <code>v2.0.0</code> release.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNAI {\n        [*] --&gt; DeployNAIAdmin\n        DeployNAIAdmin --&gt;  InstallSSLCert\n        InstallSSLCert --&gt; DownloadModel\n        DownloadModel --&gt; CreateNAI\n        CreateNAI --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI \n    DeployNAI --&gt; TestNAI : next section\n    TestNAI --&gt; [*]</code></pre>"},{"location":"airgap_nai/airgap_nai_deploy_2.0/#prepare-for-nai-deployment","title":"Prepare for NAI Deployment","text":"<ol> <li> <p>Login to VSC on the jumphost VM, append the following environment variables to the <code>$HOME\\airgap-nai\\.env</code> file and save it</p> Template .envSample .env <pre><code>export KOMMANDER_CLUSTER_HOSTNAME=_ip_or_hostname_of_kommander_dashboard_url\nexport INTERNAL_REPO=https://${KOMMANDER_CLUSTER_HOSTNAME}/dkp/kommander/helm-mirror\nexport ENVIRONMENT=nkp\nexport REGISTRY_URL=_your_registry_url\nexport REGISTRY_HOST=_your_registry_hostname\n</code></pre> <pre><code>export KOMMANDER_CLUSTER_HOSTNAME=\"10.x.x.214\"\nexport INTERNAL_REPO=https://${KOMMANDER_CLUSTER_HOSTNAME}/dkp/kommander/helm-mirror\nexport ENVIRONMENT=nkp\nexport REGISTRY_URL=\"https://harbor.10.x.x.111.nip.io/nkp\"\nexport REGISTRY_HOST=\"harbor.10.x.x.111.nip.io/nkp\"\n</code></pre> </li> <li> <p>IN VSC,go to Terminal  and run the following commands to source the environment variables</p> <pre><code>source $HOME/airgap-nai/.env\n</code></pre> </li> <li> <p>In <code>VSCode</code>, under the newly created <code>airgap-nai</code> folder, click on New File  and create file with the following name:</p> <pre><code>nai-prepare.sh\n</code></pre> <p>with the following content:</p> <pre><code>#!/usr/bin/env bash\n\nset -ex\nset -o pipefail\n\n## Deploy Istio 1.20.8\nhelm --insecure-skip-tls-verify=true upgrade --install istio-base base --repo ${INTERNAL_REPO} --version=${ISTIO_VERSION} -n istio-system --create-namespace --wait\n\nhelm --insecure-skip-tls-verify=true upgrade --install istiod istiod --repo ${INTERNAL_REPO} --version=${ISTIO_VERSION} -n istio-system \\\n--set gateways.securityContext.runAsUser=0 \\\n--set gateways.securityContext.runAsGroup=0 --wait\n\nhelm --insecure-skip-tls-verify=true upgrade --install istio-ingressgateway gateway --repo ${INTERNAL_REPO} --version=${ISTIO_VERSION} -n istio-system \\\n--set securityContext.runAsUser=0 --set securityContext.runAsGroup=0 \\\n--set containerSecurityContext.runAsUser=0 --set containerSecurityContext.runAsGroup=0 --wait\n\n## Deploy Knative \nhelm --insecure-skip-tls-verify=true upgrade --install knative-serving-crds nai-knative-serving-crds --repo ${INTERNAL_REPO} --version=${KNATIVE_VERSION} -n knative-serving --create-namespace --wait\n\nhelm --insecure-skip-tls-verify=true upgrade --install knative-serving nai-knative-serving --repo ${INTERNAL_REPO} -n knative-serving --version=${KNATIVE_VERSION} --wait\n\n\nhelm --insecure-skip-tls-verify=true upgrade --install knative-istio-controller nai-knative-istio-controller --repo ${INTERNAL_REPO} -n knative-serving --version=${KNATIVE_VERSION} --wait\n\n# Patch configurations stored in configmaps\n\nkubectl patch configmap config-features -n knative-serving -p '{\"data\":{\"kubernetes.podspec-nodeselector\":\"enabled\"}}'\n\nkubectl patch configmap config-autoscaler -n knative-serving -p '{\"data\":{\"enable-scale-to-zero\":\"false\"}}'\n\nkubectl patch configmap  config-domain -n knative-serving --type merge -p '{\"data\":{\"example.com\":\"\"}}'\n\n# This patch of config-deployment config map \n# is necessary in air-gapped environment \n# kserve will to skip image tag checks\n# for the self hosted registry if the following is configured\n\nkubectl patch configmap  config-deployment -n knative-serving --type merge -p '{\"data\":{\"registries-skipping-tag-resolving\":\"${REGISTRY_HOST\"}}'\n\n## Deploy Kserve\n\nhelm --insecure-skip-tls-verify=true upgrade --install kserve-crd kserve-crd --repo ${INTERNAL_REPO} --version=${KSERVE_VERSION} -n kserve --create-namespace\n\nhelm --insecure-skip-tls-verify=true upgrade --install kserve kserve --repo ${INTERNAL_REPO} --version=${KSERVE_VERSION} -n kserve \\\n--set kserve.modelmesh.enabled=false \\\n--set kserve.controller.image=\"${REGISTRY_HOST}/nutanix/nai-kserve-controller\" \\\n--set kserve.controller.tag=${KSERVE_VERSION} --wait\n</code></pre> </li> <li> <p>Run the script from the Terminal</p> CommandCommand output <pre><code>chmod +x $HOME/airgap-nai/nai-prepare.sh\n$HOME/airgap-nai/nai-prepare.sh\n</code></pre> <pre><code>Release \"istiod\" has been upgraded. Happy Helming!\nNAME: istiod\nLAST DEPLOYED: Tue Oct 15 02:01:58 2024\nNAMESPACE: istio-system\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\n\"istiod\" successfully installed!\n\nNAME: istio-ingressgateway\nLAST DEPLOYED: Tue Oct 15 02:02:01 2024\nNAMESPACE: istio-system\nSTATUS: deployed\n\nNAME: knative-serving-crds\nLAST DEPLOYED: Tue Oct 15 02:02:03 2024\nNAMESPACE: knative-serving\nSTATUS: deployed\n\nNAME: knative-serving\nLAST DEPLOYED: Tue Oct 15 02:02:05 2024\nNAMESPACE: knative-serving\nSTATUS: deployed\n\nNAME: kserve-crd\nLAST DEPLOYED: Tue Oct 15 02:02:16 2024\nNAMESPACE: kserve\nSTATUS: deployed\n\nNAME: kserve\nLAST DEPLOYED: Tue Oct 15 02:02:19 2024\nNAMESPACE: kserve\nSTATUS: deployed\n</code></pre> Check helm deployment status <p>Check the status of the <code>nai</code> helm deployments using the following command:</p> <pre><code>helm list -n istio-system\nhelm list -n kserve\nhelm list -n knative-serving\n</code></pre> </li> <li> <p>Validate if the resources are running in the following namespaces.</p> <ul> <li><code>istio-system</code>, </li> <li><code>knative-serving</code>, and </li> <li><code>kserve</code></li> </ul> CommandCommand output <pre><code>kubectl get po -n istio-system\nk get po -n kserve\nk get po -n knative-serving\n</code></pre> <pre><code>$ k get po -n istio-system\nNAME                                    READY   STATUS    RESTARTS   AGE\nistio-ingressgateway-6675867d85-qzrpq   1/1     Running   0          26d\nistiod-6d96569c9b-2dww4                 1/1     Running   0          26d\n\n$ k get po -n kserve\nNAME                                         READY   STATUS    RESTARTS   AGE\nkserve-controller-manager-6654f69d5c-45n64   2/2     Running   0          26d\n\n$ k get po -n knative-serving\nNAME                                   READY   STATUS    RESTARTS   AGE\nactivator-58db57894b-g2nx8             1/1     Running   0          26d\nautoscaler-76f95fff78-c8q9m            1/1     Running   0          26d\ncontroller-7dd875844b-4clqb            1/1     Running   0          26d\nnet-istio-controller-57486f879-85vml   1/1     Running   0          26d\nnet-istio-webhook-7ccdbcb557-54dn5     1/1     Running   0          26d\nwebhook-d8674645d-mscsc                1/1     Running   0          26d\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy_2.0/#deploy-nai","title":"Deploy NAI","text":"<ol> <li> <p>Source the environment variables (if not done so already)</p> <pre><code>source $HOME/airgap-nai/.env\n</code></pre> </li> <li> <p>In <code>VSCode</code> Explorer pane, browse to <code>$HOME/airgap-nai</code> folder</p> </li> <li> <p>Run the following command to create a helm values file:</p> CommandSample nkp-values.yaml <pre><code>cat &lt;&lt; EOF &gt; ${ENVIRONMENT}-values.yaml\n## Image pull secret. This is required for the huggingface image check by the Inference pod as that does not go via the kubelet and does a direct check.\nimagePullSecret:\n  ## Name of the image pull secret\n  name: nai-iep-secret\n  ## Image registry credentials\n  credentials:\n    registry: ${REGISTRY_URL}\n    username: ${REGISTRY_USERNAME}\n    password: ${REGISTRY_PASSWORD}\n    email: ${REGISTRY_USERNAME}@foobar.com\nnaiApi:\n  naiApiImage:\n    image: ${REGISTRY_HOST}/nutanix/nai-api\n    tag: ${NAI_API_VERSION}  \n  supportedRuntimeImage: ${REGISTRY_HOST}/nutanix/nai-kserve-huggingfaceserver:${NAI_KSERVE_HF_SERVER_VERSION}\n  supportedTGIImage: ${REGISTRY_HOST}/nutanix/nai-tgi\n  supportedTGIImageTag: ${NAI_TGI_RUNTIME_VERSION}\nnaiIepOperator:\n  iepOperatorImage:\n    image: ${REGISTRY_HOST}/nutanix/nai-iep-operator\n    tag: ${NAI_API_VERSION}\n  modelProcessorImage:\n    image: ${REGISTRY_HOST}/nutanix/nai-model-processor\n    tag: ${NAI_API_VERSION}\nnaiInferenceUi:\n  naiUiImage:\n    image: ${REGISTRY_HOST}/nutanix/nai-inference-ui\n    tag: ${NAI_API_VERSION}\nnaiDatabase:\n  naiDbImage:\n    image: ${REGISTRY_HOST}/nutanix/nai-postgres:16.1-alpine\nnaiMonitoring:\n  prometheus:\n    image: \n      registry: ${REGISTRY_HOST}\n      repository: prometheus/prometheus\n      tag: ${NAI_PROMETHEUS_VERSION}\n## nai-monitoring stack values for nai-monitoring stack deployment in NKE environment\nnaiMonitoring:\n  ## Component scraping node exporter\n  ##\n  nodeExporter:\n    serviceMonitor:\n      enabled: true\n      endpoint:\n        port: http-metrics\n        scheme: http\n        targetPort: 9100\n      namespaceSelector:\n        matchNames:\n        - kommander\n      serviceSelector:\n        matchLabels:\n          app.kubernetes.io/name: prometheus-node-exporter\n          app.kubernetes.io/component: metrics\n          app.kubernetes.io/version: 1.8.1\n  ## Component scraping dcgm exporter\n  ##\n  dcgmExporter:\n    podLevelMetrics: true\n    serviceMonitor:\n      enabled: true\n      endpoint:\n        targetPort: 9400\n      namespaceSelector:\n        matchNames:\n        - kommander\n      serviceSelector:\n        matchLabels:\n          app: nvidia-dcgm-exporter\nEOF\n</code></pre> <pre><code>## Image pull secret. This is required for the huggingface image check by the Inference pod as that does not go via the kubelet and does a direct check.\nimagePullSecret:\n  ## Name of the image pull secret\n  name: nai-iep-secret\n  ## Image registry credentials\n  credentials:\n    registry: https://harbor.10.x.x.111.nip.io/nkp\n    username: admin\n    password: xxxxxxx\n    email: admin@foobar.com\nnaiApi:\n  naiApiImage:\n    image: harbor.10.x.x.111.nip.io/nkp/nutanix/nai-api\n    tag: v2.0.0  \n  supportedRuntimeImage: harbor.10.x.x.111.nip.io/nkp/nutanix/nai-kserve-huggingfaceserver:v0.14.0\n  supportedTGIImage: harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi\n  supportedTGIImageTag: \"2.3.1-825f39d\"\nnaiIepOperator:\n  iepOperatorImage:\n    image:  harbor.10.x.x.111.nip.io/nkp/nutanix/nai-iep-operator\n    tag: v2.0.0\n  modelProcessorImage:\n    image:  harbor.10.x.x.111.nip.io/nkp/nutanix/nai-model-processor\n    tag: v2.0.0\nnaiInferenceUi:\n  naiUiImage:\n    image:  harbor.10.x.x.111.nip.io/nkp/nutanix/nai-inference-ui\n    tag: v2.0.0\nnaiDatabase:\n  naiDbImage:\n    image:  harbor.10.x.x.111.nip.io/nkp/nutanix/nai-postgres:16.1-alpine\nnaiMonitoring:\n  prometheus:\n    image: \n      registry: harbor.10.x.x.111.nip.io/nkp\n      repository: prometheus/prometheus\n      tag: v2.53.0     \n# nai-monitoring stack values for nai-monitoring stack deployment in NKE environment\nnaiMonitoring:\n  ## Component scraping node exporter\n  ##\n  nodeExporter:\n    serviceMonitor:\n      enabled: true\n      endpoint:\n        port: http-metrics\n        scheme: http\n        targetPort: 9100\n      namespaceSelector:\n        matchNames:\n        - kommander\n      serviceSelector:\n        matchLabels:\n          app.kubernetes.io/name: prometheus-node-exporter\n          app.kubernetes.io/component: metrics\n          app.kubernetes.io/version: 1.8.1\n  ## Component scraping dcgm exporter\n  ##\n  dcgmExporter:\n    podLevelMetrics: true\n    serviceMonitor:\n      enabled: true\n      endpoint:\n        targetPort: 9400\n      namespaceSelector:\n        matchNames:\n        - kommander\n      serviceSelector:\n        matchLabels:\n          app: nvidia-dcgm-exporter\n</code></pre> </li> <li> <p>In <code>VSCode</code>, Under <code>$HOME/airgap-nai</code> folder, click on New File  and create a file with the following name:</p> <pre><code>nai-deploy.sh\n</code></pre> <p>with the following content:</p> <pre><code>#!/usr/bin/env bash\n\nset -ex\nset -o pipefail\n\nhelm upgrade --install nai-core nai-core --repo ${INTERNAL_REPO} \\\n--version=${NAI_CORE_VERSION} -n nai-system --create-namespace \\\n--insecure-skip-tls-verify \\\n-f ${ENVIRONMENT}-values.yaml --wait\n</code></pre> </li> <li> <p>Run the following command to deploy NAI</p> CommandCommand output <pre><code>$HOME/airgap-nai/nai-deploy.sh\n</code></pre> <pre><code>$HOME/airgap-nai/nai-deploy.sh \n\n+ set -o pipefail\n+ helm repo add ntnx-charts https://nutanix.github.io/helm-releases\n\"ntnx-charts\" already exists with the same configuration, skipping\n+ helm repo update ntnx-charts\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"ntnx-charts\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\nhelm upgrade --install nai-core ntnx-charts/nai-core --version=$NAI_CORE_VERSION -n nai-system --create-namespace --wait \\\n--set naiApi.naiApiImage.tag=v1.0.0-rc2 \\\n--insecure-skip-tls-verify \\\n-f nkp-values.yaml\nRelease \"nai-core\" has been upgraded. Happy Helming!\nNAME: nai-core\nLAST DEPLOYED: Mon Sep 16 22:07:24 2024\nNAMESPACE: nai-system\nSTATUS: deployed\nREVISION: 7\nTEST SUITE: None\n</code></pre> </li> <li> <p>Verify that the NAI Core Pods are running and healthy</p> CommandCommand output <pre><code>kubens nai-system\nkubectl get po,deploy\n</code></pre> <pre><code>$ kubens nai-system\n\u2714 Active namespace is \"nai-system\"\n\n$ kubectl get po,deploy\n\nNAME                                            READY   STATUS      RESTARTS   AGE\npod/nai-api-55c665dd67-746b9                    1/1     Running     0          5d1h\npod/nai-api-db-migrate-fdz96-xtmxk              0/1     Completed   0          40h\npod/nai-db-789945b4df-lb4sd                     1/1     Running     0          43h\npod/nai-iep-model-controller-84ff5b5b87-6jst9   1/1     Running     0          5d8h\npod/nai-ui-7fc65fc6ff-clcjl                     1/1     Running     0          5d8h\npod/prometheus-nai-0                            2/2     Running     0          43h\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nai-api                    1/1     1            1           5d8h\ndeployment.apps/nai-db                     1/1     1            1           5d8h\ndeployment.apps/nai-iep-model-controller   1/1     1            1           5d8h\ndeployment.apps/nai-ui                     1/1     1            1           5d8h\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy_2.0/#install-ssl-certificate","title":"Install SSL Certificate","text":"<p>In this section we will install SSL Certificate to access the NAI UI. This is required as the endpoint will only work with a ssl endpoint with a valid certificate. </p> <p>NAI UI is accessible using the Ingress Gateway.</p> <p>The following steps show how cert-manager can be used to generate a self signed certificate using the default selfsigned-issuer present in the cluster. </p> <p>If you are using Public Certificate Authority (CA) for NAI SSL Certificate</p> <p>If an organization generates certificates using a different mechanism then obtain the certificate + key and create a kubernetes secret manually using the following command:</p> <pre><code>kubectl -n istio-system create secret tls nai-cert --cert=path/to/nai.crt --key=path/to/nai.key\n</code></pre> <p>Skip the steps in this section to create a self-signed certificate resource.</p> <ol> <li> <p>Get the Ingress host using the following command:</p> <pre><code>INGRESS_HOST=$(kubectl get svc -n istio-system istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre> </li> <li> <p>Get the value of <code>INGRESS_HOST</code> environment variable</p> CommandCommand output <pre><code>echo $INGRESS_HOST\n</code></pre> <pre><code>10.x.x.216\n</code></pre> </li> <li> <p>We will use the command output e.g: <code>10.x.x.216</code> as the IP address for NAI as reserved in this section</p> </li> <li> <p>Construct the FQDN of NAI UI using nip.io and we will use this FQDN as the certificate's Common Name (CN).</p> Template URLSample URL <pre><code>nai.${INGRESS_HOST}.nip.io\n</code></pre> <pre><code>nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>Create the ingress resource certificate using the following command:</p> <pre><code>cat &lt;&lt; EOF | k apply -f -\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: nai-cert\n  namespace: istio-system\nspec:\n  issuerRef:\n    name: selfsigned-issuer\n    kind: ClusterIssuer\n  secretName: nai-cert\n  commonName: nai.${INGRESS_HOST}.nip.io\n  dnsNames:\n  - nai.${INGRESS_HOST}.nip.io\n  ipAddresses:\n  - ${INGRESS_HOST}\nEOF\n</code></pre> </li> <li> <p>Patch the ingress gateway's IP address to the certificate file.</p> CommandCommand output <pre><code>kubectl patch gateway -n knative-serving knative-ingress-gateway --type merge --patch-file=/dev/stdin &lt;&lt;EOF\nspec:\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: nai-cert\nEOF\n</code></pre> <pre><code>gateway.networking.istio.io/knative-ingress-gateway patched \n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy_2.0/#accessing-the-ui","title":"Accessing the UI","text":"<ol> <li> <p>In a browser, open the following URL to connect to the NAI UI</p> <pre><code>https://nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>Change the password for the <code>admin</code> user</p> </li> <li> <p>Login using <code>admin</code> user and password.</p> <p></p> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy_2.0/#download-model","title":"Download Model","text":"<p>We will download and user llama3 8B model which we sized for in the previous section.</p> <ol> <li>In the NAI GUI, go to Models</li> <li>Click on Import Model from Hugging Face</li> <li>Choose the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model</li> <li> <p>Input your Hugging Face token that was created in the previous section and click Import</p> </li> <li> <p>Provide the Model Instance Name as <code>Meta-Llama-3.1-8B-Instruct</code> and click Import</p> </li> <li> <p>Go to VSC Terminal to monitor the download</p> CommandCommand output <p>Get jobs in nai-admin namespace<pre><code>kubens nai-admin\n\nkubectl get jobs\n</code></pre> Validate creation of pods and PVC<pre><code>kubectl get po,pvc\n</code></pre> Verify download of model using pod logs<pre><code>kubectl logs -f _pod_associated_with_job\n</code></pre></p> <p>Get jobs in nai-admin namespace<pre><code>kubens nai-admin\n\n\u2714 Active namespace is \"nai-admin\"\n\nkubectl get jobs\n\nNAME                                       COMPLETIONS   DURATION   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-model-job   0/1           4m56s      4m56\n</code></pre> Validate creation of pods and PVC<pre><code>kubectl get po,pvc\n\nNAME                                             READY   STATUS    RESTARTS   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-model-job-9nmff   1/1     Running   0          4m49s\n\nNAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      VOLUMEATTRIBUTESCLASS   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-pvc-claim   Bound    pvc-a63d27a4-2541-4293-b680-514b8b890fe0   28Gi       RWX            nai-nfs-storage   &lt;unset&gt;                 2d\n</code></pre> Verify download of model using pod logs<pre><code>kubectl logs -f nai-c0d6ca61-1629-43d2-b57a-9f-model-job-9nmff \n\n/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.05 MB. The target location /data/model-files only has 0.00 MB free disk space.\nwarnings.warn(\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51.0k/51.0k [00:00&lt;00:00, 3.26MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.09M/9.09M [00:00&lt;00:00, 35.0MB/s]&lt;00:30, 150MB/s]\nmodel-00004-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.17G/1.17G [00:12&lt;00:00, 94.1MB/s]\nmodel-00001-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.98G/4.98G [04:23&lt;00:00, 18.9MB/s]\nmodel-00003-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.92G/4.92G [04:33&lt;00:00, 18.0MB/s]\nmodel-00002-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [04:47&lt;00:00, 17.4MB/s]\nFetching 16 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [05:42&lt;00:00, 21.43s/it]:33&lt;00:52, 9.33MB/s]\n## Successfully downloaded model_files|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [04:47&lt;00:00, 110MB/s] \n\nDeleting directory : /data/hf_cache\n</code></pre></p> </li> <li> <p>Optional - verify the events in the namespace for the pvc creation </p> CommandCommand output <pre><code>k get events | awk '{print $1, $3}'\n</code></pre> <pre><code>$ k get events | awk '{print $1, $3}'\n\n3m43s Scheduled\n3m43s SuccessfulAttachVolume\n3m36s Pulling\n3m29s Pulled\n3m29s Created\n3m29s Started\n3m43s SuccessfulCreate\n90s   Completed\n3m53s Provisioning\n3m53s ExternalProvisioning\n3m45s ProvisioningSucceeded\n3m53s PvcCreateSuccessful\n3m48s PvcNotBound\n3m43s ModelProcessorJobActive\n90s   ModelProcessorJobComplete\n</code></pre> </li> </ol> <p>The model is downloaded to the Nutanix Files <code>pvc</code> volume.</p> <p>After a successful model import, you will see it in Active status in the NAI UI under Models menu</p> <p></p>"},{"location":"airgap_nai/airgap_nai_deploy_2.0/#create-and-test-inference-endpoint","title":"Create and Test Inference Endpoint","text":"<p>In this section we will create an inference endpoint using the downloaded model.</p> <ol> <li>Navigate to Inference Endpoints menu and click on Create Endpoint button</li> <li> <p>Fill the following details:</p> <ul> <li>Endpoint Name: <code>llama-8b</code></li> <li>Model Instance Name: <code>Meta-LLaMA-8B-Instruct</code></li> <li>Use GPUs for running the models : <code>Checked</code></li> <li>No of GPUs (per instance):</li> <li>GPU Card: <code>NVIDIA-L40S</code> (or other available GPU)</li> <li>No of Instances: <code>1</code></li> <li>API Keys: Create a new API key or use an existing one</li> </ul> </li> <li> <p>Click on Create</p> </li> <li> <p>Monitor the <code>nai-admin</code> namespace to check if the services are coming up</p> CommandCommand output <pre><code>kubens nai-admin\nkubectl get po,deploy\n</code></pre> <pre><code>kubens nai-admin\nget po,deploy\nNAME                                                     READY   STATUS        RESTARTS   AGE\npod/llama8b-predictor-00001-deployment-9ffd786db-6wkzt   2/2     Running       0          71m\n\nNAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/llama8b-predictor-00001-deployment   1/1     1            0           3d17h\n</code></pre> </li> <li> <p>Check the events in the <code>nai-admin</code> namespace for resource usage to make sure all </p> CommandCommand output <pre><code>kubectl get events -n nai-admin --sort-by='.lastTimestamp' | awk '{print $1, $3, $5}'\n</code></pre> <pre><code>$ kubectl get events -n nai-admin --sort-by='.lastTimestamp' | awk '{print $1, $3, $5}'\n\n110s FinalizerUpdate Updated\n110s FinalizerUpdate Updated\n110s RevisionReady Revision\n110s ConfigurationReady Configuration\n110s LatestReadyUpdate LatestReadyRevisionName\n110s Created Created\n110s Created Created\n110s Created Created\n110s InferenceServiceReady InferenceService\n110s Created Created\n</code></pre> </li> <li> <p>Once the services are running, check the status of the inference service</p> CommandCommand output <pre><code>kubectl get isvc\n</code></pre> <pre><code>kubectl get isvc\n\nNAME      URL                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllama8b   http://llama8b.nai-admin.svc.cluster.local   True           100                              llama8b-predictor-00001   3d17h\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_deploy_2.0/#troubleshooting-endpoint-isvc","title":"Troubleshooting Endpoint ISVC","text":"<p>TGI Imange and Self-signed Certificates</p> <p>Only follow this procedure if this <code>isvc</code> is not starting up.</p> <p>KNative Serving Image Tag Checking</p> <p>From testing, we have identified that KServe module is making sure that there are no container image tag discrepencies, by pulling image using SHA digest. This is done to avoid pulling images that are updated without updating the tag.</p> <p>We have avoided this behavior by patching the <code>config-deployment</code> config map in the <code>knative-serving</code> namespace to skip image tag checking. Check this Prepare for NAI Deployment sectionfor more details.</p> <pre><code>kubectl patch configmap  config-deployment -n knative-serving --type merge -p '{\"data\":{\"registries-skipping-tag-resolving\":\"${REGISTRY_HOST}\"}'\n</code></pre> <p>If this procedure was not followed, then the <code>isvc</code> will not start up.</p> <ol> <li> <p>If the <code>isvc</code> is not coming up, then explore the events in <code>nai-admin</code> namespace.</p> CommandCommand output <pre><code>kubens nai-admin\nkubectl get isvc\nkubectl get events  --sort-by='.lastTimestamp'\n</code></pre> <pre><code>$ kubectl get isvc\n\nNAME      URL                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllama8b   http://llama8b.nai-admin.svc.cluster.local   False\n\n$ kubectl get events --sort-by='.lastTimestamp'\n\nWarning   InternalError         revision/llama8b-predictor-00001   Unable to fetch image \"harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\": failed to resolve image to digest: \nGet \"https://harbor.10.x.x.111.nip.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority\n</code></pre> <p>The temporary workaround is to use the TGI images SHA signature from the container registry.</p> <p>This site will be updated with resolutions for the above issues in the future.</p> </li> <li> <p>Note the above TGI image SHA digest from the container registry.</p> CommandCommand output <pre><code>docker pull ${REGISTRY_HOST}/nutanix/nai-tgi:${NAI_TGI_RUNTIME_VERSION}\n</code></pre> <pre><code>docker pull harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\n\n2.3.1-825f39d: Pulling from nkp/nutanix/nai-tgi\nDigest: sha256:2df9fab2cf86ab54c2e42959f23e6cfc5f2822a014d7105369aa6ddd0de33006\nStatus: Image is up to date for harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\nharbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\n</code></pre> </li> <li> <p>The SHA digest will look like the following:</p> TGI image SHA digest will be different for different environments<pre><code>sha256:2df9fab2cf86ab54c2e42959f23e6cfc5f2822a014d7105369aa6ddd0de33006\n</code></pre> </li> <li> <p>Create a copy of the <code>isvc</code> manifest</p> <pre><code>kubectl get isvc llama8b -n nai-admin -o yaml &gt; llama8b.yaml\n</code></pre> </li> <li> <p>Edit the <code>isvc</code></p> <pre><code>kubectl edit isvc llama8b -n nai-admin\n</code></pre> </li> <li> <p>Search and replace the <code>image</code> tag with the SHA digest from the TGI image.</p> <p><pre><code>&lt;snip&gt;\n\nenv:\n- name: STORAGE_URI\n  value: pvc://nai-c34d8d58-d6f8-4cb4-94e4-28-pvc-claim/model-files\n  image: harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi:2.3.1-825f39d\n\n&lt;snip&gt;\n</code></pre> 7. After replacing the image's SHA digest, the image value should look as follows: </p> <pre><code>&lt;snip&gt;\n\nenv:\n- name: STORAGE_URI\n  value: pvc://nai-c34d8d58-d6f8-4cb4-94e4-28-pvc-claim/model-files\n  image: harbor.10.x.x.111.nip.io/nkp/nutanix/nai-tgi@sha256:2df9fab2cf86ab54c2e42959f23e6cfc5f2822a014d7105369aa6ddd0de33006\n\n&lt;snip&gt;\n</code></pre> </li> <li> <p>Save the <code>isvc</code> configuration by writing the changes to the file and exiting the vi editor using <code>:wq!</code> key combination.</p> </li> <li> <p>Verify that the <code>isvc</code> is running</p> CommandCommand output <pre><code>kubens nai-admin\nkubectl get isvc\n</code></pre> <pre><code>$ kubectl get isvc\n\nNAME      URL                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllama8b   http://llama8b.nai-admin.svc.cluster.local   True           100                              llama8b-predictor-00001   3d17h\n</code></pre> </li> </ol> <p>This should resolve the issue the issue with the TGI image.</p> <p>Report Other Issues</p> <p>If you are facing any other issues, please report them here in the NAI LLM GitHub Repo Issues page.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs/","title":"Pre-requisites for Deploying NAI","text":"<p>In this part of the lab we will prepare pre-requisites for LLM application on GPU nodes.</p> <p>The following is the flow of the applications lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state PreRequisites {\n        [*] --&gt; CreateFilesShare  \n        CreateFilesShare --&gt; PrepareHuggingFace\n        PrepareHuggingFace --&gt; [*]\n    }\n    state CreateOfflineHelmContainers {\n        [*] --&gt; PrepareNAIHelmCharts\n        PrepareNAIHelmCharts --&gt; PrepareNAIContainerImages\n        PrepareNAIContainerImages --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; CreateOfflineHelmContainers\n    CreateOfflineHelmContainers --&gt; DeployNAI : next section\n    DeployNAI --&gt; TestNAI\n    TestNAI --&gt; [*]</code></pre> <p>Prepare the following pre-requisites needed to deploy NAI on target kubernetes cluster.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs/#create-nutanix-files-storage-class","title":"Create Nutanix Files Storage Class","text":"<p>We will create Nutanix Files storage class which will be used to create a pvc that will store the <code>LLama-3-8B</code> model files.</p> <ol> <li>In Prism Central, choose Files from the menu</li> <li>Choose the file server (e.g. labFS)</li> <li>Click on Shares &amp; Exports</li> <li>Click on +New Share or Export</li> <li> <p>Fill the details of the Share</p> <ul> <li>Name - model_share</li> <li>Description - for NAI model store</li> <li>Share path - leave blank</li> <li>Max Size - 10 GiB (adjust to the model file size)</li> <li>Primary Protocol Access - NFS</li> </ul> </li> <li> <p>Click Next and make sure Enable compression in checked</p> </li> <li>Click Next </li> <li> <p>In NFS Protocol Access, choose the following: </p> <ul> <li>Authentication - System</li> <li>Default Access (for all clients) - Read-Write </li> <li>Squash - Root Squash</li> </ul> <p>Note</p> <p>Consider changing access options for Production environment</p> </li> <li> <p>Click Next</p> </li> <li>Confirm the share details and click on Create</li> </ol>"},{"location":"airgap_nai/airgap_nai_pre_reqs/#create-the-files-storage-class","title":"Create the Files Storage Class","text":"<ol> <li> <p>Run the following command to check K8S status of the <code>nkpdev</code> cluster</p> <pre><code>kubectl get nodes\n</code></pre> </li> <li> <p>In VSC Explorer, click on New File  and create a config file with the following name:</p> <pre><code>nai-nfs-storage.yaml\n</code></pre> <p>Add the following content and replace the <code>nfsServerName</code> with the name of the Nutanix Files server name .</p> <p></p> Template YAMLSample YAML <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nai-nfs-storage\nparameters:\n  nfsPath: &lt;nfs-path&gt;\n  nfsServer: &lt;nfs-server&gt;\n  storageType: NutanixFiles\nprovisioner: csi.nutanix.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nai-nfs-storage\nparameters:\n  nfsPath: /model_share\n  nfsServer: labFS.ntnxlab.local\n  storageType: NutanixFiles\nprovisioner: csi.nutanix.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n</code></pre> </li> <li> <p>Create the storage class</p> <pre><code>kubectl apply -f nai-nfs-storage.yaml\n</code></pre> </li> <li> <p>Check storage classes in the cluster for the Nutanix Files storage class</p> CommandCommand output <pre><code>kubectl get storageclass\n</code></pre> <pre><code>kubectl get storageclass\n\nNAME                       PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ndkp-object-store           kommander.ceph.rook.io/bucket   Delete          Immediate              false                  28h\nnai-nfs-storage            csi.nutanix.com                 Delete          Immediate              true                   24h\nnutanix-volume (default)   csi.nutanix.com                 Delete          WaitForFirstConsumer   false                  28h\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_pre_reqs/#request-access-to-model-on-hugging-face","title":"Request Access to Model on Hugging Face","text":"<p>Follow these steps to request access to the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model:</p> <p>LLM Recommendation</p> <p>From testing <code>google/gemma-2-2b-it</code> model is quicker to download and obtain download rights, than <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model.</p> <p>Feel free to use the google/gemma-2-2b-it model if necessary. The procedure to request access to the model is the same.</p> <ol> <li> <p>Sign in to your Hugging Face account:  </p> <ul> <li>Visit Hugging Face and log in to your account.</li> </ul> </li> <li> <p>Navigate to the model page:  </p> <ul> <li>Go to the Meta-Llama-3.1-8B-Instruct model page.</li> </ul> </li> <li> <p>Request access:</p> <ul> <li>On the model page, you will see a section or button labeled Request Access (this is usually near the top of the page or near the \"Files and versions\" section).</li> <li>Click Request Access.</li> </ul> </li> <li> <p>Complete the form:</p> <ul> <li>You may be prompted to fill out a form or provide additional details about your intended use of the model.</li> <li>Complete the required fields and submit the request.</li> </ul> </li> <li> <p>Wait for approval:</p> <ul> <li>After submitting your request, you will receive a notification or email once your access is granted.</li> <li>This process can take some time depending on the approval workflow.</li> </ul> </li> </ol> <p>Once access is granted, there will be an email notification.</p> <p>Note</p> <p>Email from Hugging Face can take a few minutes or hours before it arrives.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs/#create-a-hugging-face-token-with-read-permissions","title":"Create a Hugging Face Token with Read Permissions","text":"<p>Follow these steps to create a Hugging Face token with read permissions:</p> <ol> <li> <p>Sign in to your Hugging Face account:  </p> <ul> <li>Visit Hugging Face and log in to your account.</li> </ul> </li> <li> <p>Access your account settings:</p> <ul> <li>Click on your profile picture in the top-right corner.</li> <li>From the dropdown, select Settings.</li> </ul> </li> <li> <p>Navigate to the \"Access Tokens\" section:</p> <ul> <li>In the sidebar, click on Access Tokens.</li> <li>You will see a page where you can create and manage tokens.</li> </ul> </li> <li> <p>Create a new token:</p> <ul> <li>Click the New token button.</li> <li>Enter a name for your token (i.e., <code>read-only-token</code>).</li> </ul> </li> <li> <p>Set token permissions:</p> <ul> <li>Under the permissions dropdown, select Read. For Example:     </li> </ul> </li> <li> <p>Create and copy the token:</p> <ul> <li>After selecting the permissions, click Create.</li> <li>Your token will be generated and displayed only once, so make sure to copy it and store it securely.</li> </ul> </li> </ol> <p>Use this token for accessing Hugging Face resources with read-only permissions.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs/#prepare-helm-charts","title":"Prepare Helm Charts","text":"<p>In this section we will prepare the helm charts necessary for NAI and pre-requisite applications install</p> <ul> <li>NAI</li> <li>Envoy Gateway</li> <li>Kserve</li> </ul> <p>The procedure will be done on the jumphost VM.</p> <ol> <li> <p>Login to Nutanix Portal using your credentials</p> </li> <li> <p>Go to Downloads &gt; NAI Airgapped Bundle</p> </li> <li> <p>Download and extract the NAI air-gap <code>helm</code> bundle from the link you copied earlier</p> CommandSample command Paste the download URL within double quotes<pre><code>curl -o nai-core-2.4.0.tgz \"_paste_download_URL_here\"\n</code></pre> <pre><code>curl -o nai-core-2.4.0.tgz \"https://download.nutanix.com/downloads/nai/2.4.0/nai-core-2.4.0.tgz?........\"\n</code></pre> <ol> <li>Open new <code>VSCode</code> window on your jumphost VM</li> </ol> </li> <li> <p>In <code>VSCode</code> Explorer pane, click on existing <code>$HOME</code> folder</p> </li> <li> <p>Click on New Folder  name it: <code>airgap-nai</code></p> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>$HOME/airgap-nai</code> folder</p> </li> <li> <p>On <code>VSCode</code> menu, select <code>Terminal</code> &gt; <code>New Terminal</code></p> </li> <li> <p>Browse to <code>airgap-nai</code> directory</p> <pre><code>cd $HOME/airgap-nai\n</code></pre> </li> <li> <p>In <code>VSC</code>, under the newly created <code>airgap-nai</code> folder, click on New File  and create file with the following name:</p> <pre><code>.env\n</code></pre> </li> <li> <p>Add (append) the following environment variables and save it</p> Where to get associated container versions for NAI <code>v2.4.0</code>? <p>Extract the  downloaded<code>nai-core-2.4.0.tgz</code> file.</p> <pre><code>tar -xvfz nai-core-2.4.0.tgz\n</code></pre> <p>Under the extracted folder path <code>$PWD/nai-core/values.yaml</code> file, we will be able to see all the associated container images and their tags for NAI <code>v2.4.0</code></p> <pre><code>cat nai-core/values.yaml\n</code></pre> Template .env.env for NAI v2.4.0 <pre><code>export KSERVE_VERSION=_your_kserve_version\nexport ENVOY_GATEWAY_VERSION=_your_envoy_gateway_version\nexport NAI_CORE_VERSION=_your_nai_core_version\nexport NAI_API_VERSION=_your_nai_api_version\nexport NAI_TGI_RUNTIME_VERSION=_your_tgi_version\nexport NAI_PROMETHEUS_VERSION=_your_prometheus_version\nexport NAI_POSTGRESQL_VERSION=_your_postgres_version\nexport REGISTRY_HOST=harbor.10.x.x.111.nip.io/nkp\nexport REGISTRY_USERNAME=admin\nexport REGISTRY_CACERT=_path_to_ca_cert_of_registry  # (1)!\n</code></pre> <ol> <li>File must contain CA server and Harbor server's public certificate in one file</li> </ol> <pre><code>export KSERVE_VERSION=v0.15.0\nexport ENVOY_GATEWAY_VERSION=v1.5.0\nexport NAI_CORE_VERSION=2.4.0\nexport NAI_API_VERSION=v2.4.0\nexport NAI_TGI_RUNTIME_VERSION=\"3.3.4-b2485c9\"\nexport NAI_PROMETHEUS_VERSION=v2.54.0\nexport NAI_POSTGRESQL_VERSION=16.1-alpine\nexport REGISTRY_USERNAME=admin\nexport REGISTRY_PASSWORD=xxxxxxx\nexport REGISTRY_CACERT=$HOME/harbor/certs/full_chain.pem  # (1)!\n</code></pre> <ol> <li>File must contain CA server and Harbor server's public certificate in one file</li> </ol> </li> <li> <p>Source the <code>.env</code> file to import environment variables</p> <pre><code>source $HOME/airgap-nai/.env\n</code></pre> </li> <li> <p>Pull the Kserve and Envoy Gateway helm charts using the following commands</p> CommandCommand output <pre><code>helm pull oci://docker.io/envoyproxy/gateway-helm --version ${ENVOY_GATEWAY_VERSION}\n\nhelm pull oci://ghcr.io/kserve/charts/kserve-crd --version ${KSERVE_VERSION}\n\nhelm pull oci://ghcr.io/kserve/charts/kserve --version ${KSERVE_VERSION}\n</code></pre> <pre><code>Pulled: docker.io/envoyproxy/gateway-helm:v1.5.0\nDigest: sha256:a3dddd41ec3c58eae1b77dabe1f298bf92123fda6cac6f4940c23a11fc43c583\n\nPulled: ghcr.io/kserve/charts/kserve-crd:v0.14.0\nDigest: sha256:6ae5af970d9a9400e8456ad1dbc86360d03f4b6bb00be4f16c48bc0542283d42\n\nPulled: ghcr.io/kserve/charts/kserve:v0.14.0\nDigest: sha256:25129d39a4aa85f96159db6933729ea9f35e9d0f7f7cac7918c0a8013672eccb\n</code></pre> </li> <li> <p>Login to Harbor registry on the command line (if not done so)</p> <pre><code>helm registry login harbor.10.x.x.111.nip.io --username ${REGISTRY_USERNAME} --password ${REGISTRY_PASSWORD}\n</code></pre> </li> <li> <p>Upload the downloaded and prepared helm charts to Harbor</p> <pre><code># Push charts\nhelm push gateway-helm-v1.5.0.tgz oci://${REGISTRY_HOST}\nhelm push kserve-crd-v0.15.0.tgz oci://${REGISTRY_HOST}\nhelm push kserve-v0.15.0.tgz oci://${REGISTRY_HOST}\nhelm push nai-core-2.4.0.tgz oci://${REGISTRY_HOST}\n</code></pre> </li> <li> <p>Download the NAI air-gap binaries (NAI container images) from the link you copied earlier</p> CommandSample command Paste the download URL within double quotes<pre><code>curl -o nai-2.4.0.tar \"_paste_download_URL_here\"\n</code></pre> This download is about 44 GBs<pre><code>curl -o nai-2.4.0.tar \"https://download.nutanix.com/downloads/nai/2.4.0/nai-2.4.0.tar?...\"\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_pre_reqs/#prepare-container-images","title":"Prepare Container Images","text":"<p>The Jumphost VM will be used as a medium to download the NAI container images and upload them to the internal Harbor container registry.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state LoginToNutanixPortal {\n        [*] --&gt; CreateDockerIDandAccessToken\n        CreateDockerIDandAccessToken --&gt; LoginToDockerCLI\n        LoginToDockerCLI --&gt; [*]\n    }\n\n    state PrepareNAIDockerImages {\n        [*] --&gt; DownloadUploadImagesToHarbor\n        DownloadUploadImagesToHarbor --&gt; [*]\n    }\n\n    [*] --&gt; LoginToNutanixPortal\n    LoginToNutanixPortal --&gt; PrepareNAIDockerImages\n    PrepareNAIDockerImages --&gt; [*]</code></pre>"},{"location":"airgap_nai/airgap_nai_pre_reqs/#upload-nai-docker-images-to-harbor","title":"Upload NAI Docker Images to Harbor","text":"<p>Info</p> <p>The download and upload of the container images will be done in one <code>docker push</code> command which will use the internal Harbor container registry details.</p> <p>This will be a two-step process.</p> <ol> <li>Upload the container images from the downloaded <code>nai-2.x.x.tar</code> to the jumphost VM local docker images store</li> <li>Upload it to the internal Harbor container registry </li> </ol> <ol> <li> <p>Since we will be using the same internal Harbor container registry to upload container images, make sure the following environment variables are set (these were already set during air-gap NKP preparation)</p> <pre><code>export REGISTRY_HOST=harbor.10.x.x.111.nip.io/nkp\nexport REGISTRY_USERNAME=admin\nexport REGISTRY_PASSWORD=xxxxxxx\nexport REGISTRY_CACERT=$HOME/harbor/certs/ca.crt\n</code></pre> </li> <li> <p>(Optional) - To view the container images loaded in your local docker container registry, run the following command:</p> CommandOutput <pre><code>docker images --format '{{.Repository}}:{{.Tag}}' | grep nai\n</code></pre> <pre><code>nutanix/nai-api:v2.4.0\nnutanix/nai-inference-ui:v2.4.0\nnutanix/nai-model-processor:v2.4.0\nnutanix/nai-iep-operator:v2.4.0\nnutanix/nai-tgi:3.3.4-b2485c9\nnutanix/nai-kserve-huggingfaceserver:v0.15.2\nnutanix/nai-kserve-huggingfaceserver:v0.15.2\nnutanix/nai-kserve-huggingfaceserver:v0.15.2-gpu\nnutanix/nai-kserve-controller:v0.15.0\nnutanix/nai-postgres:16.1-alpine\n</code></pre> </li> <li> <p>Push the images to the jumphost VM local docker images store</p> <pre><code>docker image load -i nai-2.4.0.tar\n</code></pre> </li> <li> <p>Login to the internal Harbor registy if the harbor project needs authentication</p> <pre><code>docker login harbor.10.x.x.111.nip.io\n</code></pre> </li> <li> <p>Tag and push all the NAI images to refer to the internal harbor registry</p> <pre><code>for image in $(docker images --format '{{.Repository}}:{{.Tag}}' | grep 'nai' | grep -v $${REGISTRY_HOST}); do\n  docker tag $image $${REGISTRY_HOST}/$(echo $image);\n  docker push $${REGISTRY_HOST}/$(echo $image);\ndone\n</code></pre> </li> <li> <p>Download and push the Envoy Gateway, Kserve and Prometheus container images from the jumphost VM to harbor container registry.</p> <pre><code>for image in docker.io/envoyproxy/gateway-controller:${ENVOY_GATEWAY_VERSION} \\\nkserve/kserve-controller:${KSERVE_VERSION} \\\nquay.io/prometheus/prometheus:${NAI_PROMETHEUS_VERSION}; do\n  docker tag $image $${REGISTRY_HOST}/$(echo $image | sed 's|.*/||')\n  docker push $${REGISTRY_HOST}/$(echo $image | sed 's|.*/||')\ndone\n</code></pre> </li> </ol> <p>Now we are ready to deploy our NAI workloads.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/","title":"Pre-requisites for Deploying NAI","text":"<p>In this part of the lab we will prepare pre-requisites for LLM application on GPU nodes.</p> <p>The following is the flow of the applications lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state PreRequisites {\n        [*] --&gt; CreateFilesShare  \n        CreateFilesShare --&gt; PrepareHuggingFace\n        PrepareHuggingFace --&gt; [*]\n    }\n    state CreateOfflineBundle {\n        [*] --&gt; PrepareNAIHelmCharts\n        PrepareNAIHelmCharts --&gt; PrepareNAIContainerImages\n        PrepareNAIContainerImages --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; CreateOfflineBundle\n    CreateOfflineBundle --&gt; DeployNAI : next section\n    DeployNAI --&gt; TestNAI\n    TestNAI --&gt; [*]</code></pre> <p>Prepare the following pre-requisites needed to deploy NAI on target kubernetes cluster.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/#create-nutanix-files-storage-class","title":"Create Nutanix Files Storage Class","text":"<p>We will create Nutanix Files storage class which will be used to create a pvc that will store the <code>LLama-3-8B</code> model files.</p> <ol> <li>In Prism Central, choose Files from the menu</li> <li>Choose the file server (e.g. labFS)</li> <li>Click on Shares &amp; Exports</li> <li>Click on +New Share or Export</li> <li> <p>Fill the details of the Share</p> <ul> <li>Name - model_share</li> <li>Description - for NAI model store</li> <li>Share path - leave blank</li> <li>Max Size - 10 GiB (adjust to the model file size)</li> <li>Primary Protocol Access - NFS</li> </ul> </li> <li> <p>Click Next and make sure Enable compression in checked</p> </li> <li>Click Next </li> <li> <p>In NFS Protocol Access, choose the following: </p> <ul> <li>Authentication - System</li> <li>Default Access (for all clients) - Read-Write </li> <li>Squash - Root Squash</li> </ul> <p>Note</p> <p>Consider changing access options for Production environment</p> </li> <li> <p>Click Next</p> </li> <li>Confirm the share details and click on Create</li> </ol>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/#create-the-files-storage-class","title":"Create the Files Storage Class","text":"<ol> <li> <p>Run the following command to check K8S status of the <code>nkpdev</code> cluster</p> <pre><code>kubectl get nodes\n</code></pre> </li> <li> <p>In VSC Explorer, click on New File  and create a config file with the following name:</p> <pre><code>nai-nfs-storage.yaml\n</code></pre> <p>Add the following content and replace the <code>nfsServerName</code> with the name of the Nutanix Files server name .</p> <p></p> Template YAMLSample YAML <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nai-nfs-storage\nparameters:\n  nfsPath: &lt;nfs-path&gt;\n  nfsServer: &lt;nfs-server&gt;\n  storageType: NutanixFiles\nprovisioner: csi.nutanix.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nai-nfs-storage\nparameters:\n  nfsPath: /model_share\n  nfsServer: labFS.ntnxlab.local\n  storageType: NutanixFiles\nprovisioner: csi.nutanix.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n</code></pre> </li> <li> <p>Create the storage class</p> <pre><code>kubectl apply -f nai-nfs-storage.yaml\n</code></pre> </li> <li> <p>Check storage classes in the cluster for the Nutanix Files storage class</p> CommandCommand output <pre><code>kubectl get storageclass\n</code></pre> <pre><code>kubectl get storageclass\n\nNAME                       PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ndkp-object-store           kommander.ceph.rook.io/bucket   Delete          Immediate              false                  28h\nnai-nfs-storage            csi.nutanix.com                 Delete          Immediate              true                   24h\nnutanix-volume (default)   csi.nutanix.com                 Delete          WaitForFirstConsumer   false                  28h\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/#request-access-to-model-on-hugging-face","title":"Request Access to Model on Hugging Face","text":"<p>Follow these steps to request access to the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model:</p> <p>LLM Recommendation</p> <p>From testing <code>google/gemma-2-2b-it</code> model is quicker to download and obtain download rights, than <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model.</p> <p>Feel free to use the google/gemma-2-2b-it model if necessary. The procedure to request access to the model is the same.</p> <ol> <li> <p>Sign in to your Hugging Face account:  </p> <ul> <li>Visit Hugging Face and log in to your account.</li> </ul> </li> <li> <p>Navigate to the model page:  </p> <ul> <li>Go to the Meta-Llama-3.1-8B-Instruct model page.</li> </ul> </li> <li> <p>Request access:</p> <ul> <li>On the model page, you will see a section or button labeled Request Access (this is usually near the top of the page or near the \"Files and versions\" section).</li> <li>Click Request Access.</li> </ul> </li> <li> <p>Complete the form:</p> <ul> <li>You may be prompted to fill out a form or provide additional details about your intended use of the model.</li> <li>Complete the required fields and submit the request.</li> </ul> </li> <li> <p>Wait for approval:</p> <ul> <li>After submitting your request, you will receive a notification or email once your access is granted.</li> <li>This process can take some time depending on the approval workflow.</li> </ul> </li> </ol> <p>Once access is granted, there will be an email notification.</p> <p>Note</p> <p>Email from Hugging Face can take a few minutes or hours before it arrives.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/#create-a-hugging-face-token-with-read-permissions","title":"Create a Hugging Face Token with Read Permissions","text":"<p>Follow these steps to create a Hugging Face token with read permissions:</p> <ol> <li> <p>Sign in to your Hugging Face account:  </p> <ul> <li>Visit Hugging Face and log in to your account.</li> </ul> </li> <li> <p>Access your account settings:</p> <ul> <li>Click on your profile picture in the top-right corner.</li> <li>From the dropdown, select Settings.</li> </ul> </li> <li> <p>Navigate to the \"Access Tokens\" section:</p> <ul> <li>In the sidebar, click on Access Tokens.</li> <li>You will see a page where you can create and manage tokens.</li> </ul> </li> <li> <p>Create a new token:</p> <ul> <li>Click the New token button.</li> <li>Enter a name for your token (i.e., <code>read-only-token</code>).</li> </ul> </li> <li> <p>Set token permissions:</p> <ul> <li>Under the permissions dropdown, select Read. For Example:     </li> </ul> </li> <li> <p>Create and copy the token:</p> <ul> <li>After selecting the permissions, click Create.</li> <li>Your token will be generated and displayed only once, so make sure to copy it and store it securely.</li> </ul> </li> </ol> <p>Use this token for accessing Hugging Face resources with read-only permissions.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/#prepare-helm-charts-and-generate-bundle","title":"Prepare Helm Charts and Generate Bundle","text":"<ol> <li>Login to the Jumphost VM</li> <li>In VSC Explorer, click on New Folder  and name it: <code>airgap-nai</code></li> <li> <p>In <code>VSC</code>, under the newly created <code>airgap-nai</code> folder, click on New File  and create file with the following name:</p> <pre><code>.env\n</code></pre> </li> <li> <p>Add (append) the following environment variables and save it</p> Template .env.env for NAI v2.0.0 <pre><code>export ISTIO_VERSION=_your_istio_version\nexport KNATIVE_VERSION=_your_knative_version\nexport KSERVE_VERSION=_your_kserve_version\nexport KUBE_PROMETHEUS_STACK_VERSION=_your_kube_prometheus_stack_version\nexport NAI_CORE_VERSION=_your_nai_core_version\nexport NAI_API_VERSION=_your_nai_api_version\nexport NAI_PROMETHEUS_VERSION=_your_nai_prometheus_version\nexport NAI_POSTGRESQL_VERSION=_your_nai_postgresql_version\nexport NAI_KSERVE_HF_SERVER_VERSION=_your_nai_kserve_hf_server_version\nexport NAI_TGI_RUNTIME_VERSION=_your_nai_tgi_runtime_version\n</code></pre> <pre><code>export ISTIO_VERSION=1.20.8\nexport KNATIVE_VERSION=1.13.1\nexport KSERVE_VERSION=v0.14.0\nexport KUBE_PROMETHEUS_STACK_VERSION=61.3.1\nexport NAI_CORE_VERSION=v2.0.0\nexport NAI_API_VERSION=v2.0.0\nexport NAI_PROMETHEUS_VERSION=v2.54.0\nexport NAI_POSTGRESQL_VERSION=16.1-alpine\nexport NAI_KSERVE_HF_SERVER_VERSION=v0.14.0\nexport NAI_TGI_RUNTIME_VERSION=2.3.1-825f39d\n</code></pre> </li> <li> <p>Create a new directory to store the custom helm charts</p> <pre><code>mkdir custom-charts &amp;&amp; cd custom-charts\n</code></pre> </li> <li> <p>Fetch the helm charts using the following commands</p> CommandCommand output <pre><code>helm fetch base --repo https://istio-release.storage.googleapis.com/charts --version=${ISTIO_VERSION}\n\nhelm fetch istiod --repo https://istio-release.storage.googleapis.com/charts --version=${ISTIO_VERSION}\n\nhelm fetch gateway --repo https://istio-release.storage.googleapis.com/charts --version=${ISTIO_VERSION}\n\nhelm fetch nai-knative-serving-crds --repo https://nutanix.github.io/helm-releases --version=${KNATIVE_VERSION}\n\nhelm fetch nai-knative-serving --repo https://nutanix.github.io/helm-releases --version=${KNATIVE_VERSION}\n\nhelm fetch nai-knative-istio-controller  --repo https://nutanix.github.io/helm-releases --version=${KNATIVE_VERSION}\n\nhelm fetch oci://ghcr.io/kserve/charts/kserve-crd --version=${KSERVE_VERSION}\n\nhelm fetch oci://ghcr.io/kserve/charts/kserve --version=${KSERVE_VERSION}\n\nhelm fetch nai-core --repo https://nutanix.github.io/helm-releases --version=${NAI_CORE_VERSION}\n</code></pre> <pre><code>Pulled: ghcr.io/kserve/charts/kserve-crd:v0.14.0\nDigest: sha256:6ae5af970d9a9400e8456ad1dbc86360d03f4b6bb00be4f16c48bc0542283d42\n\nPulled: ghcr.io/kserve/charts/kserve:v0.14.0\nDigest: sha256:25129d39a4aa85f96159db6933729ea9f35e9d0f7f7cac7918c0a8013672eccb\n</code></pre> </li> <li> <p>Create a tar ball of the helm charts and compress it</p> <pre><code>tar -czvf nai-iep-chartbundle.tar.gz *.tgz\n</code></pre> </li> <li> <p>Upload the helm charts to ChartMuseum instance deployed automatically by Kommander. This will make this consistent with other NKP catalog items and will work seamlessly when this is converted to a catalog item.</p> <pre><code>nkp push chart-bundle nai-iep-chartbundle.tar.gz\n</code></pre> </li> </ol>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/#prepare-nai-container-images","title":"Prepare NAI Container Images","text":"<p>The Jumphost VM will be used as a medium to download the NAI container images and upload them to the internal Harbor container registry.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state LoginToNutanixPortal {\n        [*] --&gt; CreateDockerIDandAccessToken\n        CreateDockerIDandAccessToken --&gt; LoginToDockerCLI\n        LoginToDockerCLI --&gt; [*]\n    }\n\n    state PrepareNAIDockerImages {\n        [*] --&gt; DownloadUploadImagesToHarbor\n        DownloadUploadImagesToHarbor --&gt; [*]\n    }\n\n    [*] --&gt; LoginToNutanixPortal\n    LoginToNutanixPortal --&gt; PrepareNAIDockerImages\n    PrepareNAIDockerImages --&gt; [*]</code></pre>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/#prepare-nai-docker-download-credentials","title":"Prepare NAI Docker Download Credentials","text":"<p>All NAI Docker images will be downloaded from the public Docker Hub registry. In order to download the images, you will need to logon to Nutanix Portal - NAI and create a Docker ID and access token.</p> <ol> <li>Login to Nutanix Portal - NAI using your credentials</li> <li>Click on Generate Access Token option</li> <li>Copy the generated Docker ID and access token</li> <li> <p>Login to the Docker CLI on your jumphost VM</p> CommandCommand output <pre><code>docker login --username ntnxsvcgpt -p _docker_id_and_access_token_\n</code></pre> <pre><code>docker login --username ntnxsvcgpt -p dckr_pat_xxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre> </li> </ol> <p>Warning</p> <p>Currently there are issues with the Nutanix Portal to create a Docker ID and access token. This will be fixed soon.</p> <p>Click on the Manage Access Token option and use the credentials listed there until the Nutanix Portal is fixed.</p>"},{"location":"airgap_nai/airgap_nai_pre_reqs_2.0/#upload-nai-docker-images-to-harbor","title":"Upload NAI Docker Images to Harbor","text":"<p>Info</p> <p>The download and upload of the container images will be done in one <code>nkp</code> command which will use the internal Harbor container registry details.</p> <p><code>nkp</code> command will do this in a three-step process.</p> <ol> <li>Download the container images to the jumphost VM</li> <li>Create a tar ball of the container images and </li> <li>Upload it to the internal Harbor container registry</li> </ol> <ol> <li> <p>Create images yaml file for all the required container images</p> </li> <li> <p>Change to <code>$HOME/airgap-nai</code> directory if you are not already there</p> <pre><code>cd $HOME/airgap-nai\n</code></pre> </li> <li> <p>In VSC Explorer, click on New File  and create file with the following name:</p> <pre><code>nai-prereq-images.yaml\n</code></pre> <p>with the following content:</p> <pre><code>docker.io/istio/proxyv2:1.20.8\ndocker.io/istio/pilot:1.20.8\ndocker.io/library/busybox:1.28\nk8s.gcr.io/cuda-vector-add:v0.1\ngcr.io/knative-releases/knative.dev/serving/cmd/activator:v1.13.1\ngcr.io/knative-releases/knative.dev/serving/cmd/autoscaler:v1.13.1\ngcr.io/knative-releases/knative.dev/serving/cmd/controller:v1.13.1\ngcr.io/knative-releases/knative.dev/serving/cmd/webhook:v1.13.1\ngcr.io/knative-releases/knative.dev/serving/cmd/queue:v1.13.1\ngcr.io/knative-releases/knative.dev/net-istio/cmd/controller:v1.13.1\ngcr.io/knative-releases/knative.dev/net-istio/cmd/webhook:v1.13.1\nghcr.io/mesosphere/dkp-container-images/nvcr.io/nvidia/cloud-native/gpu-operator-validator:v24.3.0-d2iq.0\nghcr.io/mesosphere/dkp-container-images/nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8-d2iq.0\nharbor.infrastructure.cloudnative.nvdlab.net/nvidia/driver:535.161.08-ubuntu22.04\nnvcr.io/nvidia/cloud-native/dcgm:3.3.5-1-ubuntu22.04\nnvcr.io/nvidia/cloud-native/k8s-driver-manager:v0.6.8\nnvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8\nnvcr.io/nvidia/k8s/container-toolkit:v1.15.0-ubuntu20.04\nnvcr.io/nvidia/k8s/dcgm-exporter:3.3.5-3.4.1-ubuntu22.04\ngcr.io/kubebuilder/kube-rbac-proxy:v0.13.1\ndocker.io/kserve/kserve-controller:v0.13.1\ndocker.io/kserve/modelmesh-controller:v0.12.0-rc0\n</code></pre> </li> <li> <p>In VSC Explorer, click on New File  and create file with the following name:</p> <pre><code>nai-core-images.yaml\n</code></pre> <p>with the following content:</p> <pre><code>docker.io/nutanix/nai-iep-operator:v2.0.0\ndocker.io/nutanix/nai-model-processor:v2.0.0\ndocker.io/nutanix/nai-inference-ui:v2.0.0\ndocker.io/nutanix/nai-api:v2.0.0\ndocker.io/nutanix/nai-postgres:16.1-alpine\ndocker.io/nutanix/nai-kserve-controller:v0.13.1\nnutanix/nai-kserve-huggingfaceserver:v0.13.1\nnutanix/nai-tgi:2.3.1-825f39d\nquay.io/karbon/prometheus:v2.41.0\nquay.io/prometheus-operator/prometheus-config-reloader:v0.74.0\n</code></pre> </li> <li> <p>Create image bundle with the following command</p> <pre><code>nkp create image-bundle --images-file nai-prereq-images.yaml --output-file nai-prereq-images.tar --overwrite\n\nnkp create image-bundle --images-file nai-core-images.yaml --output-file nai-core-images.tar --overwrite\n</code></pre> </li> <li> <p>Since we will be using the same internal Harbor container registry to upload container images, make sure the following environment variables are set (these were already set during air-gap NKP preparation)</p> <pre><code>export REGISTRY_URL=harbor.10.x.x.111.nip.io/nkp\nexport REGISTRY_USERNAME=admin\nexport REGISTRY_PASSWORD=xxxxxxx\nexport REGISTRY_CACERT=$HOME/harbor/certs/ca.crt\n</code></pre> </li> <li> <p>Push the images to the internal Harbor container registry</p> CommandCommand output <p><pre><code>nkp push bundle --bundle nai-prereq-images.tar --to-registry ${REGISTRY_URL} \\\n--to-registry-username ${REGISTRY_USERNAME} --to-registry-password ${REGISTRY_PASSWORD} \\ \n--to-registry-ca-cert-file ${REGISTRY_CACERT}  \n</code></pre> <pre><code>nkp push bundle --bundle nai-core-images.tar --to-registry ${REGISTRY_URL} \\ \n--to-registry-username ${REGISTRY_USERNAME} --to-registry-password ${REGISTRY_PASSWORD} \\ \n--to-registry-ca-cert-file ${REGISTRY_CACERT}\n</code></pre></p> <p><pre><code>\u2713 Creating temporary directory\n\u2713 Unarchiving image bundle \"nai-prereq-images.tar\" \n\u2713 Parsing image bundle config\n\u2713 Starting temporary Docker registry\n\u2713 Pushing bundled images [================================&gt;24/24] (time elapsed 153s) \n</code></pre> <pre><code>\u2713 Creating temporary directory\n\u2713 Unarchiving image bundle \"nai-core-images.tar\" \n\u2713 Parsing image bundle config\n\u2713 Starting temporary Docker registry\n\u2713 Pushing bundled images [================================&gt;10/10] (time elapsed 25s)\n</code></pre></p> </li> </ol> <p>Now we are ready to deploy our AI workloads.</p>"},{"location":"airgap_nai/airgap_nai_test/","title":"Deploying GPT-in-a-Box NVD Reference Application using GitOps (FluxCD)","text":"<pre><code>stateDiagram-v2\n    direction LR\n\n    state TestNAI {\n        [*] --&gt; CheckInferencingService\n        CheckInferencingService --&gt;  TestChatApp\n        TestChatApp --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI \n    DeployNAI --&gt; TestNAI : previous section\n    TestNAI --&gt; [*]</code></pre>"},{"location":"airgap_nai/airgap_nai_test/#test-querying-inference-service-api","title":"Test Querying Inference Service API","text":"<ol> <li> <p>Prepare the API key that was created in the previous section</p> Template commandSample command <pre><code>export API_KEY=_your_endpoint_api_key\n</code></pre> <pre><code>export API_KEY=5840a693-254d-41ef-a2d3-1xxxxxxxxxx\n</code></pre> </li> <li> <p>Construct your <code>curl</code> command using the API key obtained above, and run it on the terminal</p> CommandCommand output <pre><code>curl -k -X 'POST' 'https://nai.10.x.x.216.nip.io/api/v1/chat/completions' \\\n-H \"Authorization: Bearer $API_KEY\" \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model\": \"llama-8b\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": \"What is the capital of France?\"\n        }\n    ],\n    \"max_tokens\": 256,\n    \"stream\": false\n}'\n</code></pre> <pre><code>{\n    \"id\": \"9e55abd1-2c91-4dfc-bd04-5db78f65c8b2\",\n    \"object\": \"chat.completion\",\n    \"created\": 1728966493,\n    \"model\": \"llama-8b\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"The capital of France is Paris. It is a historic city on the Seine River in the north-central part of the country. Paris is also the political, cultural, and economic center of France.\"\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 17,\n        \"completion_tokens\": 41,\n        \"total_tokens\": 58\n    },\n    \"system_fingerprint\": \"\"\n}\n</code></pre> </li> </ol> <p>We have a successful NAI deployment.</p>"},{"location":"airgap_nai/airgap_nai_test/#accessing-llm-frontend-ui","title":"Accessing LLM Frontend UI","text":"<ol> <li> <p>In the NAI GUI, under Endpoints, click on the llama8b</p> </li> <li> <p>Click on Test</p> </li> <li> <p>Provide a sample prompt and check the output</p> <p></p> </li> </ol>"},{"location":"airgap_nai/airgap_nai_test/#sample-chat-application","title":"Sample Chat Application","text":"<p>We have a sample chat application that uses NAI to provide chatbot capabilities. We will install and use the chat application in this section.</p> <ol> <li> <p>Download and push the chat application container from upstream registry to internal harbor registry</p> <pre><code>docker pull johnugeorge/nai-chatapp:0.12\ndocker tag johnugeorge/nai-chatapp:0.12 ${REGISTRY_HOST}/nai-chatapp:0.12 \ndocker push ${REGISTRY_HOST}/nai-chatapp:0.12\n</code></pre> </li> <li> <p>Run the following command to deploy the chat application.</p> <p>Create the namespace</p> <pre><code>kubectl create ns chat\nkubens chat\n</code></pre> <p>Create the application</p> <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nai-chatapp\n  namespace: chat\n  labels:\n    app: nai-chatapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nai-chatapp\n  template:\n    metadata:\n      labels:\n        app: nai-chatapp\n    spec:\n      containers:\n      - name: nai-chatapp\n        image: johnugeorge/nai-chatapp:0.12\n        ports:\n        - containerPort: 8502\nEOF\n</code></pre> <p>Create the service</p> <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: nai-chatapp\n  namespace: chat\nspec:\n  selector:\n    app: nai-chatapp\n  ports:\n    - protocol: TCP\n      port: 8502\nEOF\n</code></pre> </li> <li> <p>Create the route for application access</p> </li> <li> <p>Insert <code>chat</code> as the subdomain in the <code>nai.10.x.x.216.nip.io</code> main domain.</p> <p>Example: complete URL</p> <pre><code>chat.nai.10.x.x.216.nip.io\n</code></pre> <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: nai-chatapp-route\n  namespace: chat                   # Same namespace as your chat app service\nspec:\n  parentRefs:\n  - name: nai-ingress-gateway\n    namespace: nai-system           # Namespace of the Gateway\n  hostnames:\n  - \"chat.nai.10.x.x.216.nip.io\"    # Input Gateway IP address\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: nai-chatapp\n      kind: Service\n      port: 8502\nEOF\n</code></pre> </li> <li> <p>We should be able to see the chat application running on the NAI cluster at the following url</p> <pre><code>chat.nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>In the chat app GUI, input the following:</p> <ul> <li>Endpoint URL - e.g. <code>https://nai.10.x.x.216.nip.io/api/v1/chat/completions</code> (can be found in the Endpoints on NAI GUI)</li> <li>Endpoint Name - e.g. <code>llama-8b</code></li> <li>API key - created during Endpoint creation</li> </ul> <p></p> </li> </ol> <p>We have successfully deployed the following:</p> <ul> <li>Inferencing endpoint</li> <li>A sample chat application that uses NAI to provide chatbot capabilities</li> </ul>"},{"location":"airgap_nai/harbor/","title":"Install Harbor","text":"<p>In this section, we will install Harbor container registry in the cluster.</p>"},{"location":"airgap_nai/harbor/#prerequisites","title":"Prerequisites","text":"<p>We will use the jumphost to install and host Harbor container registry.</p> <p>Since the jumphost also will host the <code>kind</code> cluster, we will need to ensure that the jumphost has enough resources.</p> # CPU Memory Disk Purpose Before 4 16 GB 300 GB <code>Jumphost</code> + <code>Tools</code> After 8 16 GB 300 GB <code>Jumphost</code> + <code>Tools</code> + <code>Harbor</code> + <code>kind</code> <p>Note</p> <p>If the jumphost does not have the resources, make sure to stop the jumphost and add the resources in Prism Central.</p>"},{"location":"airgap_nai/harbor/#install-harbor_1","title":"Install Harbor","text":"<p>Follow the instructions in Appendix section of this site to deploy Harbor container registry.</p> <p>Harbor Container Registry</p>"},{"location":"airgap_nai/infra_nkp_airgap/","title":"Deploy NKP Clusters","text":"<p>This lab will take you through install Air-gapped NKP(Kubernetes) on Nutanix cluster as we will be deploying AI applications on these kubernetes clusters. </p> <p>Airgapped NKP Cluster</p> <p>This lab assumes the following:</p> <ul> <li>The subnet (VPC or otherwise) is air-gapped and there is no internet connectivity.</li> <li>The jumphost VM has connectivity to download the NKP air-gapped bundle</li> <li>The jumphost is in the same subnet as the Kubernetes cluster</li> <li>The jumphost VM is hosting Harbor container registry</li> </ul> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKP {\n        [*] --&gt; CreateNkpMachineImage\n        CreateNkpMachineImage --&gt; UploadImagestoRegistry\n        UploadImagestoRegistry --&gt;  CreateNKPCluster\n        CreateNKPCluster --&gt; GenerateLicense\n        GenerateLicense --&gt; InstallLicense\n        InstallLicense --&gt; DeployGpuNodePool\n        DeployGpuNodePool --&gt; EnableGpuOperator\n        EnableGpuOperator --&gt; [*]\n    }\n\n    PrepWorkstation --&gt; DeployJumpHost \n    DeployJumpHost --&gt; DeployNKP \n    DeployNKP --&gt; DeployNai : Next section</code></pre> <p>Deploying NKP Cluster</p> <p>This lab will focus on deploying NKP to host NAI workloads. However, the steps can also be used deploy a custom NKP deployment if that's the aim.</p> <p>Consider using NKP The Hard Way section to create a customized version of your NKP cluster.</p> <p>Once you have determined the resource requirements for a custom NKP deployment, modify the environment variables and values in the <code>.env</code> file to suit your resource needs for your NKP cluster.</p>"},{"location":"airgap_nai/infra_nkp_airgap/#nkp-high-level-cluster-design","title":"NKP High Level Cluster Design","text":"<p>The <code>nkpdarksite</code> cluster will be hosting the LLM model serving endpoints and AI application stack. This cluster and will require a dedicated GPU node pool.</p>"},{"location":"airgap_nai/infra_nkp_airgap/#sizing-requirements","title":"Sizing Requirements","text":"<p>Below are the sizing requirements needed to successfully deploy NAI on a NKP Cluster (labeled as <code>nkpdarksite</code>) and subsequently deploying single LLM inferencing endpoint on NAI using the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> LLM model.</p> Calculating GPU Resources Tips <p>The calculations below assume that you're already aware of how much memory is required to load target LLM model.</p> <p>For a general example:</p> <ul> <li>To host a 8b(illion) parameter model, multiply the parameter number by 2 to get minimum GPU memory requirments.    e.g. 16GB of GPU memory is required for 8b parameter model.</li> </ul> <p>So in the case of the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> model, you'll need a min. 16 GiB GPU vRAM available</p> <p>Below are additional sizing consideration \"Rule of Thumb\" for further calculating min. GPU node resources:</p> <ul> <li>For each GPU node will have 8 CPU cores, 24 GB of memory, and 300 GB of disk space.</li> <li>For each GPU attached to the node, add 16 GiB of memory.</li> <li>For each endpoint attached to the node, add 8 CPU cores.</li> <li>If a model needs multiple GPUs, ensure all GPUs are attached to the same worker node</li> <li>For resiliency, while running multiple instances of the same endpoint, ensure that the GPUs are on different worker nodes.</li> </ul> <p>Since we will be testing with the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> HuggingFace model, we will require a GPU with a min. of 24 GiB GPU vRAM available to support this demo.</p> <p>Note</p> <p>GPU min. vRAM should be 24 GB, such as NVIDIA L4 Model.</p> <p>Below are minimum requirements for deploying NAI on the NKP Demo Cluster.</p> Role No. of Nodes (VM) vCPU per Node Memory per Node Storage per Node Total vCPU Total Memory Control plane 3 4 16 GB 150 GB 12 48 GB Worker 4 8 32 GB 150 GB 32 128 GB GPU 1 16 40 GB 300 GB 16 40 GB Totals 60 216 GB"},{"location":"airgap_nai/infra_nkp_airgap/#pre-requisites-for-nkp-deployment","title":"Pre-requisites for NKP Deployment","text":"<ol> <li>Nutanix PC is at least <code>2024.3</code></li> <li>Nutanix AOS is at least <code>6.8+</code>, <code>6.10</code></li> <li> <p>Existing Ubuntu Linux jumphost VM. See here for jumphost installation steps.</p> <p>Before you proceed!</p> <p>Make sure the Jumphost VM has enough resources to host the following:</p> <ul> <li>Harbor container registry</li> <li>Bootstrap <code>kind</code> cluster</li> </ul> <p>with at least the following resources:</p> # CPU Memory Disk Jumphost VM 8 16 GB 80 GB </li> <li> <p>Docker or Podman installed on the jumphost VM</p> </li> <li>Existing Harbor container registry on the jumphost VM. See here for installation steps.</li> <li>Download and install <code>nkp</code> binary from Nutanix Portal</li> <li>Find and reserve 3 IPs for control plane and MetalLB access from AHV network</li> <li>Find GPU details from Nutanix cluster</li> <li>Create a base image to use with NKP nodes using <code>nkp</code> command</li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#download-offline-nkp-air-gapped-bundle","title":"Download Offline NKP Air-gapped Bundle","text":"<ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Downloads &gt; NKP Airgapped Bundle</li> <li>Select NKP for Linux and copy the download link to the <code>.tar.gz</code> file</li> <li> <p>If you haven't already done so, Open new <code>VSCode</code> window on your jumphost VM</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, click on existing <code>$HOME</code> folder</p> </li> <li> <p>Click on New Folder  name it: <code>airgap-nkp</code></p> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>$HOME/airgap-nkp</code> folder</p> </li> <li> <p>On <code>VSCode</code> menu, select <code>Terminal</code> &gt; <code>New Terminal</code></p> </li> <li> <p>Browse to <code>airgap-nkp</code> directory</p> <pre><code>cd $HOME/airgap-nkp\n</code></pre> </li> <li> <p>Download and extract the NKP air-gap bundle from the link you copied earlier</p> CommandSample command Paste the download URL within double quotes<pre><code>curl -o nkp-air-gapped-bundle_v2.16.0_linux_amd64.tar.gz \"_paste_download_URL_here\"\n</code></pre> <pre><code>curl -o nkp-air-gapped-bundle_v2.16.0_linux_amd64.tar.gz \"https://download.nutanix.com/downloads/nkp/v2.16.0/nkp-air-gapped-bundle_v2.16.0_linux_amd64.tar.gz?........\"\n</code></pre> <pre><code>tar xvfz nkp-air-gapped-bundle_v2.16.0_linux_amd64.tar.gz\n</code></pre> </li> <li> <p>Move the <code>nkp</code> binary to a directory that is included in your <code>PATH</code> environment variable</p> <pre><code>sudo cp nkp-v2.16.0/cli/nkp /usr/local/bin/\n</code></pre> </li> <li> <p>Verify the <code>nkp</code> binary is installed correctly. Ensure the version is latest</p> <p>Note</p> <p>At the time of writing this lab nkp version is <code>v2.16.0</code></p> CommandCommand Output <pre><code>nkp version\n</code></pre> <pre><code>$ nkp version\ncatalog: v0.7.0\ndiagnose: v0.12.0\nimagebuilder: v2.16.0\nkommander: v2.16.0\nkonvoy: v2.16.0\nmindthegap: v1.22.1\nnkp: v2.16.0\n</code></pre> </li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#setup-docker-on-jumphost","title":"Setup Docker on Jumphost","text":"<p>If not already done, follow the steps in Setup Docker on Jumphost section. </p>"},{"location":"airgap_nai/infra_nkp_airgap/#load-docker-with-nkp-images","title":"Load Docker with NKP Images","text":"<ol> <li> <p>From VSC, open Terminal and run the following command to load the <code>nkp</code> images</p> CommandCommand output <pre><code>cd $HOME/airgap-nkp/nkp-v2.16.0/\ndocker load -i nkp-image-builder-image-v2.16.0.tar\ndocker load -i konvoy-bootstrap-image-v2.16.0.tar\n</code></pre> <p><pre><code>$ docker load -i nkp-image-builder-image-v2.16.0.tar \n9fe9a137fd00: Loading layer [==================================================&gt;]   7.63MB/7.63MB\n76fcadd9b36b: Loading layer [==================================================&gt;]  33.57MB/33.57MB\n9a230b56c773: Loading layer [==================================================&gt;]  85.18MB/85.18MB\n64339032f692: Loading layer [==================================================&gt;]  76.31MB/76.31MB\n28d0bd0e7b64: Loading layer [==================================================&gt;]  33.71MB/33.71MB\na3c8281963fd: Loading layer [==================================================&gt;]  23.93MB/23.93MB\nae15986b43a6: Loading layer [==================================================&gt;]  335.5MB/335.5MB\n483d6ecaa75d: Loading layer [==================================================&gt;]  173.2MB/173.2MB\n5210ca26c0aa: Loading layer [==================================================&gt;]  528.4MB/528.4MB\n2ffff926e4e0: Loading layer [==================================================&gt;]  12.69MB/12.69MB\nea1091ae88c8: Loading layer [==================================================&gt;]  5.632kB/5.632kB\nLoaded image: mesosphere/nkp-image-builder:v2.16.0\n</code></pre> <pre><code>$ docker load -i konvoy-bootstrap-image-v2.16.0.tar \nLoaded image: mesosphere/konvoy-bootstrap:v2.16.0\n</code></pre></p> <p>`</p> </li> <li> <p>Confirm presence of container images on jumhost VM</p> CommandCommand output <pre><code>docker image ls\n</code></pre> <pre><code>$ docker image ls\n\nREPOSITORY                                            TAG          IMAGE ID       CREATED        SIZE\nmesosphere/nkp-image-builder                          v2.16.0      4bae9be67aa2   45 years ago   1.3GB\nmesosphere/konvoy-bootstrap                           v2.16.0      a2aa0268435b   2 weeks ago    2.64GB\n</code></pre> </li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#reserve-control-plane-and-metallb-ip","title":"Reserve Control Plane and MetalLB IP","text":"<p>Nutanix AHV IPAM network allows you to black list IPs that needs to be reserved for specific application endpoints. We will use this feature to find and reserve three IPs. </p> <p>We will reserve a total of three IPs for the following:</p> Cluster Role Cluster Name NKP NAI Dev <code>nkpdarksite</code> 2 1 <ol> <li> <p>Get the CIDR range for the AHV network(subnet) where the application will be deployed</p> CIDR example for your Nutanix cluster<pre><code>10.x.x.0/24\n</code></pre> </li> <li> <p>From VSC, logon to your jumpbox VM and open Terminal</p> </li> <li> <p>Install <code>nmap</code> tool (if not already done)</p> <pre><code>cd $HOME/sol-cnai-infra\ndevbox add nmap\n</code></pre> </li> <li> <p>Find three unused static IP addresses in the subnet</p> CommandSample command <pre><code>nmap -v -sn  &lt;your CIDR&gt;\n</code></pre> <pre><code>nmap -v -sn 10.x.x.0/24\n</code></pre> Sample output - choose the first three consecutive IPs<pre><code>Nmap scan report for 10.x.x.214 [host down]\nNmap scan report for 10.x.x.215 [host down]\nNmap scan report for 10.x.x.216 [host down]\nNmap scan report for 10.x.x.217\nHost is up (-0.098s latency).\n</code></pre> </li> <li> <p>Logon to any CVM in your Nutanix cluster and execute the following to add chosen static IPs to the AHV IPAM network</p> <ul> <li>Username: nutanix</li> <li>Password: your Prism Element password </li> </ul> CommandSample command <pre><code>acli net.add_to_ip_blacklist &lt;your-ipam-ahv-network&gt; \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> <pre><code>acli net.add_to_ip_blacklist User1 \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> </li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#reservation-of-ips","title":"Reservation of IPs","text":"<p>Reserve the firs IPs for NKP control plane  Reserve the second two IPs for MetalLB distributed load balancer - We will use one of these IP for NAI</p> <p>Reserve the third IP for NAI. We will use the NAI IP in the next NAI section to assign the FDQN and install SSL certificate.</p> Component IP FQDN NKP Control Plane VIP <code>10.x.x.214</code> - NKP MetalLB IP Range <code>10.x.x.215-10.x.x.216</code> - NAI <code>10.x.x.216</code> <code>nai.10.x.x.216.nip.io</code>"},{"location":"airgap_nai/infra_nkp_airgap/#create-base-image-for-nkp","title":"Create Base Image for NKP","text":"<p>In this section we will go through creating a base image for all the control plane and worker node VMs on Nutanix.</p> <ol> <li> <p>In VSC Explorer pane, Click on New Folder </p> </li> <li> <p>Call the folder <code>airgap-nkp</code> under <code>$HOME</code> directory</p> </li> <li> <p>In the <code>airgap-nkp</code> folder, click on New File  and create new file with the following name:</p> <pre><code>.env\n</code></pre> </li> <li> <p>Run the following command to generate an new RSA key pair on the jumphost VM. This SSH key pair will be used for authentication between the jumphost and NKP K8S cluster nodes.</p> Do you have existing SSH key pair? <p>Copy the key pair from your workstation (PC/Mac) to <code>~/.ssh/</code> directory on your Jumphost VM.</p> <pre><code>mac/pc $ scp ~/.ssh/id_rsa.pub ubuntu@10.x.x.171:~/.ssh/id_rsa.pub\nmac/pc $ scp ~/.ssh/id_rsa ubuntu@10.x.x.171:~/.ssh/id_rsa\n</code></pre> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>Accept the default file location as <code>~/.ssh/id_rsa</code></p> <p>SSH key pair will stored in the following location:</p> <pre><code>~/.ssh/id_rsa.pub \n~/.ssh/id_rsa\n</code></pre> </li> <li> <p>Fill the following values inside the <code>.env</code> file</p> Template .envSample .env <pre><code>export NUTANIX_USER=_your_nutanix_username\nexport NUTANIX_PASSWORD=_your_nutanix_password\nexport NUTANIX_ENDPOINT=_your_prism_central_fqdn\nexport NUTANIX_CLUSTER=_your_prism_element_cluster_name\nexport NUTANIX_SUBNET_NAME=_your_ahv_ipam_network_name\nexport STORAGE_CONTAINER=_your_storage_container_nmae\nexport SSH_PUBLIC_KEY=_path_to_ssh_pub_key_on_jumphost_vm\nexport NKP_CLUSTER_NAME=_your_nkp_cluster_name\nexport CONTROLPLANE_VIP=_your_nkp_cluster_controlplane_ip\nexport LB_IP_RANGE=_your_range_of_two_ips\nexport OS_BUNDLE_DIR=_your_artifacts_directory\nexport OS=_your_preferred_os\nexport BASE_IMAGE=_your_baseimage_name\n</code></pre> <pre><code>export NUTANIX_USER=admin\nexport NUTANIX_PASSWORD=xxxxxxxx\nexport NUTANIX_ENDPOINT=pc.example.com\nexport NUTANIX_CLUSTER=pe\nexport NUTANIX_SUBNET_NAME=User1\nexport STORAGE_CONTAINER=default\nexport SSH_PUBLIC_KEY=$HOME/.ssh/id_rsa.pub\nexport NKP_CLUSTER_NAME=nkpdarksite\nexport CONTROLPLANE_VIP=10.x.x.214\nexport LB_IP_RANGE=10.x.x.215-10.x.x.216\nexport OS_BUNDLE_DIR=kib/artifacts\nexport OS=ubuntu-22.04\nexport BASE_IMAGE=ubuntu-22.04-server-cloudimg-amd64.img\n</code></pre> </li> <li> <p>Using VSC Terminal, load the environment variables and its values</p> <pre><code>cd $HOME/airgap-nkp\nsource .env\ncd nkp-v2.16.0/\n</code></pre> </li> <li> <p>Create the base image</p> CommandCommand output <pre><code>nkp create package-bundle --artifacts-directory ${OS_BUNDLE_DIR} ${OS}\n</code></pre> <pre><code>$ nkp create package-bundle --artifacts-directory ${OS_BUNDLE_DIR} ${OS}\n\nOS bundle configuration files extracted to /home/ubuntu/airgap-nkp/nkp-v2.16.0/kib/artifacts/.dkp-image-builder-2593079857\nGet:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\nGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n\n&lt;snip&gt;\n\nGet:241 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 gpgv2 all 2.2.27-3ubuntu2.1 [4392 B]                              \nGet:242 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1306 kB]                  \nGet:243 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]               \nFetched 176 MB in 9s (19.9 MB/s) \ndpkg-scanpackages: info: Wrote 243 entries to output Packages file.\n/home/ubuntu/airgap-nkp/nkp-v2.16.0/kib/artifacts/.dkp-image-builder-2593079857/ubuntu-22.04/Packages       \n</code></pre> </li> <li> <p>Create the os image and upload to Prism Central using the following command. </p> <p>Note</p> <p>Image creation will take up to 5 minutes.</p> CommandCommand output <pre><code>nkp create image nutanix ${OS} --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} --subnet ${NUTANIX_SUBNET_NAME} --source-image ${BASE_IMAGE} --artifacts-directory ${OS_BUNDLE_DIR}\n</code></pre> <pre><code>nkp create image nutanix ${OS} --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} --subnet ${NUTANIX_SUBNET_NAME} --source-image ${BASE_IMAGE} --artifacts-directory ${OS_BUNDLE_DIR}\n\n&gt; Provisioning and configuring image\nManifest files extracted to $HOME/nkp/.nkp-image-builder-3243021807\nnutanix.kib_image: output will be in this color.\n\n==&gt; nutanix.kib_image: Creating Packer Builder virtual machine...\n    nutanix.kib_image: Virtual machine nkp-ubuntu-22.04-1.33.2-20250925013631 created\n    nutanix.kib_image: Found IP for virtual machine: 10.x.x.234\n==&gt; nutanix.kib_image: Running post-processor: packer-manifest (type manifest)\n\n---&gt; 100%\nBuild 'nutanix.kib_image' finished after 4 minutes 55 seconds.\n==&gt; Wait completed after 4 minutes 55 seconds\n\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.33.2-20250925013631\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.33.2-20250925013631\n</code></pre> <p>Image name - This will be different in your environment</p> <p>Note image name from the previous <code>nkp</code> create image command output</p> <pre><code>==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.33.2-20250925013631\n</code></pre> <p>Warning</p> <p>Make sure to use image name that is generated in your environment for the next steps.</p> </li> <li> <p>Populate the <code>.env</code> file with the NKP image name by adding (appending) the following environment variables and save it</p> Template .envSample .env <pre><code>export NKP_IMAGE=nkp-image-name\n</code></pre> <pre><code>export NKP_IMAGE=nkp-ubuntu-22.04-1.33.2-20250925013631\n</code></pre> </li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#push-container-images-to-localprivate-registry-to-be-used-by-nkp","title":"Push Container Images to Local/Private Registry to be used by NKP","text":"<p>In this section we will use internal Harbor container registry to upload NKP container images for deployment. </p> <p>Warning</p> <p>This section requires a air-gapped container registry to be functional.</p> <p>If you haven't already deployed the air gapped container registry, please follow the steps in Harbor Container Registry section to deploy a Harbor container registry on the jumphost VM.</p> <p>NKP v2.16 has optional inbuilt CNCF OCI registry for deployment ease</p> <p>NKP <code>v2.16</code> has an optional inbuilt CNCF OCI Registry for deployment ease.</p> <p>While deploying NKP use the following switch to specify the location of the container image bundles. </p> <pre><code>nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n--bundle=/path_to_extracted_airgap_bundle/nkp-v2.16.0/container-images/*.tar \\  # (1)\n# other options\n</code></pre> <ol> <li>Specify path of the extracted air-gap bundle</li> </ol> <p>Note that the registry deployment will take <code>30 minutes</code>, with the cluster deployed in about <code>60 minutes</code>. This registry is only to be used for NKP clusters (management and workload) deployment. Any on-going use of inbuilt registry should be considered only after assesing requirements, design, capacity, availability and scalability. </p> <ol> <li> <p>Open <code>$HOME/airgap-nkp/.env</code> file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export REGISTRY_URL=_your_registry_url\nexport REGISTRY_USERNAME=_your_registry_username\nexport REGISTRY_PASSWORD=_your_registry_password\nexport REGISTRY_CACERT=_path_to_ca_cert_of_registry  # (1)!\n</code></pre> <ol> <li>File must contain CA server and Harbor server's public certificate in one file</li> </ol> <pre><code>export REGISTRY_URL=https://harbor.10.x.x.111.nip.io/nkp\nexport REGISTRY_USERNAME=admin\nexport REGISTRY_PASSWORD=xxxxxxxx\nexport REGISTRY_CACERT=$HOME/harbor/certs/full_chain.pem  # (1)!\n</code></pre> <ol> <li>File must contain CA server and Harbor server's public certificate in one file</li> </ol> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>cd $HOME/airgap-nkp/\nsource .env\n</code></pre> </li> <li> <p>Push the images to air-gapped registry</p> <pre><code>cd nkp-v2.16.0/\n</code></pre> CommandCommand output <p><pre><code>nkp push bundle --bundle ./container-images/konvoy-image-bundle-v2.16.0.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n</code></pre> <pre><code>nkp push bundle --bundle ./container-images/kommander-image-bundle-v2.16.0.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n</code></pre> <pre><code>nkp push bundle --bundle ./application-repositories/kommander-applications-v2.16.0.tar.gz \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n</code></pre></p> <p><pre><code>$ nkp push bundle --bundle ./container-images/konvoy-image-bundle-v2.16.0.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n\u2713 Creating temporary directory\n\u2713 Unarchiving image bundle \"./container-images/konvoy-image-bundle-v2.16.0.tar\" \n\u2713 Parsing image bundle config\n\u2713 Starting temporary Docker registry\n\u2713 Pushing bundled images [================================&gt;129/129] (time elapsed 153s) \n</code></pre> <pre><code>$ nkp push bundle --bundle ./container-images/kommander-image-bundle-v2.16.0.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n\u2713 Creating temporary directory\n\u2713 Unarchiving image bundle \"./container-images/kommander-image-bundle-v2.16.0.tar\" \n\u2713 Parsing image bundle config\n\u2713 Starting temporary Docker registry\n\u2713 Pushing bundled images [================================&gt;131/131] (time elapsed 183s) \n(devbox) \n</code></pre> <pre><code>nkp push bundle --bundle ../application-repositories/kommander-applications-v2.16.0.tar.gz \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n \u2713 Creating temporary directory\n \u2713 Unarchiving image bundle \"./application-repositories/kommander-applications-v2.16.0.tar.gz\"\n \u2713 Starting temporary Docker registry\n</code></pre></p> </li> </ol> <p>We are now ready to install the workload <code>nkpdarksite</code> cluster</p>"},{"location":"airgap_nai/infra_nkp_airgap/#create-air-gapped-nkp-workload-cluster","title":"Create Air-gapped NKP Workload Cluster","text":"<p>Warning</p> <p>Do not use hyphens <code>-</code> in the nkp cluster name. </p> Clustername Validation Rules<pre><code>a lowercase RFC 1123 subdomain must consist of lower case alphanumeric       \u2502\n\u2502characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com',  \u2502\n\u2502regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')\n</code></pre> <p>Note</p> <p>In this lab the workload cluster will have the Management cluster role as well to reduce resource consumption in a lab environment. </p> <p>However, for production environments, the ideal design is to have a separate management and workload clusters. </p> <ol> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export CONTROL_PLANE_REPLICAS=_no_of_control_plane_replicas\nexport CONTROL_PLANE_VCPUS=_no_of_control_plane_vcpus\nexport CONTROL_PLANE_CORES_PER_VCPU=_no_of_control_plane_cores_per_vcpu\nexport CONTROL_PLANE_MEMORY_GIB=_no_of_control_plane_memory_gib\nexport WORKER_REPLICAS=_no_of_worker_replicas\nexport WORKER_VCPUS=_no_of_worker_vcpus\nexport WORKER_CORES_PER_VCPU=_no_of_worker_cores_per_vcpu\nexport WORKER_MEMORY_GIB=_no_of_worker_memory_gib\nexport CSI_FILESYSTEM=_preferred_filesystem_ext4/xfs\nexport CSI_HYPERVISOR_ATTACHED=_true/false\nexport NUTANIX_PROJECT_NAME=_your_pc_project_name\n</code></pre> <pre><code>export CONTROL_PLANE_REPLICAS=3\nexport CONTROL_PLANE_VCPUS=4\nexport CONTROL_PLANE_CORES_PER_VCPU=1\nexport CONTROL_PLANE_MEMORY_GIB=16\nexport WORKER_REPLICAS=4\nexport WORKER_VCPUS=8 \nexport WORKER_CORES_PER_VCPU=1\nexport WORKER_MEMORY_GIB=32\nexport CSI_FILESYSTEM=ext4\nexport CSI_HYPERVISOR_ATTACHED=true\nexport NUTANIX_PROJECT_NAME=dev-lab\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source .env\n</code></pre> </li> <li> <p>In VSC, open Terminal, enter the following command to create the workload cluster</p> Check your command for correct argument values <p>Run the following command to verify your <code>nkp</code> command and associated environment variables and values.</p> <pre><code>echo \"nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n        --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n        --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n        --control-plane-vm-image ${NKP_IMAGE} \\\n        --csi-storage-container ${STORAGE_CONTAINER} \\\n        --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n        --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n        --worker-vm-image ${NKP_IMAGE} \\\n        --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n        --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n        --control-plane-disk-size 150 \\\n        --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} \\\n        --control-plane-vcpus ${CONTROL_PLANE_VCPUS} \\\n        --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n        --worker-disk-size 150 \\\n        --worker-memory ${WORKER_MEMORY_GIB} \\\n        --worker-vcpus ${WORKER_VCPUS} \\\n        --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n        --csi-file-system ${CSI_FILESYSTEM} \\\n        --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n        --registry-mirror-url=${REGISTRY_URL} \\\n        --registry-mirror-username=${REGISTRY_USERNAME} \\\n        --registry-mirror-password=${REGISTRY_PASSWORD} \\\n        --registry-mirror-cacert=${REGISTRY_CACERT} \\\n        --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --self-managed \\\n        --airgapped\"\n</code></pre> <p>If the values are incorrect, add the correct values to <code>.env</code> and source the  again by running the following command</p> <pre><code>source .env\n</code></pre> <p>Then rerun the <code>echo nkp</code> command to verify the values again before running the <code>nkp create cluster nutanix</code> command.</p> CommandCommand output <pre><code>nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n    --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n    --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n    --control-plane-vm-image ${NKP_IMAGE} \\\n    --csi-storage-container ${STORAGE_CONTAINER} \\\n    --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n    --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n    --worker-vm-image ${NKP_IMAGE} \\\n    --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n    --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n    --control-plane-disk-size 150 \\\n    --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} \\\n    --control-plane-vcpus ${CONTROL_PLANE_VCPUS} \\\n    --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n    --worker-disk-size 150 \\\n    --worker-memory ${WORKER_MEMORY_GIB} \\\n    --worker-vcpus ${WORKER_VCPUS} \\\n    --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n    --csi-file-system ${CSI_FILESYSTEM} \\\n    --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n    --registry-mirror-url=${REGISTRY_URL} \\\n    --registry-mirror-username=${REGISTRY_USERNAME} \\\n    --registry-mirror-password=${REGISTRY_PASSWORD} \\\n    --registry-mirror-cacert=${REGISTRY_CACERT} \\\n    --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --self-managed \\\n    --airgapped\n</code></pre> <pre><code>&gt; \u2713 Creating a bootstrap cluster \n\u2713 Upgrading CAPI components \n\u2713 Waiting for CAPI components to be upgraded \n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Creating ClusterClass resources\n&gt; Generating cluster resources\ncluster.cluster.x-k8s.io/nkpdarksite created\nsecret/nkpdarksite-pc-credentials created\nsecret/nkpdarksite-pc-credentials-for-csi created\nsecret/nkpdarksite-image-registry-credentials created\n\u2713 Waiting for cluster infrastructure to be ready \n\u2713 Waiting for cluster control-planes to be ready \n\u2713 Waiting for machines to be ready\n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Moving cluster resources\n\n&gt; You can now view resources in the moved cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdarksite.conf\" get nodes\n\n&gt; \u2713 Deleting bootstrap cluster \n\nCluster default/nkpdarksite kubeconfig was written to to the filesystem.\nYou can now view resources in the new cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdarksite.conf\" get nodes\n\n&gt; Starting kommander installation\n\u2713 Deploying Flux \n\u2713 Deploying Ingress certificate \n\u2713 Creating kommander-overrides ConfigMap\n\u2713 Deploying Git Operator \n\u2713 Creating GitClaim for management GitRepository \n\u2713 Creating GitClaimUser for accessing management GitRepository \n\u2713 Creating HTTP Proxy configuration\n\u2713 Deploying Flux configuration\n\u2713 Deploying Kommander Operator \n\u2713 Creating KommanderCore resource \n\u2713 Cleaning up kommander bootstrap resources\n\u2713 Deploying Substitution variables\n\u2713 Deploying Flux configuration \n\u2713 Deploying Gatekeeper \n\u2713 Deploying Kommander AppManagement \n\u2713 Creating Core AppDeployments \n\u2713 4 out of 12 core applications have been installed (waiting for dex, dex-k8s-authenticator and 6 more) \n\u2713 5 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 5 more) \n\u2713 7 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 3 more) \n\u2713 8 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 2 more) \n\u2713 9 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 1 more) \n\u2713 10 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, traefik-forward-auth-mgmt) \n\u2713 11 out of 12 core applications have been installed (waiting for traefik-forward-auth-mgmt) \n\u2713 Creating cluster-admin credentials\n\n&gt; Cluster was created successfully! Get the dashboard details with:\n&gt; nkp get dashboard --kubeconfig=\"$HOME/airgap-nkp/nkpdarksite.conf\"\n</code></pre> <p>Deployment info</p> <p>The above command with the use of <code>--self-managed</code> argument, will create a bootstrap cluster, deploy CAPI resources on it and create a NKP base cluster (konvoy) using the CAPI components in the bootstrap cluster. It will automatically do the following once the NKP base cluster is provisioned:</p> <ul> <li>Deploy CAPI components on the bootstrap cluster</li> <li>Move the CAPI components from the bootstrap cluster to the new cluster</li> <li>Delete the Bootstrap cluster</li> <li>Deploy the Kommander components on top of the new base NKP cluster </li> </ul> <p>See NKP the Hard Way section for more information for customizable NKP cluster deployments. </p> </li> <li> <p>Observe the events in the shell and in Prism Central events</p> </li> <li> <p>Run the following command to check K8S status of the <code>nkpdarksite</code> cluster</p> CommandCommand output <pre><code>export KUBECONFIG=${NKP_CLUSTER_NAME}.conf\nkubectl get nodes\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                  STATUS   ROLES           AGE     VERSION\nnkpdarksite-md-0-x948v-hvxtj-9r698           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdarksite-md-0-x948v-hvxtj-fb75c           Ready    &lt;none&gt;          4h50m   v1.29.6\nnkpdarksite-md-0-x948v-hvxtj-mdckn           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdarksite-md-0-x948v-hvxtj-shxc8           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdarksite-r4fwl-8q4ch                      Ready    control-plane   4h50m   v1.29.6\nnkpdarksite-r4fwl-jf2s8                      Ready    control-plane   4h51m   v1.29.6\nnkpdarksite-r4fwl-q888c                      Ready    control-plane   4h49m   v1.29.6\n</code></pre> </li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#add-nkp-gpu-workload-pool","title":"Add NKP GPU Workload Pool","text":"<p>Are you just deploying NKP?</p> <p>If you are doing this lab only to deploy NKP, then you can skip this GPU section.</p> <p>The steps below covers the following:     - Retrieving and Applying NKP Pro License     - Identifying the GPU device name     - Deploying the GPU nodepool     - Enabling the NVIDIA GPU Operator</p> <p>Note</p> <p>To Enable the GPU Operator afterwards using the NKP Marketplace, a minimal NKP Pro license is required.</p>"},{"location":"airgap_nai/infra_nkp_airgap/#find-gpu-device-details","title":"Find GPU Device Details","text":"<p>As we will be deploying Nutanix Enterprise AI (NAI) in the next section, we need to find the GPU details beforehand.</p> <p>Find the details of GPU on the Nutanix cluster while still connected to Prism Central (PC).</p> <ol> <li>Logon to Prism Central GUI</li> <li>On the general search, type GPUs</li> <li> <p>Click on the GPUs result</p> <p></p> </li> <li> <p><code>Lovelace 40s</code> is the GPU available for use</p> </li> <li>Use <code>Lovelace 40s</code> in the evironment variables in the next section.</li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#create-nkp-gpu-workload-pool","title":"Create NKP GPU Workload Pool","text":"<p>In this section we will create a nodepool to host the AI apps with a GPU.</p> <ol> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export GPU_NAME=_name_of_gpu_device_\nexport GPU_REPLICA_COUNT=_no_of_gpu_worker_nodes\nexport GPU_POOL=_name_of_gpu_pool\nexport GPU_NODE_VCPUS=_no_of_gpu_node_vcpus\nexport GPU_NODE_CORES_PER_VCPU=_per_gpu_node_cores_per_vcpu\nexport GPU_NODE_MEMORY_GIB=_per_gpu_node_memory_gib\nexport GPU_NODE_DISK_SIZE_GIB=_per_gpu_node_memory_gib\n</code></pre> <pre><code>export GPU_NAME=\"Lovelace 40S\"\nexport GPU_REPLICA_COUNT=1\nexport GPU_POOL=gpu-nodepool\nexport GPU_NODE_VCPUS=16\nexport GPU_NODE_CORES_PER_VCPU=1\nexport GPU_NODE_MEMORY_GIB=40\nexport GPU_NODE_DISK_SIZE_GIB=200\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source .env\n</code></pre> </li> <li> <p>Run the following command to create a GPU nodepool manifest</p> <pre><code>nkp create nodepool nutanix \\\n    --cluster-name ${NKP_CLUSTER_NAME} \\\n    --prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --pc-project ${NUTANIX_PROJECT_NAME} \\\n    --subnets ${NUTANIX_SUBNET_NAME} \\\n    --vm-image ${NKP_IMAGE} \\\n    --disk-size ${GPU_NODE_DISK_SIZE_GIB} \\\n    --memory ${GPU_NODE_MEMORY_GIB} \\\n    --vcpus ${GPU_NODE_VCPUS} \\\n    --cores-per-vcpu ${GPU_NODE_CORES_PER_VCPU} \\\n    --replicas ${GPU_REPLICA_COUNT} \\\n    --wait \\\n    ${GPU_POOL} --dry-run -o yaml &gt; gpu-nodepool.yaml\n</code></pre> <p>Note</p> <p>Right now there is no switch for GPU in <code>nkp</code> command. We need to do dry-run the output into a file and then add the necessary GPU specifications</p> </li> <li> <p>Add the necessary gpu section to our new <code>gpu-nodepool.yaml</code> using <code>yq</code> command</p> <pre><code>yq e '(.spec.topology.workers.machineDeployments[] | select(.name == \"gpu-nodepool\").variables.overrides[] | select(.name == \"workerConfig\").value.nutanix.machineDetails) += {\"gpus\": [{\"type\": \"name\", \"name\": strenv(GPU_NAME)}]}' -i gpu-nodepool.yaml\n</code></pre> Successful addtion of GPU specs? <p>You would be able to see the added gpu section at the end of the <code>gpu-nodepool.yaml</code> file</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\n\n&lt;snip&gt;\n\n  name: gpu-nodepool\n  variables:\n    overrides:\n      - name: workerConfig\n        value:\n          nutanix:\n            machineDetails:\n              bootType: legacy\n              cluster:\n                name: romanticism\n                type: name\n              image:\n                name: nkp-ubuntu-22.04-1.29.6-20240718055804\n                type: name\n              memorySize: 40Gi\n              subnets:\n                - name: User1\n                  type: name\n              systemDiskSize: 200Gi\n              vcpuSockets: 16\n              vcpusPerSocket: 1\n              gpus:\n                - type: name\n                  name: Lovelace 40S\n</code></pre> </li> <li> <p>Monitor Cluster-Api resources to ensure gpu machine will be successfully</p> <pre><code>watch kubectl get cluster-api\n</code></pre> </li> <li> <p>Apply the <code>gpu-nodepool.yaml</code> file to the workload cluster</p> <pre><code>kubectl apply -f gpu-nodepool.yaml\n</code></pre> </li> <li> <p>Monitor the progress of the command and check Prism Central events for creation of the GPU worker node</p> </li> <li> <p>Check nodes status in workload <code>nkpdarksite</code> cluster and note the gpu worker node</p> CommandCommand output <pre><code>kubectl get nodes -w\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                   STATUS   ROLES           AGE     VERSION\nnkpdarksite-gpu-nodepool-7g4jt-2p7l7-49wvd   Ready    &lt;none&gt;          5m57s   v1.29.6\nnkpdarksite-md-0-q679c-khl2n-9k7jk           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdarksite-md-0-q679c-khl2n-9nk6h           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdarksite-md-0-q679c-khl2n-nf9p6           Ready    &lt;none&gt;          73m     v1.29.6\nnkpdarksite-md-0-q679c-khl2n-qgxp9           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdarksite-ncnww-2dg7h                      Ready    control-plane   73m     v1.29.6\nnkpdarksite-ncnww-bbm4s                      Ready    control-plane   72m     v1.29.6\nnkpdarksite-ncnww-hldm9                      Ready    control-plane   75m     v1.29.6\n</code></pre> </li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#licensing","title":"Licensing","text":"<p>We need to generate a license for the NKP cluster which is the total for all the vCPUs used by worker nodes.</p> <p>For example, in the Sizing Requirements section, the NKP Demo Cluster <code>Total vCPU count</code> is equal to <code>60</code>, whereas the actual worker nodes total vCPU count is only <code>48</code>.</p>"},{"location":"airgap_nai/infra_nkp_airgap/#generate-nkp-pro-license","title":"Generate NKP Pro License","text":"<p>To generate a NKP Pro License for the NKP cluster:</p> <p>Note</p> <p>Nutanix Internal users should logon using Nutanix SSO</p> <p>Nutanix Partners/Customers should logon to Portal using their Nutanix Portal account credentials</p> <ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Licensing &gt; License Summary</li> <li>Click on the small drop down arrow  on Manage Licenses and choose Nutanix Kubernetes Platform (NKP)</li> <li>Input the NKP cluster name</li> <li>Click on the plus icon </li> <li>Click on Next in the bottom right corner</li> <li>Select NKP Pro License</li> <li>Select Apply to cluster</li> <li>Choose Non-production license and Save</li> <li>Select the cluster name and click on Next</li> <li>Input the number of vCPU (<code>60</code>) from our calculations in the previous section</li> <li>Click on Save</li> <li>Download the csv file and store it in a safe place</li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#applying-nkp-pro-license-to-nkp-cluster","title":"Applying NKP Pro License to NKP Cluster","text":"<ol> <li> <p>Login to the Kommander URL for <code>nkpdarksite</code> cluster with the generated credentials that was generated in the previous section. The following commands will give you the credentials and URL.</p> CommandCommand output <pre><code>nkp get dashboard\n</code></pre> <pre><code>nkp get dashboard\n\nUsername: recursing_xxxxxxxxx\nPassword: YHbPsslIDB7p7rqwnfxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nURL: https://10.x.x.215/dkp/kommander/dashboard\n</code></pre> </li> <li> <p>Go to Licensing and click on Remove License to remove the Starter license</p> </li> <li>Type nutanix-license in the confirmation box and click on Remove License</li> <li>Click on Add License, choose Nutanix platform and paste the license key from the previous section</li> <li>Click on Save</li> <li>Confirm the license is applied to the cluster by cheking the License Status in the License menu</li> <li>The license will be applied to the cluster and the license status will reflect NKP Pro in the top right corner of the dashboard</li> </ol>"},{"location":"airgap_nai/infra_nkp_airgap/#enable-gpu-operator","title":"Enable GPU Operator","text":"<p>We will need to enable GPU operator for deploying NKP application. </p> <ol> <li>In the NKP GUI, Go to Clusters</li> <li>Click on Kommander Host</li> <li>Go to Applications </li> <li>Search for NVIDIA GPU Operator</li> <li>Click on Enable</li> <li>Click on Configuration tab</li> <li> <p>Click on Workspace Application Configuration Override and paste the following yaml content</p> <pre><code>driver:\n  enabled: true\n</code></pre> <p>As shown here:</p> <p></p> </li> <li> <p>Click on Enable on the top right-hand corner to enable GPU driver on the Ubuntu GPU nodes</p> </li> <li> <p>Check GPU operator resources and make sure they are running</p> CommandCommand output <pre><code>kubectl get po -A | grep -i nvidia\n</code></pre> <pre><code>kubectl get po -A | grep -i nvidia\n\nnvidia-container-toolkit-daemonset-fjzbt                          1/1     Running     0          28m\nnvidia-cuda-validator-f5dpt                                       0/1     Completed   0          26m\nnvidia-dcgm-exporter-9f77d                                        1/1     Running     0          28m\nnvidia-dcgm-szqnx                                                 1/1     Running     0          28m\nnvidia-device-plugin-daemonset-gzpdq                              1/1     Running     0          28m\nnvidia-driver-daemonset-dzf55                                     1/1     Running     0          28m\nnvidia-operator-validator-w48ms                                   1/1     Running     0          28m\n</code></pre> </li> <li> <p>Run a sample GPU workload to confirm GPU operations</p> CommandCommand output <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre> <pre><code>pod/cuda-vector-add created\n</code></pre> </li> <li> <p>Follow the logs to check if the GPU operations are successful</p> CommandSample CommandCommand output <pre><code>kubectl logs _gpu_worload_pod_name\n</code></pre> <pre><code>kubectl logs cuda-vector-add-xxx\n</code></pre> <pre><code>kubectl logs cuda-vector-add\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\n</code></pre> </li> </ol> <p>Now we are ready to deploy our AI workloads.</p>"},{"location":"appendix/infra_nkp_airgap/","title":"Deploy NKP Clusters","text":"<p>This lab will take you through install Air-gapped NKP(Kubernetes) on Nutanix cluster as we will be deploying AI applications on these kubernetes clusters. </p> <p>Airgapped NKP Cluster</p> <p>This lab assumes the following:</p> <ul> <li>The subnet (VPC or otherwise) is air-gapped and there is no internet connectivity.</li> <li>The jumphost VM has connectivity to download the NKP air-gapped bundle</li> <li>The jumphost is in the same subnet as the Kubernetes cluster</li> <li>The jumphost VM is hosting Harbor container registry</li> </ul> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKP {\n        [*] --&gt; CreateNkpMachineImage\n        CreateNkpMachineImage --&gt; UploadImagestoRegistry\n        UploadImagestoRegistry --&gt;  CreateNKPCluster\n        CreateNKPCluster --&gt; GenerateLicense\n        GenerateLicense --&gt; InstallLicense\n        InstallLicense --&gt; DeployGpuNodePool\n        DeployGpuNodePool --&gt; EnableGpuOperator\n        EnableGpuOperator --&gt; [*]\n    }\n\n    PrepWorkstation --&gt; DeployJumpHost \n    DeployJumpHost --&gt; DeployNKP \n    DeployNKP --&gt; DeployNai : Next section</code></pre> <p>Deploying NKP Cluster</p> <p>This lab will focus on deploying NKP to host NAI workloads. However, the steps can also be used deploy a custom NKP deployment if that's the aim.</p> <p>Consider using NKP The Hard Way section to create a customized version of your NKP cluster.</p> <p>Once you have determined the resource requirements for a custom NKP deployment, modify the environment variables and values in the <code>.env</code> file to suit your resource needs for your NKP cluster.</p>"},{"location":"appendix/infra_nkp_airgap/#nkp-high-level-cluster-design","title":"NKP High Level Cluster Design","text":"<p>The <code>nkpdarksite</code> cluster will be hosting the LLM model serving endpoints and AI application stack. This cluster and will require a dedicated GPU node pool.</p>"},{"location":"appendix/infra_nkp_airgap/#sizing-requirements","title":"Sizing Requirements","text":"<p>Below are the sizing requirements needed to successfully deploy NAI on a NKP Cluster (labeled as <code>nkpdarksite</code>) and subsequently deploying single LLM inferencing endpoint on NAI using the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> LLM model.</p> Calculating GPU Resources Tips <p>The calculations below assume that you're already aware of how much memory is required to load target LLM model.</p> <p>For a general example:</p> <ul> <li>To host a 8b(illion) parameter model, multiply the parameter number by 2 to get minimum GPU memory requirments.    e.g. 16GB of GPU memory is required for 8b parameter model.</li> </ul> <p>So in the case of the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> model, you'll need a min. 16 GiB GPU vRAM available</p> <p>Below are additional sizing consideration \"Rule of Thumb\" for further calculating min. GPU node resources:</p> <ul> <li>For each GPU node will have 8 CPU cores, 24 GB of memory, and 300 GB of disk space.</li> <li>For each GPU attached to the node, add 16 GiB of memory.</li> <li>For each endpoint attached to the node, add 8 CPU cores.</li> <li>If a model needs multiple GPUs, ensure all GPUs are attached to the same worker node</li> <li>For resiliency, while running multiple instances of the same endpoint, ensure that the GPUs are on different worker nodes.</li> </ul> <p>Since we will be testing with the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> HuggingFace model, we will require a GPU with a min. of 24 GiB GPU vRAM available to support this demo.</p> <p>Note</p> <p>GPU min. vRAM should be 24 GB, such as NVIDIA L4 Model.</p> <p>Below are minimum requirements for deploying NAI on the NKP Demo Cluster.</p> Role No. of Nodes (VM) vCPU per Node Memory per Node Storage per Node Total vCPU Total Memory Control plane 3 4 16 GB 150 GB 12 48 GB Worker 4 8 32 GB 150 GB 32 128 GB GPU 1 16 40 GB 300 GB 16 40 GB Totals 60 216 GB"},{"location":"appendix/infra_nkp_airgap/#pre-requisites-for-nkp-deployment","title":"Pre-requisites for NKP Deployment","text":"<ol> <li>Nutanix PC is at least <code>2024.1</code></li> <li>Nutanix AOS is at least <code>6.5</code>,<code>6.8+</code></li> <li> <p>Existing Ubuntu Linux jumphost VM. See here for jumphost installation steps.</p> <p>Before you proceed!</p> <p>Make sure the Jumphost VM has enough resources to host the following:</p> <ul> <li>Harbor container registry</li> <li>Bootstrap <code>kind</code> cluster</li> </ul> <p>with at least the following resources:</p> # CPU Memory Disk Jumphost VM 8 16 GB 80 GB </li> <li> <p>Docker or Podman installed on the jumphost VM</p> </li> <li>Existing Harbor container registry on the jumphost VM. See here for installation steps.</li> <li>Download and install <code>nkp</code> binary from Nutanix Portal</li> <li>Find and reserve 3 IPs for control plane and MetalLB access from AHV network</li> <li>Find GPU details from Nutanix cluster</li> <li>Create a base image to use with NKP nodes using <code>nkp</code> command</li> </ol>"},{"location":"appendix/infra_nkp_airgap/#download-offline-nkp-air-gapped-bundle","title":"Download Offline NKP Air-gapped Bundle","text":"<ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Downloads &gt; NKP Airgapped Bundle</li> <li>Select NKP for Linux and copy the download link to the <code>.tar.gz</code> file</li> <li> <p>If you haven't already done so, Open new <code>VSCode</code> window on your jumphost VM</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, click on existing <code>$HOME</code> folder</p> </li> <li> <p>Click on New Folder  name it: <code>airgap-nkp</code></p> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>$HOME/airgap-nkp</code> folder</p> </li> <li> <p>On <code>VSCode</code> menu, select <code>Terminal</code> &gt; <code>New Terminal</code></p> </li> <li> <p>Browse to <code>airgap-nkp</code> directory</p> <pre><code>cd $HOME/airgap-nkp\n</code></pre> </li> <li> <p>Download and extract the NKP air-gap bundle from the link you copied earlier</p> CommandSample command Paste the download URL within double quotes<pre><code>curl -o nkp-air-gapped-bundle_v2.12.1_linux_amd64.tar.gz \"_paste_download_URL_here\"\n</code></pre> <pre><code>curl -o nkp-air-gapped-bundle_v2.12.0_linux_amd64.tar.gz \"https://download.nutanix.com/downloads/nkp/v2.12.0/nkp_v2.12.0_linux_amd64.tar.gz?Expires=1729016864&amp;........\"\n</code></pre> <pre><code>tar xvfz nkp-air-gapped-bundle_v2.12.0_linux_amd64.tar.gz\n</code></pre> </li> <li> <p>Move the <code>nkp</code> binary to a directory that is included in your <code>PATH</code> environment variable</p> <pre><code>sudo cp nkp-v2.12.1/cli/nkp /usr/local/bin/\n</code></pre> </li> <li> <p>Verify the <code>nkp</code> binary is installed correctly. Ensure the version is latest</p> <p>Note</p> <p>At the time of writing this lab nkp version is <code>v2.15.0</code></p> CommandCommand output <pre><code>nkp version\n</code></pre> <pre><code>$ nkp version\ndiagnose: v0.11.0\nimagebuilder: v2.15.0\nkommander: v2.15.0\nkonvoy: v2.15.0\nmindthegap: v1.16.0\nnkp: v2.15.0\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_airgap/#setup-docker-on-jumphost","title":"Setup Docker on Jumphost","text":"<ol> <li>From VSC, logon to your jumpbox VM</li> <li>Open VSC Terminal</li> <li> <p>Run the following commands to install <code>docker</code> binaries</p> <pre><code>cd $HOME/sol-cnai-infra/; devbox init; devbox shell\ntask workstation:install-docker\n</code></pre> <p>Tip</p> <p>Restart the jumpbox host if <code>ubuntu</code> user has permission issues using <code>docker</code> commands.</p> </li> </ol>"},{"location":"appendix/infra_nkp_airgap/#load-docker-with-nkp-images","title":"Load Docker with NKP Images","text":"<ol> <li> <p>From VSC, open Terminal and run the following command to load the <code>nkp</code> images</p> <pre><code>cd $HOME/airgap-nkp/nkp-v2.12.1/cli\ndocker load -i nkp-image-builder-image-v0.13.1.tar\ndocker load -i konvoy-bootstrap-image-v2.12.0.tar\n</code></pre> </li> <li> <p>Confirm presence of container images on jumhost VM</p> CommandCommand output <pre><code>docker image ls\n</code></pre> <pre><code>$ docker image ls\n\nREPOSITORY                                                   TAG       IMAGE ID       CREATED        SIZE\nmesosphere/nkp-image-builder                                 v0.13.3   fbb35cce9a8f   44 years ago   594MB\nmesosphere/konvoy-bootstrap                                  v2.12.1   7ca8eeaa8381   3 weeks ago    2.15GB\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_airgap/#reserve-control-plane-and-metallb-ip","title":"Reserve Control Plane and MetalLB IP","text":"<p>Nutanix AHV IPAM network allows you to black list IPs that needs to be reserved for specific application endpoints. We will use this feature to find and reserve three IPs. </p> <p>We will reserve a total of three IPs for the following:</p> Cluster Role Cluster Name NKP NAI Dev <code>nkpdarksite</code> 2 1 <ol> <li> <p>Get the CIDR range for the AHV network(subnet) where the application will be deployed</p> CIDR example for your Nutanix cluster<pre><code>10.x.x.0/24\n</code></pre> </li> <li> <p>From VSC, logon to your jumpbox VM and open Terminal</p> </li> <li> <p>Install <code>nmap</code> tool (if not already done)</p> <pre><code>cd $HOME/sol-cnai-infra\ndevbox add nmap\n</code></pre> </li> <li> <p>Find three unused static IP addresses in the subnet</p> CommandSample command <pre><code>nmap -v -sn  &lt;your CIDR&gt;\n</code></pre> <pre><code>nmap -v -sn 10.x.x.0/24\n</code></pre> Sample output - choose the first three consecutive IPs<pre><code>Nmap scan report for 10.x.x.214 [host down]\nNmap scan report for 10.x.x.215 [host down]\nNmap scan report for 10.x.x.216 [host down]\nNmap scan report for 10.x.x.217\nHost is up (-0.098s latency).\n</code></pre> </li> <li> <p>Logon to any CVM in your Nutanix cluster and execute the following to add chosen static IPs to the AHV IPAM network</p> <ul> <li>Username: nutanix</li> <li>Password: your Prism Element password </li> </ul> CommandSample command <pre><code>acli net.add_to_ip_blacklist &lt;your-ipam-ahv-network&gt; \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> <pre><code>acli net.add_to_ip_blacklist User1 \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_airgap/#reservation-of-ips","title":"Reservation of IPs","text":"<p>Reserve the firs IPs for NKP control plane  Reserve the second two IPs for MetalLB distributed load balancer - We will use one of these IP for NAI</p> <p>Reserve the third IP for NAI. We will use the NAI IP in the next NAI section to assign the FDQN and install SSL certificate.</p> Component IP FQDN NKP Control Plane VIP <code>10.x.x.214</code> - NKP MetalLB IP Range <code>10.x.x.215-10.x.x.216</code> - NAI <code>10.x.x.216</code> <code>nai.10.x.x.216.nip.io</code>"},{"location":"appendix/infra_nkp_airgap/#create-base-image-for-nkp","title":"Create Base Image for NKP","text":"<p>In this section we will go through creating a base image for all the control plane and worker node VMs on Nutanix.</p> <ol> <li> <p>In VSC Explorer pane, Click on New Folder </p> </li> <li> <p>Call the folder <code>airgap-nkp</code> under <code>$HOME</code> directory</p> </li> <li> <p>In the <code>airgap-nkp</code> folder, click on New File  and create new file with the following name:</p> <pre><code>.env\n</code></pre> </li> <li> <p>Run the following command to generate an new RSA key pair on the jumphost VM. This SSH key pair will be used for authentication between the jumphost and NKP K8S cluster nodes.</p> Do you have existing SSH key pair? <p>Copy the key pair from your workstation (PC/Mac) to <code>~/.ssh/</code> directory on your Jumphost VM.</p> <pre><code>mac/pc $ scp ~/.ssh/id_rsa.pub ubuntu@10.x.x.171:~/.ssh/id_rsa.pub\nmac/pc $ scp ~/.ssh/id_rsa ubuntu@10.x.x.171:~/.ssh/id_rsa\n</code></pre> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>Accept the default file location as <code>~/.ssh/id_rsa</code></p> <p>SSH key pair will stored in the following location:</p> <pre><code>~/.ssh/id_rsa.pub \n~/.ssh/id_rsa\n</code></pre> </li> <li> <p>Fill the following values inside the <code>.env</code> file</p> Template .envSample .env <pre><code>export NUTANIX_USER=_your_nutanix_username\nexport NUTANIX_PASSWORD=_your_nutanix_password\nexport NUTANIX_ENDPOINT=_your_prism_central_fqdn\nexport NUTANIX_CLUSTER=_your_prism_element_cluster_name\nexport NUTANIX_SUBNET_NAME=_your_ahv_ipam_network_name\nexport STORAGE_CONTAINER=_your_storage_container_nmae\nexport SSH_PUBLIC_KEY=_path_to_ssh_pub_key_on_jumphost_vm\nexport NKP_CLUSTER_NAME=_your_nkp_cluster_name\nexport CONTROLPLANE_VIP=_your_nkp_cluster_controlplane_ip\nexport LB_IP_RANGE=_your_range_of_two_ips\nexport OS_BUNDLE_DIR=_your_artifacts_directory\nexport OS=_your_preferred_os\nexport BASE_IMAGE=_your_baseimage_name\n</code></pre> <pre><code>export NUTANIX_USER=admin\nexport NUTANIX_PASSWORD=xxxxxxxx\nexport NUTANIX_ENDPOINT=pc.example.com\nexport NUTANIX_CLUSTER=pe\nexport NUTANIX_SUBNET_NAME=User1\nexport STORAGE_CONTAINER=default\nexport SSH_PUBLIC_KEY=$HOME/.ssh/id_rsa.pub\nexport NKP_CLUSTER_NAME=nkpdarksite\nexport CONTROLPLANE_VIP=10.x.x.214\nexport LB_IP_RANGE=10.x.x.215-10.x.x.216\nexport OS_BUNDLE_DIR=kib/artifacts\nexport OS=ubuntu-22.04\nexport BASE_IMAGE=ubuntu-22.04-server-cloudimg-amd64.img\n</code></pre> </li> <li> <p>Using VSC Terminal, load the environment variables and its values</p> <pre><code>source $HOME/airgap-nkp/.env\ncd nkp-v2.12.1/\n</code></pre> </li> <li> <p>Create the base image</p> CommandCommand output <pre><code>nkp create package-bundle --artifacts-directory ${OS_BUNDLE_DIR} ${OS}\n</code></pre> <pre><code>$ nkp create package-bundle --artifacts-directory ${OS_BUNDLE_DIR} ${OS}\n\nOS bundle configuration files extracted to /home/ubuntu/airgap-nkp/nkp-v2.12.1/kib/artifacts/.dkp-image-builder-2593079857\nGet:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\nGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n\n&lt;snip&gt;\n\nGet:241 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 gpgv2 all 2.2.27-3ubuntu2.1 [4392 B]                              \nGet:242 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1306 kB]                  \nGet:243 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]               \nFetched 176 MB in 9s (19.9 MB/s) \ndpkg-scanpackages: info: Wrote 243 entries to output Packages file.\n/home/ubuntu/airgap-nkp/nkp-v2.12.1/kib/artifacts/.dkp-image-builder-2593079857/ubuntu-22.04/Packages       \n</code></pre> </li> <li> <p>Create the os image and upload to Prism Central using the following command. </p> <p>Note</p> <p>Image creation will take up to 5 minutes.</p> CommandCommand output <pre><code>nkp create image nutanix ${OS} --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} --subnet ${NUTANIX_SUBNET_NAME} --source-image ${BASE_IMAGE} --artifacts-directory ${OS_BUNDLE_DIR}\n</code></pre> <pre><code>nkp create image nutanix ${OS} --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} --subnet ${NUTANIX_SUBNET_NAME} --source-image ${BASE_IMAGE} --artifacts-directory ${OS_BUNDLE_DIR}\n\n&gt; Provisioning and configuring image\nManifest files extracted to $HOME/nkp/.nkp-image-builder-3243021807\nnutanix.kib_image: output will be in this color.\n\n==&gt; nutanix.kib_image: Creating Packer Builder virtual machine...\n    nutanix.kib_image: Virtual machine nkp-ubuntu-22.04-1.29.6-20240717082720 created\n    nutanix.kib_image: Found IP for virtual machine: 10.x.x.234\n==&gt; nutanix.kib_image: Running post-processor: packer-manifest (type manifest)\n\n---&gt; 100%\nBuild 'nutanix.kib_image' finished after 4 minutes 55 seconds.\n==&gt; Wait completed after 4 minutes 55 seconds\n\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> <p>Image name - This will be different in your environment</p> <p>Note image name from the previous <code>nkp</code> create image command output</p> <pre><code>==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> <p>Warning</p> <p>Make sure to use image name that is generated in your environment for the next steps.</p> </li> <li> <p>Populate the <code>.env</code> file with the NKP image name by adding (appending) the following environment variables and save it</p> Template .envSample .env <pre><code>export NKP_IMAGE=nkp-image-name\n</code></pre> <pre><code>export NKP_IMAGE=nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_airgap/#push-container-images-to-localprivate-registry-to-be-used-by-nkp","title":"Push Container Images to Local/Private Registry to be used by NKP","text":"<p>Warning</p> <p>This section requires a air-gapped container registry to be functional.</p> <p>If you haven't already deployed the air gapped container registry, please follow the steps in Harbor Container Registry section to deploy a Harbor container registry on the jumphost VM.</p> <ol> <li> <p>Open <code>$HOME/airgap-nkp/.env</code> file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export REGISTRY_URL=_your_registry_url\nexport REGISTRY_USERNAME=_your_registry_username\nexport REGISTRY_PASSWORD=_your_registry_password\nexport REGISTRY_CACERT=_path_to_ca_cert_of_registry\n</code></pre> <pre><code>export REGISTRY_URL=https://harbor.10.x.x.111.nip.io/nkp\nexport REGISTRY_USERNAME=admin\nexport REGISTRY_PASSWORD=xxxxxxxx\nexport REGISTRY_CACERT=$HOME/harbor/certs/ca.crt\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source $HOME/airgap-nkp/.env\n</code></pre> </li> <li> <p>Push the images to air-gapped registry</p> <pre><code>cd nkp-v2.12.1/\n</code></pre> CommandCommand output <p><pre><code>nkp push bundle --bundle ./container-images/konvoy-image-bundle-v2.12.1.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n</code></pre> <pre><code>nkp push bundle --bundle ./container-images/kommander-image-bundle-v2.12.1.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n</code></pre> <pre><code>nkp push bundle --bundle ./container-images/nkp-catalog-applications-image-bundle-v2.12.1.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n</code></pre></p> <p><pre><code>$ nkp push bundle --bundle ./container-images/konvoy-image-bundle-v2.12.1.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n\u2713 Creating temporary directory\n\u2713 Unarchiving image bundle \"./container-images/konvoy-image-bundle-v2.12.1.tar\" \n\u2713 Parsing image bundle config\n\u2713 Starting temporary Docker registry\n\u2713 Pushing bundled images [================================&gt;129/129] (time elapsed 153s) \n</code></pre> <pre><code>$ nkp push bundle --bundle ./container-images/kommander-image-bundle-v2.12.1.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n\u2713 Creating temporary directory\n\u2713 Unarchiving image bundle \"./container-images/kommander-image-bundle-v2.12.1.tar\" \n\u2713 Parsing image bundle config\n\u2713 Starting temporary Docker registry\n\u2713 Pushing bundled images [================================&gt;131/131] (time elapsed 183s) \n(devbox) \n</code></pre> <pre><code>nkp push bundle --bundle ./container-images/nkp-catalog-applications-image-bundle-v2.12.1.tar \\\n--to-registry=${REGISTRY_URL} --to-registry-username=${REGISTRY_USERNAME} \\\n--to-registry-password=${REGISTRY_PASSWORD} \\\n--to-registry-ca-cert-file=${REGISTRY_CACERT}\n\u2713 Creating temporary directory\n\u2713 Unarchiving image bundle \"./container-images/nkp-catalog-applications-image-bundle-v2.12.1.tar\" \n\u2713 Parsing image bundle config\n\u2713 Starting temporary Docker registry\n\u2713 Pushing bundled images [====================================&gt;8/8] (time elapsed 25s) \n(devbox) \n</code></pre></p> </li> </ol> <p>We are now ready to install the workload <code>nkpdarksite</code> cluster</p>"},{"location":"appendix/infra_nkp_airgap/#create-air-gapped-nkp-workload-cluster","title":"Create Air-gapped NKP Workload Cluster","text":"<ol> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export CONTROL_PLANE_REPLICAS=_no_of_control_plane_replicas\nexport CONTROL_PLANE_VCPUS=_no_of_control_plane_vcpus\nexport CONTROL_PLANE_CORES_PER_VCPU=_no_of_control_plane_cores_per_vcpu\nexport CONTROL_PLANE_MEMORY_GIB=_no_of_control_plane_memory_gib\nexport WORKER_REPLICAS=_no_of_worker_replicas\nexport WORKER_VCPUS=_no_of_worker_vcpus\nexport WORKER_CORES_PER_VCPU=_no_of_worker_cores_per_vcpu\nexport WORKER_MEMORY_GIB=_no_of_worker_memory_gib\nexport CSI_FILESYSTEM=_preferred_filesystem_ext4/xfs\nexport CSI_HYPERVISOR_ATTACHED=_true/false\nexport NUTANIX_PROJECT_NAME=_your_pc_project_name\n</code></pre> <pre><code>export CONTROL_PLANE_REPLICAS=3\nexport CONTROL_PLANE_VCPUS=4\nexport CONTROL_PLANE_CORES_PER_VCPU=1\nexport CONTROL_PLANE_MEMORY_GIB=16\nexport WORKER_REPLICAS=4\nexport WORKER_VCPUS=8 \nexport WORKER_CORES_PER_VCPU=1\nexport WORKER_MEMORY_GIB=32\nexport CSI_FILESYSTEM=ext4\nexport CSI_HYPERVISOR_ATTACHED=true\nexport NUTANIX_PROJECT_NAME=dev-lab\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source $HOME/airgap-nkp/.env\n</code></pre> </li> <li> <p>In VSC, open Terminal, enter the following command to create the workload cluster</p> Check your command for correct argument values <p>Run the following command to verify your <code>nkp</code> command and associated environment variables and values.</p> <pre><code>echo \"nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n        --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n        --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n        --control-plane-vm-image ${NKP_IMAGE} \\\n        --csi-storage-container ${STORAGE_CONTAINER} \\\n        --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n        --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n        --worker-vm-image ${NKP_IMAGE} \\\n        --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n        --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n        --control-plane-disk-size 150 \\\n        --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} \\\n        --control-plane-vcpus ${CONTROL_PLANE_VCPUS} \\\n        --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n        --worker-disk-size 150 \\\n        --worker-memory ${WORKER_MEMORY_GIB} \\\n        --worker-vcpus ${WORKER_VCPUS} \\\n        --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n        --csi-file-system ${CSI_FILESYSTEM} \\\n        --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n        --registry-mirror-url=${REGISTRY_URL} \\\n        --registry-mirror-username=${REGISTRY_USERNAME} \\\n        --registry-mirror-password=${REGISTRY_PASSWORD} \\\n        --registry-mirror-cacert=${REGISTRY_CACERT} \\\n        --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --self-managed \\\n        --airgapped\"\n</code></pre> <p>If the values are incorrect, add the correct values to <code>.env</code> and source the  again by running the following command</p> <pre><code>source $HOME/airgap-nkp/.env\n</code></pre> <p>Then rerun the <code>echo nkp</code> command to verify the values again before running the <code>nkp create cluster nutanix</code> command.</p> CommandCommand output <pre><code>nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n    --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n    --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n    --control-plane-vm-image ${NKP_IMAGE} \\\n    --csi-storage-container ${STORAGE_CONTAINER} \\\n    --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n    --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n    --worker-vm-image ${NKP_IMAGE} \\\n    --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n    --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n    --control-plane-disk-size 150 \\\n    --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} \\\n    --control-plane-vcpus ${CONTROL_PLANE_VCPUS} \\\n    --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n    --worker-disk-size 150 \\\n    --worker-memory ${WORKER_MEMORY_GIB} \\\n    --worker-vcpus ${WORKER_VCPUS} \\\n    --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n    --csi-file-system ${CSI_FILESYSTEM} \\\n    --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n    --registry-mirror-url=${REGISTRY_URL} \\\n    --registry-mirror-username=${REGISTRY_USERNAME} \\\n    --registry-mirror-password=${REGISTRY_PASSWORD} \\\n    --registry-mirror-cacert=${REGISTRY_CACERT} \\\n    --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --self-managed \\\n    --airgapped\n</code></pre> <pre><code>&gt; \u2713 Creating a bootstrap cluster \n\u2713 Upgrading CAPI components \n\u2713 Waiting for CAPI components to be upgraded \n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Creating ClusterClass resources\n&gt; Generating cluster resources\ncluster.cluster.x-k8s.io/nkpdarksite created\nsecret/nkpdarksite-pc-credentials created\nsecret/nkpdarksite-pc-credentials-for-csi created\nsecret/nkpdarksite-image-registry-credentials created\n\u2713 Waiting for cluster infrastructure to be ready \n\u2713 Waiting for cluster control-planes to be ready \n\u2713 Waiting for machines to be ready\n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Moving cluster resources\n\n&gt; You can now view resources in the moved cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdarksite.conf\" get nodes\n\n&gt; \u2713 Deleting bootstrap cluster \n\nCluster default/nkpdarksite kubeconfig was written to to the filesystem.\nYou can now view resources in the new cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdarksite.conf\" get nodes\n\n&gt; Starting kommander installation\n\u2713 Deploying Flux \n\u2713 Deploying Ingress certificate \n\u2713 Creating kommander-overrides ConfigMap\n\u2713 Deploying Git Operator \n\u2713 Creating GitClaim for management GitRepository \n\u2713 Creating GitClaimUser for accessing management GitRepository \n\u2713 Creating HTTP Proxy configuration\n\u2713 Deploying Flux configuration\n\u2713 Deploying Kommander Operator \n\u2713 Creating KommanderCore resource \n\u2713 Cleaning up kommander bootstrap resources\n\u2713 Deploying Substitution variables\n\u2713 Deploying Flux configuration \n\u2713 Deploying Gatekeeper \n\u2713 Deploying Kommander AppManagement \n\u2713 Creating Core AppDeployments \n\u2713 4 out of 12 core applications have been installed (waiting for dex, dex-k8s-authenticator and 6 more) \n\u2713 5 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 5 more) \n\u2713 7 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 3 more) \n\u2713 8 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 2 more) \n\u2713 9 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 1 more) \n\u2713 10 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, traefik-forward-auth-mgmt) \n\u2713 11 out of 12 core applications have been installed (waiting for traefik-forward-auth-mgmt) \n\u2713 Creating cluster-admin credentials\n\n&gt; Cluster was created successfully! Get the dashboard details with:\n&gt; nkp get dashboard --kubeconfig=\"$HOME/airgap-nkp/nkpdarksite.conf\"\n</code></pre> <p>Deployment info</p> <p>The above command with the use of <code>--self-managed</code> argument, will create a bootstrap cluster, deploy CAPI resources on it and create a NKP base cluster (konvoy) using the CAPI components in the bootstrap cluster. It will automatically do the following once the NKP base cluster is provisioned:</p> <ul> <li>Deploy CAPI components on the bootstrap cluster</li> <li>Move the CAPI components from the bootstrap cluster to the new cluster</li> <li>Delete the Bootstrap cluster</li> <li>Deploy the Kommander components on top of the new base NKP cluster </li> </ul> <p>See NKP the Hard Way section for more information for customizable NKP cluster deployments. </p> </li> <li> <p>Observe the events in the shell and in Prism Central events</p> </li> <li> <p>Store kubeconfig file for bootstrap cluster</p> <pre><code>kind get kubeconfig --name konvoy-capi-bootstrapper &gt; bs.cfg\nexport KUBECONFIG=bs.cfg\n</code></pre> </li> <li> <p>Store kubeconfig files for the workload cluster</p> <pre><code>nkp get kubeconfig -c ${NKP_CLUSTER_NAME} &gt; ${NKP_CLUSTER_NAME}.cfg\n</code></pre> </li> <li> <p>Combine the bootstrap and workload clusters <code>KUBECONFIG</code> file so that we can use it with <code>kubectx</code>command to change context between clusters</p> <pre><code>export KUBECONFIG=bs.cfg:${NKP_CLUSTER_NAME}.cfg\nkubectl config view --flatten &gt; all-in-one-kubeconfig.yaml\nexport KUBECONFIG=all-in-one-kubeconfig.yaml\n</code></pre> </li> <li> <p>Run the following command to check K8S status of the <code>nkpdarksite</code> cluster</p> CommandCommand output <pre><code>kubectx ${NKP_CLUSTER_NAME}-admin@${NKP_CLUSTER_NAME} \nkubectl get nodes\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                  STATUS   ROLES           AGE     VERSION\nnkpdarksite-md-0-x948v-hvxtj-9r698           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdarksite-md-0-x948v-hvxtj-fb75c           Ready    &lt;none&gt;          4h50m   v1.29.6\nnkpdarksite-md-0-x948v-hvxtj-mdckn           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdarksite-md-0-x948v-hvxtj-shxc8           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdarksite-r4fwl-8q4ch                      Ready    control-plane   4h50m   v1.29.6\nnkpdarksite-r4fwl-jf2s8                      Ready    control-plane   4h51m   v1.29.6\nnkpdarksite-r4fwl-q888c                      Ready    control-plane   4h49m   v1.29.6\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_airgap/#add-nkp-gpu-workload-pool","title":"Add NKP GPU Workload Pool","text":"<p>Are you just deploying NKP?</p> <p>If you are doing this lab only to deploy NKP, then you can skip this GPU section.</p> <p>The steps below covers the following:     - Retrieving and Applying NKP Pro License     - Identifying the GPU device name     - Deploying the GPU nodepool     - Enabling the NVIDIA GPU Operator</p> <p>Note</p> <p>To Enable the GPU Operator afterwards using the NKP Marketplace, a minimal NKP Pro license is required.</p>"},{"location":"appendix/infra_nkp_airgap/#find-gpu-device-details","title":"Find GPU Device Details","text":"<p>As we will be deploying Nutanix Enterprise AI (NAI) in the next section, we need to find the GPU details beforehand.</p> <p>Find the details of GPU on the Nutanix cluster while still connected to Prism Central (PC).</p> <ol> <li>Logon to Prism Central GUI</li> <li>On the general search, type GPUs</li> <li> <p>Click on the GPUs result</p> <p></p> </li> <li> <p><code>Lovelace 40s</code> is the GPU available for use</p> </li> <li>Use <code>Lovelace 40s</code> in the evironment variables in the next section.</li> </ol>"},{"location":"appendix/infra_nkp_airgap/#create-nkp-gpu-workload-pool","title":"Create NKP GPU Workload Pool","text":"<p>In this section we will create a nodepool to host the AI apps with a GPU.</p> <ol> <li> <p>Open <code>.env</code> file in VSC and add (append) the following environment variables and save it</p> Template .envSample .env <pre><code>export GPU_NAME=_name_of_gpu_device_\nexport GPU_REPLICA_COUNT=_no_of_gpu_worker_nodes\nexport GPU_POOL=_name_of_gpu_pool\nexport GPU_NODE_VCPUS=_no_of_gpu_node_vcpus\nexport GPU_NODE_CORES_PER_VCPU=_per_gpu_node_cores_per_vcpu\nexport GPU_NODE_MEMORY_GIB=_per_gpu_node_memory_gib\nexport GPU_NODE_DISK_SIZE_GIB=_per_gpu_node_memory_gib\n</code></pre> <pre><code>export GPU_NAME=\"Lovelace 40S\"\nexport GPU_REPLICA_COUNT=1\nexport GPU_POOL=gpu-nodepool\nexport GPU_NODE_VCPUS=16\nexport GPU_NODE_CORES_PER_VCPU=1\nexport GPU_NODE_MEMORY_GIB=40\nexport GPU_NODE_DISK_SIZE_GIB=200\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source $HOME/airgap-nkp/.env\n</code></pre> </li> <li> <p>Run the following command to create a GPU nodepool manifest</p> <pre><code>nkp create nodepool nutanix \\\n    --cluster-name ${NKP_CLUSTER_NAME} \\\n    --prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --pc-project ${NUTANIX_PROJECT_NAME} \\\n    --subnets ${NUTANIX_SUBNET_NAME} \\\n    --vm-image ${NKP_IMAGE} \\\n    --disk-size ${GPU_NODE_DISK_SIZE_GIB} \\\n    --memory ${GPU_NODE_MEMORY_GIB} \\\n    --vcpus ${GPU_NODE_VCPUS} \\\n    --cores-per-vcpu ${GPU_NODE_CORES_PER_VCPU} \\\n    --replicas ${GPU_REPLICA_COUNT} \\\n    --wait \\\n    ${GPU_POOL} --dry-run -o yaml &gt; gpu-nodepool.yaml\n</code></pre> <p>Note</p> <p>Right now there is no switch for GPU in <code>nkp</code> command. We need to do dry-run the output into a file and then add the necessary GPU specifications</p> </li> <li> <p>Add the necessary gpu section to our new <code>gpu-nodepool.yaml</code> using <code>yq</code> command</p> <pre><code>yq e '(.spec.topology.workers.machineDeployments[] | select(.name == \"gpu-nodepool\").variables.overrides[] | select(.name == \"workerConfig\").value.nutanix.machineDetails) += {\"gpus\": [{\"type\": \"name\", \"name\": strenv(GPU_NAME)}]}' -i gpu-nodepool.yaml\n</code></pre> Successful addtion of GPU specs? <p>You would be able to see the added gpu section at the end of the <code>gpu-nodepool.yaml</code> file</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\n\n&lt;snip&gt;\n\n  name: gpu-nodepool\n  variables:\n    overrides:\n      - name: workerConfig\n        value:\n          nutanix:\n            machineDetails:\n              bootType: legacy\n              cluster:\n                name: romanticism\n                type: name\n              image:\n                name: nkp-ubuntu-22.04-1.29.6-20240718055804\n                type: name\n              memorySize: 40Gi\n              subnets:\n                - name: User1\n                  type: name\n              systemDiskSize: 200Gi\n              vcpuSockets: 16\n              vcpusPerSocket: 1\n              gpus:\n                - type: name\n                  name: Lovelace 40S\n</code></pre> </li> <li> <p>Monitor Cluster-Api resources to ensure gpu machine will be successfully</p> <pre><code>watch kubectl get cluster-api\n</code></pre> </li> <li> <p>Apply the <code>gpu-nodepool.yaml</code> file to the workload cluster</p> <pre><code>kubectl apply -f gpu-nodepool.yaml\n</code></pre> </li> <li> <p>Monitor the progress of the command and check Prism Central events for creation of the GPU worker node</p> <p>Change to workload <code>nkpdev</code> cluster context</p> <pre><code>kubectx ${NKP_CLUSTER_NAME}-admin@${NKP_CLUSTER_NAME}\n</code></pre> </li> <li> <p>Check nodes status in workload <code>nkpdarksite</code> cluster and note the gpu worker node</p> CommandCommand output <pre><code>kubectl get nodes -w\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                   STATUS   ROLES           AGE     VERSION\nnkpdarksite-gpu-nodepool-7g4jt-2p7l7-49wvd   Ready    &lt;none&gt;          5m57s   v1.29.6\nnkpdarksite-md-0-q679c-khl2n-9k7jk           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdarksite-md-0-q679c-khl2n-9nk6h           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdarksite-md-0-q679c-khl2n-nf9p6           Ready    &lt;none&gt;          73m     v1.29.6\nnkpdarksite-md-0-q679c-khl2n-qgxp9           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdarksite-ncnww-2dg7h                      Ready    control-plane   73m     v1.29.6\nnkpdarksite-ncnww-bbm4s                      Ready    control-plane   72m     v1.29.6\nnkpdarksite-ncnww-hldm9                      Ready    control-plane   75m     v1.29.6\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_airgap/#licensing","title":"Licensing","text":"<p>We need to generate a license for the NKP cluster which is the total for all the vCPUs used by worker nodes.</p> <p>For example, in the Sizing Requirements section, the NKP Demo Cluster <code>Total vCPU count</code> is equal to <code>60</code>, whereas the actual worker nodes total vCPU count is only <code>48</code>.</p>"},{"location":"appendix/infra_nkp_airgap/#generate-nkp-pro-license","title":"Generate NKP Pro License","text":"<p>To generate a NKP Pro License for the NKP cluster:</p> <p>Note</p> <p>Nutanix Internal users should logon using Nutanix SSO</p> <p>Nutanix Partners/Customers should logon to Portal using their Nutanix Portal account credentials</p> <ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Licensing &gt; License Summary</li> <li>Click on the small drop down arrow  on Manage Licenses and choose Nutanix Kubernetes Platform (NKP)</li> <li>Input the NKP cluster name</li> <li>Click on the plus icon </li> <li>Click on Next in the bottom right corner</li> <li>Select NKP Pro License</li> <li>Select Apply to cluster</li> <li>Choose Non-production license and Save</li> <li>Select the cluster name and click on Next</li> <li>Input the number of vCPU (<code>60</code>) from our calculations in the previous section</li> <li>Click on Save</li> <li>Download the csv file and store it in a safe place</li> </ol>"},{"location":"appendix/infra_nkp_airgap/#applying-nkp-pro-license-to-nkp-cluster","title":"Applying NKP Pro License to NKP Cluster","text":"<ol> <li> <p>Login to the Kommander URL for <code>nkpdarksite</code> cluster with the generated credentials that was generated in the previous section. The following commands will give you the credentials and URL.</p> CommandCommand output <pre><code>nkp get dashboard\n</code></pre> <pre><code>nkp get dashboard\n\nUsername: recursing_xxxxxxxxx\nPassword: YHbPsslIDB7p7rqwnfxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nURL: https://10.x.x.215/dkp/kommander/dashboard\n</code></pre> </li> <li> <p>Go to Licensing and click on Remove License to remove the Starter license</p> </li> <li>Type nutanix-license in the confirmation box and click on Remove License</li> <li>Click on Add License, choose Nutanix platform and paste the license key from the previous section</li> <li>Click on Save</li> <li>Confirm the license is applied to the cluster by cheking the License Status in the License menu</li> <li>The license will be applied to the cluster and the license status will reflect NKP Pro in the top right corner of the dashboard</li> </ol>"},{"location":"appendix/infra_nkp_airgap/#enable-gpu-operator","title":"Enable GPU Operator","text":"<p>We will need to enable GPU operator for deploying NKP application. </p> <ol> <li>In the NKP GUI, Go to Clusters</li> <li>Click on Kommander Host</li> <li>Go to Applications </li> <li>Search for NVIDIA GPU Operator</li> <li>Click on Enable</li> <li>Click on Configuration tab</li> <li> <p>Click on Workspace Application Configuration Override and paste the following yaml content</p> <pre><code>driver:\n  enabled: true\n</code></pre> <p>As shown here:</p> <p></p> </li> <li> <p>Click on Enable on the top right-hand corner to enable GPU driver on the Ubuntu GPU nodes</p> </li> <li> <p>Check GPU operator resources and make sure they are running</p> CommandCommand output <pre><code>kubectl get po -A | grep -i nvidia\n</code></pre> <pre><code>kubectl get po -A | grep -i nvidia\n\nnvidia-container-toolkit-daemonset-fjzbt                          1/1     Running     0          28m\nnvidia-cuda-validator-f5dpt                                       0/1     Completed   0          26m\nnvidia-dcgm-exporter-9f77d                                        1/1     Running     0          28m\nnvidia-dcgm-szqnx                                                 1/1     Running     0          28m\nnvidia-device-plugin-daemonset-gzpdq                              1/1     Running     0          28m\nnvidia-driver-daemonset-dzf55                                     1/1     Running     0          28m\nnvidia-operator-validator-w48ms                                   1/1     Running     0          28m\n</code></pre> </li> <li> <p>Run a sample GPU workload to confirm GPU operations</p> CommandCommand output <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre> <pre><code>pod/cuda-vector-add created\n</code></pre> </li> <li> <p>Follow the logs to check if the GPU operations are successful</p> CommandSample CommandCommand output <pre><code>kubectl logs _gpu_worload_pod_name\n</code></pre> <pre><code>kubectl logs cuda-vector-add-xxx\n</code></pre> <pre><code>kubectl logs cuda-vector-add\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\n</code></pre> </li> </ol> <p>Now we are ready to deploy our AI workloads.</p>"},{"location":"appendix/infra_nkp_hard_way/","title":"Deploy NKP Clusters","text":"<p>This section will take you through install NKP(Kubernetes) on Nutanix cluster as we will be deploying AI applications on these kubernetes clusters. </p> <p>This section will expand to other available Kubernetes implementations on Nutanix.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKPk8s {\n        [*] --&gt; CreateBootStrapCluster\n        CreateBootStrapCluster --&gt; CreateNKPCluster\n        CreateNKPCluster --&gt; DeployKommander\n        DeployKommander --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n\n    PrepWorkstation --&gt; DeployJumpHost \n    DeployJumpHost --&gt; DeployNKPk8s \n    DeployNKPk8s --&gt; DeployAIApps : Next section</code></pre>"},{"location":"appendix/infra_nkp_hard_way/#nkp-high-level-cluster-design","title":"NKP High Level Cluster Design","text":"<p>The <code>Bootstrap</code> NKP cluster will be a temporary kind cluster that will be used to deploy the <code>nkpdev</code> cluster.</p> <p>The <code>nkpdev</code> cluster will be hosting the LLM model serving endpoints and AI application stack. This cluster and will require a dedicated GPU node pool.</p> <p>Once <code>nkpdev</code> deployment has been tested successfully, we can deploy applications to optional PROD Workload cluster.</p>"},{"location":"appendix/infra_nkp_hard_way/#management-cluster","title":"Management Cluster","text":"<p>Since the Management Cluster called <code>nkpmanage</code> will be essential to deploying a workload <code>nkpdev</code> cluster. </p> <p>We will use <code>Kind</code> cluster packaged by Nutanix to deploy the management cluster.</p> Role No. of Nodes (VM) vCPU RAM Storage Master 1 4 6 GB 150 GB Worker 2 4 6 GB 150 GB"},{"location":"appendix/infra_nkp_hard_way/#dev-workload-cluster","title":"Dev Workload Cluster","text":"<p>For <code>nkpdev</code>, we will deploy an NKP Cluster of with the following resources to be able to deploy <code>LLama 8B</code> LLM. See Sizing Requirements section of this site for more information.</p> Role No. of Nodes (VM) vCPU RAM Storage Master 3 4 16 GB 150 GB Worker 4 8 32 GB 150 GB GPU 1 16 64 GB 200 GB"},{"location":"appendix/infra_nkp_hard_way/#pre-requisites-for-nkp-deployment","title":"Pre-requisites for NKP Deployment","text":"<ol> <li>Existing Jumphost VM. See here for installation steps</li> <li>Download and install <code>nkp</code> binary from Nutanix Portal</li> <li>Find and reserve 3 IPs for control plane and MetalLB access from AHV network</li> <li>Find GPU details from Nutanix cluster</li> <li>Create a base image to use with NKP nodes using <code>nkp</code> command</li> </ol>"},{"location":"appendix/infra_nkp_hard_way/#install-nkp-binaries","title":"Install NKP Binaries","text":"<ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Downloads &gt; Nutanix Kubernetes Platform (NKP)</li> <li>Select NKP for Linux and copy the download link to the <code>.tar.gz</code> file</li> <li> <p>If you haven't already done so, Open new <code>VSCode</code> window on your jumphost VM</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, click on existing <code>$HOME</code> folder</p> </li> <li> <p>Click on New Folder  name it: <code>nkp</code></p> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>$HOME/nkp</code> folder</p> </li> <li> <p>On <code>VSCode</code> menu, select <code>Terminal</code> &gt; <code>New Terminal</code></p> </li> <li> <p>Browse to <code>nkp</code> directory</p> <pre><code>cd $HOME/nkp\n</code></pre> </li> <li> <p>Download and extract the NKP binary from the link you copied earlier</p> CommandSample command Paste the download URL within double quotes<pre><code>curl -o nkp_v2.16.0_linux_amd64.tar.gz \"_paste_download_URL_here\"\n</code></pre> <pre><code>curl -o nkp_v2.16.0_linux_amd64.tar.gz \"https://download.nutanix.com/downloads/nkp/v2.16.0/nkp_v2.16.0_linux_amd64.tar.gz?Expires=1729016864&amp;........\"\n</code></pre> <pre><code>tar xvfz nkp_v2.16.0_linux_amd64.tar\n</code></pre> </li> <li> <p>Move the <code>nkp</code> binary to a directory that is included in your <code>PATH</code> environment variable</p> <pre><code>sudo cp nkp /usr/local/bin/\n</code></pre> </li> <li> <p>Verify the <code>nkp</code> binary is installed correctly. Ensure the version is latest</p> <p>Note</p> <p>At the time of writing this lab nkp version is <code>v2.16.0</code></p> CommandCommand Output <pre><code>nkp version\n</code></pre> <pre><code>$ nkp version\ncatalog: v0.7.0\ndiagnose: v0.12.0\nimagebuilder: v2.16.0\nkommander: v2.16.0\nkonvoy: v2.16.0\nmindthegap: v1.22.1\nnkp: v2.16.0\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_hard_way/#setup-docker-on-jumphost","title":"Setup Docker on Jumphost","text":"<ol> <li>From VSC, logon to your jumpbox VM</li> <li>Open VSC Terminal</li> <li> <p>Run the following commands to install <code>docker</code> binaries</p> <pre><code>cd $HOMEsol-cnai-infra/; devbox init; devbox shell\ntask workstation:install-docker\n</code></pre> <p>Tip</p> <p>Restart the jumpbox host if <code>ubuntu</code> user has permission issues using <code>docker</code> commands.</p> </li> </ol>"},{"location":"appendix/infra_nkp_hard_way/#reserve-control-plane-and-metallb-endpoint-ips","title":"Reserve Control Plane and MetalLB Endpoint IPs","text":"<p>Nutanix AHV IPAM network allows you to black list IPs that needs to be reserved for specific application endpoints. We will use this feature to find and reserve three IPs. </p> <p>We will need a total of three IPs for the following:</p> Cluster Role Cluster Name Control Plane IP MetalLB  IP Manage <code>nkpmanage</code> 1 2 Dev <code>nkpdev</code> 1 2 <ol> <li> <p>Get the CIDR range for the AHV network(subnet) where the application will be deployed</p> CIDR example for your Nutanix cluster<pre><code>10.x.x.0/24\n</code></pre> </li> <li> <p>From VSC, logon to your jumpbox VM and open Terminal</p> </li> <li> <p>Install <code>nmap</code> tool (if not already done)</p> <pre><code>cd $HOME/sol-cnai-infra\ndevbox add nmap\n</code></pre> </li> <li> <p>Find six unused static IP addresses in the subnet</p> CommandSample command <pre><code>nmap -v -sn  &lt;your CIDR&gt;\n</code></pre> <pre><code>nmap -v -sn 10.x.x.0/24\n</code></pre> Sample output - choose the first three consecutive IPs<pre><code>Nmap scan report for 10.x.x.210 [host down]\nNmap scan report for 10.x.x.211 [host down]\nNmap scan report for 10.x.x.212 [host down]\nNmap scan report for 10.x.x.213\nHost is up (-0.098s latency).\nNmap scan report for 10.x.x.214 [host down] \nNmap scan report for 10.x.x.215 [host down] \nNmap scan report for 10.x.x.216 [host down]\nNmap scan report for 10.x.x.217\nHost is up (-0.098s latency).\n</code></pre> </li> <li> <p>Logon to any CVM in your Nutanix cluster and execute the following to add chosen static IPs to the AHV IPAM network</p> <ul> <li>Username: nutanix</li> <li>Password: your Prism Element password </li> </ul> CommandSample command <pre><code>acli net.add_to_ip_blacklist &lt;your-ipam-ahv-network&gt; \\\nip_list=10.x.x.210,10.x.x.211,10.x.x.212,10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> <pre><code>acli net.add_to_ip_blacklist User1 \\\nip_list=10.x.x.210,10.x.x.211,10.x.x.212,10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_hard_way/#optional-find-gpu-details","title":"Optional - Find GPU Details","text":"<p>If there is a requirement to deploy workloads that rquire GPU, find the GPU details in your Nutanix cluster.</p> <p>Find the details of GPU on the Nutanix cluster while still connected to Prism Central (PC).</p> <ol> <li>Logon to Prism Central GUI</li> <li>On the general search, type GPUs</li> <li> <p>Click on the GPUs result</p> <p></p> </li> <li> <p><code>Lovelace 40s</code> is the GPU available for use</p> </li> <li>Use <code>Lovelace 40s</code> in the evironment variables in the next section. </li> </ol>"},{"location":"appendix/infra_nkp_hard_way/#create-base-image-for-nkp","title":"Create Base Image for NKP","text":"<p>In this section we will go through creating a base image for all the control plane and worker node VMs on Nutanix.</p> <ol> <li> <p>In VSC Explorer pane, Click on New Folder </p> </li> <li> <p>Call the folder <code>nkp</code> under <code>$HOME</code> directory</p> </li> <li> <p>In the <code>nkp</code> folder, click on New File  and create new file with the following name:</p> <pre><code>.env\n</code></pre> </li> <li> <p>Fill the following values inside the <code>.env</code> file</p> Template .envSample .env <pre><code>export NUTANIX_USER=_your_nutanix_username\nexport NUTANIX_PASSWORD=_your_nutanix_password\nexport NUTANIX_ENDPOINT=_your_prism_central_cluster_name\nexport NUTANIX_CLUSTER=_your_prism_element_cluster_name\nexport NUTANIX_SUBNET_NAME=_your_ahv_ipam_network_name\nexport STORAGE_CONTAINER=_your_storage_container_nmae\nexport SSH_PUBLIC_KEY=_your_path_to_ssh_pub_key\nexport CONTROLPLANE_VIP=_your_nkp_cluster_controlplane_ip\nexport LB_IP_RANGE=_your_range_of_two_ips\n</code></pre> <pre><code>export NUTANIX_USER=admin\nexport NUTANIX_PASSWORD=xxxxxxxx\nexport NUTANIX_ENDPOINT=pc.example.com\nexport NUTANIX_CLUSTER=pe\nexport NUTANIX_SUBNET_NAME=User1\nexport STORAGE_CONTAINER=default\nexport SSH_PUBLIC_KEY=$HOME/.ssh/id_rsa.pub\nexport CONTROLPLANE_VIP=10.x.x.210\nexport LB_IP_RANGE=10.x.x.211-10.x.x.212\n</code></pre> </li> <li> <p>Using VSC Terminal, load the environment variables and its values</p> <pre><code>cd $HOME/nkp\nsource .env\n</code></pre> </li> <li> <p>Create the base image and upload to Prism Central using the following command. </p> <p>Note</p> <p>Image creation will take up to 5 minutes.</p> CommandExample CommandCommand output <pre><code>nkp create image nutanix ubuntu-22.04 \\\n  --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} \\\n  --subnet ${NUTANIX_SUBNET_NAME} --insecure\n</code></pre> <pre><code>nkp create image nutanix ubuntu-22.04 \\\n  --endpoint pc.example.com --cluster pe \\\n  --subnet User1 --insecure\n</code></pre> <pre><code>nkp create image nutanix ubuntu-22.04 \\\n  --endpoint pc.example.com --cluster pe \\\n  --subnet User1 --insecure\n\n&gt; Provisioning and configuring image\nManifest files extracted to $HOME/nkp/.nkp-image-builder-3243021807\nnutanix.kib_image: output will be in this color.\n\n==&gt; nutanix.kib_image: Creating Packer Builder virtual machine...\n    nutanix.kib_image: Virtual machine nkp-ubuntu-22.04-1.29.6-20240717082720 created\n    nutanix.kib_image: Found IP for virtual machine: 10.x.x.234\n==&gt; nutanix.kib_image: Running post-processor: packer-manifest (type manifest)\n\n---&gt; 100%\nBuild 'nutanix.kib_image' finished after 4 minutes 55 seconds.\n==&gt; Wait completed after 4 minutes 55 seconds\n\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> <p>Image name - This will be different in your environment</p> <p>Note image name from the previous <code>nkp</code> create image command output</p> <pre><code>==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.31.4-20250320042646\n</code></pre> </li> <li> <p>Populate the <code>.env</code> file with the NKP image name by adding (appending) the following environment variables and save it</p> Template .envSample .env <pre><code>export NKP_IMAGE=nkp-image-name\n</code></pre> <pre><code>export NKP_IMAGE=nkp-ubuntu-22.04-1.31.4-20250320042646\n</code></pre> <p>Warning</p> <p>Make sure to use image name that is generated in your environment for the next steps.</p> </li> </ol>"},{"location":"appendix/infra_nkp_hard_way/#create-a-nkp-management-k8s-cluster","title":"Create a NKP Management K8S Cluster","text":"<p>In this section we will create a NKP Management (bootstrap)  <code>nkpmanage</code> cluster.</p> <p>Warning</p> <p>We are creating the management cluster with minimal resoruces in this lab environment. </p> <p>Consider adding additional control plane nodes and increasing CPU and memory of NKP management cluster for production environments as discussed in the Pre-requisites section. </p> <ol> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export NKP_MGT_CLUSTER_NAME=_name_of_nkp_management_cluster\nexport CONTROL_PLANE_REPLICAS=_no_of_control_plane_replicas\nexport CONTROL_PLANE_VCPUS=_no_of_control_plane_vcpus\nexport CONTROL_PLANE_CORES_PER_VCPU=_no_of_control_plane_cores_per_vcpu\nexport CONTROL_PLANE_MEMORY_GIB=_no_of_control_plane_memory_gib\nexport WORKER_REPLICAS=_no_of_worker_replicas\nexport WORKER_VCPUS=_no_of_worker_vcpus\nexport WORKER_CORES_PER_VCPU=_no_of_worker_cores_per_vcpu\nexport WORKER_MEMORY_GIB=_no_of_worker_memory_gib\nexport CSI_FILESYSTEM=_preferred_filesystem_ext4/xfs\nexport CSI_HYPERVISOR_ATTACHED=_true/false\nexport DOCKER_USERNAME=_your_docker_username\nexport DOCKER_PASSWORD=_your_docker_password\nexport NUTANIX_PROJECT_NAME=_your_pc_project_name\n</code></pre> <pre><code>export NKP_MGT_CLUSTER_NAME=nkpmanage\nexport CONTROL_PLANE_REPLICAS=1\nexport CONTROL_PLANE_VCPUS=4\nexport CONTROL_PLANE_CORES_PER_VCPU=1\nexport CONTROL_PLANE_MEMORY_GIB=6\nexport WORKER_REPLICAS=2\nexport WORKER_VCPUS=4 \nexport WORKER_CORES_PER_VCPU=1\nexport WORKER_MEMORY_GIB=6\nexport CSI_FILESYSTEM=ext4\nexport CSI_HYPERVISOR_ATTACHED=true\nexport DOCKER_USERNAME=_your_docker_username\nexport DOCKER_PASSWORD=_your_docker_password\nexport NUTANIX_PROJECT_NAME=dev-lab\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source .env\n</code></pre> </li> <li> <p>In VSC, open Terminal, enter the following command to create the management cluster</p> Optional - Check your command for correct argument values <p>Run the following command to verify your <code>nkp</code> command and associated environment variables and values.</p> <pre><code>echo \"nkp create cluster nutanix -c ${NKP_MGT_CLUSTER_NAME} \\\n        --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n        --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n        --control-plane-vm-image ${NKP_IMAGE} \\\n        --csi-storage-container ${STORAGE_CONTAINER} \\\n        --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n        --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n        --worker-vm-image ${NKP_IMAGE} \\\n        --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n        --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n        --control-plane-disk-size 150 --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} --control-plane-vcpus ${CONTROL_PLANE_VCPUS} --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n        --worker-disk-size 150 --worker-memory ${WORKER_MEMORY_GIB} --worker-vcpus ${WORKER_VCPUS} --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n        --csi-file-system ${CSI_FILESYSTEM} \\\n        --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n        --registry-mirror-url \"https://registry-1.docker.io\" \\\n        --registry-mirror-username ${DOCKER_USERNAME} \\\n        --registry-mirror-password ${DOCKER_PASSWORD} \\\n        --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --insecure\"\n</code></pre> <p>If the values are incorrect, source the <code>.env</code> file again by running the following command</p> <pre><code>source .env\n</code></pre> <p>Then rerun the <code>echo nkp</code> command to verify the values again before running the <code>nkp create cluster nutanix</code> command.</p> CommandCommand output <pre><code>nkp create cluster nutanix -c ${NKP_MGT_CLUSTER_NAME} \\\n    --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n    --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n    --control-plane-vm-image ${NKP_IMAGE} \\\n    --csi-storage-container ${STORAGE_CONTAINER} \\\n    --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n    --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n    --worker-vm-image ${NKP_IMAGE} \\\n    --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n    --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n    --control-plane-disk-size 150 --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} --control-plane-vcpus ${CONTROL_PLANE_VCPUS} --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n    --worker-disk-size 150 --worker-memory ${WORKER_MEMORY_GIB} --worker-vcpus ${WORKER_VCPUS} --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n    --csi-file-system ${CSI_FILESYSTEM} \\\n    --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n    --registry-mirror-url \"https://registry-1.docker.io\" \\\n    --registry-mirror-username ${DOCKER_USERNAME} \\\n    --registry-mirror-password ${DOCKER_PASSWORD} \\\n    --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --insecure \\\n    --self-managed\n</code></pre> <pre><code>&gt; \u2713 Creating a bootstrap cluster \n\u2713 Upgrading CAPI components \n\u2713 Waiting for CAPI components to be upgraded \n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Creating ClusterClass resources\n&gt; Generating cluster resources\ncluster.cluster.x-k8s.io/nkpdev created\nsecret/nkpdev-pc-credentials created\nsecret/nkpdev-pc-credentials-for-csi created\nsecret/nkpdev-image-registry-credentials created\n\u2713 Waiting for cluster infrastructure to be ready \n\u2713 Waiting for cluster control-planes to be ready \n\u2713 Waiting for machines to be ready\n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Moving cluster resources \n\n&gt; You can now view resources in the moved cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdev.conf\" get nodes\n\n&gt; \u2713 Deleting bootstrap cluster \n\nCluster default/nkpdev kubeconfig was written to to the filesystem.\nYou can now view resources in the new cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdev.conf\" get nodes\n\n&gt; Starting kommander installation\n\u2713 Deploying Flux \n\u2713 Deploying Ingress certificate \n\u2713 Creating kommander-overrides ConfigMap\n\u2713 Deploying Git Operator \n\u2713 Creating GitClaim for management GitRepository \n\u2713 Creating GitClaimUser for accessing management GitRepository \n\u2713 Creating HTTP Proxy configuration\n\u2713 Deploying Flux configuration\n\u2713 Deploying Kommander Operator \n\u2713 Creating KommanderCore resource \n\u2713 Cleaning up kommander bootstrap resources\n\u2713 Deploying Substitution variables\n\u2713 Deploying Flux configuration \n\u2713 Deploying Gatekeeper \n\u2713 Deploying Kommander AppManagement \n\u2713 Creating Core AppDeployments \n\u2713 4 out of 12 core applications have been installed (waiting for dex, dex-k8s-authenticator and 6 more) \n\u2713 5 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 5 more) \n\u2713 7 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 3 more) \n\u2713 8 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 2 more) \n\u2713 9 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 1 more) \n\u2713 10 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, traefik-forward-auth-mgmt) \n\u2713 11 out of 12 core applications have been installed (waiting for traefik-forward-auth-mgmt) \n\u2713 Creating cluster-admin credentials\n\n&gt; Cluster was created successfully! Get the dashboard details with:\n&gt; nkp get dashboard --kubeconfig=\"$HOME/nkp/nkpmanage.conf\"\n</code></pre> <p>Self-Manged Cluster??</p> <p>The <code>--self-managed</code> argument of the <code>nkp create cluster nutanix</code> command will deploy bootstrap, and Kommander management automatically. </p> <p>The appendix section has information on how to deploy a cluster without using the <code>--self-managed</code> option. </p> <p>Usually preferred by customer DevOps teams to have more control over the deployment process. This way the customer can do the following:</p> <ul> <li>Deploy bootstrap (<code>Kind</code>) cluster</li> <li>Deploy NKP Management cluster</li> <li>Choose to migrate the CAPI components over to NKP Management cluster</li> <li>Choose to deploy workload clusters from NKP Kommander GUI or</li> <li>Choose to deploy workload clusters using scripts to automate the process</li> </ul> </li> <li> <p>Observe the events in the shell and in Prism Central events</p> </li> <li> <p>Export kubeconfig file for mangement cluster. This file would be present in the same directory where the <code>nkp create cluster nutanix -c nkpmanage ...</code> command was run</p> <pre><code>export KUBECONFIG=nkpmanage.conf    \n</code></pre> </li> <li> <p>Check connectivity to the NKP Managment cluster</p> CommandCommand output <pre><code>nkp get nodes\n</code></pre> <pre><code>NAME                               STATUS   ROLES           AGE   VERSION\nnkpmanage-md-0-4w2xc-cm5xm-dwhdh   Ready    &lt;none&gt;          40m   v1.32.3\nnkpmanage-md-0-4w2xc-cm5xm-qbsv7   Ready    &lt;none&gt;          40m   v1.32.3\nnkpmanage-rhl4q-2pfpd              Ready    control-plane   41m   v1.32.3\n</code></pre> </li> <li> <p>Get management cluster's dashboard credentials to login to the NKP UI</p> CommandCommand output <pre><code>nkp get dashboard\n</code></pre> <pre><code>nkp get dashboard\n\nUsername: recursing_xxxxxxxxx\nPassword: YHbPsslIDB7p7rqwnfxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nURL: https://10.x.x.215/dkp/kommander/dashboard\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_hard_way/#license-management-cluster","title":"License Management Cluster","text":"<p>It is necessary to install license to the Management Cluster <code>nkpmanage</code> to be able to deploy workload clusters. Especially if the OS of the workload clusters' nodes is going to be <code>Ubuntu</code></p> <p>Follow the steps in this document to create and apply licenses on the management cluster.</p> <p>Note</p> <p>This Pro/Ultimate licensing requirement to deploy workload clusters with Ubuntu OS may change in the future. We will be sure to update here.</p>"},{"location":"appendix/infra_nkp_hard_way/#create-nkp-workload-cluster","title":"Create NKP Workload Cluster","text":"<ol> <li>In <code>VSCode</code>, create a new file <code>.env_workload</code></li> <li> <p>Open <code>.env_workload</code> file in VSC and add (append) the following environment variables and save it</p> Template .envSample .env <pre><code>export NKP_WORKLOAD_CLUSTER_NAME=_workload_nkp_clustername\nexport CONTROL_PLANE_REPLICAS=_no_of_control_plane_replicas\nexport CONTROL_PLANE_VCPUS=_no_of_control_plane_vcpus\nexport CONTROL_PLANE_CORES_PER_VCPU=_no_of_control_plane_cores_per_vcpu\nexport CONTROL_PLANE_MEMORY_GIB=_no_of_control_plane_memory_gib\nexport WORKER_REPLICAS=_no_of_worker_replicas\nexport WORKER_VCPUS=_no_of_worker_vcpus\nexport WORKER_CORES_PER_VCPU=_no_of_worker_cores_per_vcpu\nexport WORKER_MEMORY_GIB=_no_of_worker_memory_gib\nexport CSI_FILESYSTEM=_preferred_filesystem_ext4/xfs\nexport CSI_HYPERVISOR_ATTACHED=_true/false\nexport NUTANIX_PROJECT_NAME=_your_pc_project_name\nexport CONTROLPLANE_VIP=_your_nkp_cluster_controlplane_ip\nexport LB_IP_RANGE=_your_range_of_two_ips\n</code></pre> <pre><code>export NKP_WORKLOAD_CLUSTER_NAME=nkpdev\nexport CONTROL_PLANE_REPLICAS=3\nexport CONTROL_PLANE_VCPUS=4\nexport CONTROL_PLANE_CORES_PER_VCPU=1\nexport CONTROL_PLANE_MEMORY_GIB=16\nexport WORKER_REPLICAS=4\nexport WORKER_VCPUS=8 \nexport WORKER_CORES_PER_VCPU=1\nexport WORKER_MEMORY_GIB=32\nexport CSI_FILESYSTEM=ext4\nexport CSI_HYPERVISOR_ATTACHED=true\nexport NUTANIX_PROJECT_NAME=dev-lab\nexport CONTROLPLANE_VIP=10.x.x.214\nexport LB_IP_RANGE=10.x.x.215-10.x.x.216\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source .env_workload\n</code></pre> <p>Warning</p> <p>All the previous environment variable values will be replaced by this action. </p> <p>Since we do not need the management clusters' configuration values, this is acceptable. However, do consider how to manage environment variables in a production environment.</p> </li> <li> <p>In VSC, open Terminal, enter the following command to create the workload cluster</p> Optional - Check your command for correct argument values <p>Run the following command to verify your <code>nkp</code> command and associated environment variables and values.</p> <pre><code>echo \"nkp create cluster nutanix -c ${NKP_WORKLOAD_CLUSTER_NAME} \\\n        --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n        --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n        --control-plane-vm-image ${NKP_IMAGE} \\\n        --csi-storage-container ${STORAGE_CONTAINER} \\\n        --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n        --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n        --worker-vm-image ${NKP_IMAGE} \\\n        --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n        --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n        --control-plane-disk-size 150 --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} --control-plane-vcpus ${CONTROL_PLANE_VCPUS} --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n        --worker-disk-size 150 --worker-memory ${WORKER_MEMORY_GIB} --worker-vcpus ${WORKER_VCPUS} --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n        --csi-file-system ${CSI_FILESYSTEM} \\\n        --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n        --registry-mirror-url \"https://registry-1.docker.io\" \\\n        --registry-mirror-username ${DOCKER_USERNAME} \\\n        --registry-mirror-password ${DOCKER_PASSWORD} \\\n        --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --insecure\"\n</code></pre> <p>If the values are incorrect, source the <code>.env</code> file again by running the following command</p> <pre><code>source .env\n</code></pre> <p>Then rerun the <code>echo nkp</code> command to verify the values again before running the <code>nkp create cluster nutanix</code> command.</p> CommandCommand output <pre><code>nkp create cluster nutanix -c ${NKP_WORKLOAD_CLUSTER_NAME} \\\n    --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n    --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n    --control-plane-vm-image ${NKP_IMAGE} \\\n    --csi-storage-container ${STORAGE_CONTAINER} \\\n    --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n    --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n    --worker-vm-image ${NKP_IMAGE} \\\n    --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n    --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n    --control-plane-disk-size 150 --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} --control-plane-vcpus ${CONTROL_PLANE_VCPUS} --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n    --worker-disk-size 150 --worker-memory ${WORKER_MEMORY_GIB} --worker-vcpus ${WORKER_VCPUS} --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n    --csi-file-system ${CSI_FILESYSTEM} \\\n    --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n    --registry-mirror-url \"https://registry-1.docker.io\" \\\n    --registry-mirror-username ${DOCKER_USERNAME} \\\n    --registry-mirror-password ${DOCKER_PASSWORD} \\\n    --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --insecure\n</code></pre> <pre><code>\u2713 Upgrading CAPI components \n\u2713 Waiting for CAPI components to be upgraded \n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Creating ClusterClass resources\n&gt; Generating cluster resources\ncluster.cluster.x-k8s.io/nkpdev created\nsecret/nkpdev-pc-credentials created\nsecret/nkpdev-pc-credentials-for-csi created\nsecret/nkpdev-image-registry-credentials created\n\u2713 Waiting for cluster infrastructure to be ready \n\u2713 Waiting for cluster control-planes to be ready \n\u2713 Waiting for machines to be ready\n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Moving cluster resources \n\n&gt; You can now view resources in the moved cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdev.conf\" get nodes\n\n&gt; \u2713 Deleting bootstrap cluster \n\nCluster default/nkpdev kubeconfig was written to to the filesystem.\nYou can now view resources in the new cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdev.conf\" get nodes\n\n&gt; Cluster was created successfully! Get the dashboard details with:\n&gt; nkp get dashboard --kubeconfig=\"$HOME/nkp/nkpdev.conf\"\n</code></pre> Self-Manged Cluster?? <p>The <code>--self-managed</code> argument of the <code>nkp create cluster nutanix</code> command will deploy bootstrap, and Kommander management automatically. </p> <p>However, we are specifically not using it here in this lab, to run through Kommander installation later. </p> </li> <li> <p>Observe the events in the shell and in Prism Central events</p> </li> <li> <p>Store kubeconfig files for the workload cluster</p> <pre><code>nkp get kubeconfig -c ${NKP_WORKLOAD_CLUSTER_NAME} &gt; ${NKP_WORKLOAD_CLUSTER_NAME}.conf\nexport KUBECONFIG=${PWD}/${NKP_WORKLOAD_CLUSTER_NAME}.conf\n</code></pre> </li> <li> <p>Combine the management and workload clusters <code>KUBECONFIG</code> file so that we can use it with <code>kubectx</code> command to change context between clusters</p> <pre><code>export KUBECONFIG=${NKP_MGT_CLUSTER_NAME}.conf:${NKP_WORKLOAD_CLUSTER_NAME}.conf\nkubectl config view --flatten &gt; all-in-one-kubeconfig.yaml\nexport KUBECONFIG=all-in-one-kubeconfig.yaml\n</code></pre> </li> <li> <p>Run the following command to check K8S status of the <code>nkpdev</code> cluster</p> CommandCommand output <pre><code>kubectx ${NKP_WORKLOAD_CLUSTER_NAME}-admin@${NKP_WORKLOAD_CLUSTER_NAME} \nkubectl get nodes\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                  STATUS   ROLES           AGE     VERSION\nnkpdev-md-0-x948v-hvxtj-9r698           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-fb75c           Ready    &lt;none&gt;          4h50m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-mdckn           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-shxc8           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-r4fwl-8q4ch                      Ready    control-plane   4h50m   v1.29.6\nnkpdev-r4fwl-jf2s8                      Ready    control-plane   4h51m   v1.29.6\nnkpdev-r4fwl-q888c                      Ready    control-plane   4h49m   v1.29.6\n</code></pre> </li> <li> <p>Get dashboard URL and login credentials for the workload cluster</p> CommandCommand output <pre><code>nkp get dashboard\n</code></pre> <pre><code>nkp get dashboard\n\nUsername: recursing_xxxxxxxxx\nPassword: YHbPsslIDB7p7rqwnfxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nURL: https://10.x.x.215/dkp/kommander/dashboard\n</code></pre> </li> </ol>"},{"location":"appendix/infra_nkp_hard_way/#create-nkp-gpu-workload-pool","title":"Create NKP GPU Workload Pool","text":"<p>In this section we will create a nodepool to host the AI apps with a GPU.</p> <ol> <li> <p>Change KUBECONFIG context to use the workload <code>nkpmanage</code> cluster if not already there.</p> <pre><code>kubectx ${NKP_MGT_CLUSTER_NAME}-admin@${NKP_MGT_CLUSTER_NAME}\n</code></pre> </li> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export GPU_NAME=_name_of_gpu_device_\nexport GPU_REPLICA_COUNT=_no_of_gpu_worker_nodes\nexport GPU_POOL=_name_of_gpu_pool\n</code></pre> <pre><code>export GPU_NAME=\"Lovelace 40S\"\nexport GPU_REPLICA_COUNT=1\nexport GPU_POOL=gpu-nodepool\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source .env\n</code></pre> </li> <li> <p>Run the following command to create a GPU nodepool manifest</p> <pre><code>nkp create nodepool nutanix \\\n    --cluster-name ${NKP_WORKLOAD_CLUSTER_NAME} \\\n    --prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --subnets ${NUTANIX_SUBNET_NAME} \\\n    --vm-image ${NKP_IMAGE} \\\n    --disk-size 200 \\\n    --memory 40 \\\n    --vcpus 16 \\\n    --replicas ${GPU_REPLICA_COUNT} \\\n    --wait \\\n    ${GPU_POOL} --dry-run -o yaml &gt; gpu-nodepool.yaml\n</code></pre> <p>Note</p> <p>Right now there is no switch for GPU in <code>nkp</code> command. We need to do dry-run the output into a file and then add the necessary GPU specifications</p> </li> <li> <p>Add the necessary gpu section to our new <code>gpu-nodepool.yaml</code> using <code>yq</code> command</p> <pre><code>yq e '(.spec.topology.workers.machineDeployments[] | select(.name == \"gpu-nodepool\").variables.overrides[] | select(.name == \"workerConfig\").value.nutanix.machineDetails) += {\"gpus\": [{\"type\": \"name\", \"name\": strenv(GPU_NAME)}]}' -i gpu-nodepool.yaml\n</code></pre> Successful addtion of GPU specs? <p>You would be able to see the added gpu section at the end of the <code>gpu-nodepool.yaml</code> file</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\n\n&lt;snip&gt;\n\n  name: gpu-nodepool\n  variables:\n    overrides:\n      - name: workerConfig\n        value:\n          nutanix:\n            machineDetails:\n              bootType: legacy\n              cluster:\n                name: romanticism\n                type: name\n              image:\n                name: nkp-ubuntu-22.04-1.29.6-20240718055804\n                type: name\n              memorySize: 64Gi\n              subnets:\n                - name: User1\n                  type: name\n              systemDiskSize: 200Gi\n              vcpuSockets: 2\n              vcpusPerSocket: 8\n              gpus:\n                - type: name\n                  name: Lovelace 40S\n</code></pre> </li> <li> <p>Apply the <code>gpu-nodepool.yaml</code> file to the workload cluster </p> <pre><code>kubectl apply -f gpu-nodepool.yaml\n</code></pre> </li> <li> <p>Monitor the progress of the command and check Prism Central events for creation of the GPU worker node</p> <p>Change to workload <code>nkpdev</code> cluster context</p> <pre><code>kubectx ${NKP_WORKLOAD_CLUSTER_NAME}-admin@${NKP_WORKLOAD_CLUSTER_NAME}\n</code></pre> </li> <li> <p>Check nodes status in workload <code>nkpdev</code> cluster and note the gpu worker node</p> CommandCommand output <pre><code>kubectl get nodes -w\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                   STATUS   ROLES           AGE     VERSION\nnkpdev-gpu-nodepool-7g4jt-2p7l7-49wvd   Ready    &lt;none&gt;          5m57s   v1.29.6\nnkpdev-md-0-q679c-khl2n-9k7jk           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-md-0-q679c-khl2n-9nk6h           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-md-0-q679c-khl2n-nf9p6           Ready    &lt;none&gt;          73m     v1.29.6\nnkpdev-md-0-q679c-khl2n-qgxp9           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-ncnww-2dg7h                      Ready    control-plane   73m     v1.29.6\nnkpdev-ncnww-bbm4s                      Ready    control-plane   72m     v1.29.6\nnkpdev-ncnww-hldm9                      Ready    control-plane   75m     v1.29.6\n</code></pre> </li> </ol> <p>Now we are ready to deploy our AI workloads.</p>"},{"location":"appendix/infra_nkp_hard_way/#optional-cleanup","title":"Optional - Cleanup","text":"<p>Optionally, cleanup the workloads on nkp cluster by deleting it after deploying and testing your AI/ML application. </p> <ol> <li> <p>Change cluster context to use the workload <code>bootstrap</code> cluster</p> <pre><code>kubectx ${NKP_MGT_CLUSTER_NAME}-admin@${NKP_MGT_CLUSTER_NAME}\n</code></pre> </li> <li> <p>Delete the workload cluster</p> CommandCommand output <pre><code>nkp delete cluster -c ${NKP_WORKLOAD_CLUSTER_NAME}\n</code></pre> <pre><code>nkp delete cluster -c nkpdev\n\n\u2713 Upgrading CAPI components \n\u2713 Waiting for CAPI components to be upgraded \n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Creating ClusterClass resources\n\u2713 Moving cluster resources \n\u2713 Waiting for cluster infrastructure to be ready \n\u2713 Waiting for cluster control-planes to be ready\n\u2713 Waiting for machines to be ready\n\u2713 Deleting cluster resources\n\u2713 Waiting for cluster to be fully deleted \nDeleted default/nkpdev cluster\n</code></pre> </li> </ol> <p>Info</p> <p>If the workload cluster was created as self-managed, then the following command will delete the cluster by creating a small bootstrap cluster. This bootstrap cluster will also be deleted automatically after the workload cluster is deleted.</p>"},{"location":"appendix/nkp_cert_ds/","title":"Deploying Private CA Certificate to NKP Cluster","text":"<p>In this section we will deploy a private CA certificate to the NKP air-gapped cluster nodes if not already added at the time of deployment.</p> <p>Best Practice for Self-signed Certificates</p> <p>The best practice is to deploy the NKP air-gapped cluster with a Self-signed Certificates (private) CA certificate at Day 0 using the <code>nkp create cluster nutanix --additional-trust-bundle</code> among other options.</p> <p>For Day 1 and 2 operations, the private CA certificate will need to be added to all the NKP air-gapped cluster nodes.</p> <p>Follow the steps here to add the Harbor container registry's CA certificate <code>ca.crt</code> that you created in this section to the nodes.</p> <p>Self-signed Certificates</p> <p>Make sure to install self-signed certificates only if you are using a test, lab or development environment.</p> <p>For production environments, use a trusted public CA certificate.</p> <p>The recommendation from Nutanix is to use a trusted public CA certificate.</p> <ol> <li> <p>Login to the Jumphost VM using VSCode</p> </li> <li> <p>In VSCode explorer pane, change to <code>$HOME/harbor</code> directory and create the secret manifest file by clicking on  with the following name:</p> <pre><code>ca-crt-secret.yaml\n</code></pre> <p>with the following content:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: trusted-ca\n  namespace: kube-system\ndata:\n  ca.crt: |\n    -----BEGIN CERTIFICATE-----\n\n    PASTE_YOUR_CA_CERTIFICATE_HERE\n\n    -----END CERTIFICATE-----    \n</code></pre> </li> <li> <p>Create a configmap manifest file by clicking on  and create a new file with the following name: </p> <pre><code>ca-crt-cm.yaml\n</code></pre> <p>with the following content:</p> <pre><code>kind: ConfigMap\nmetadata:\n  name: registry-ca-setup-script\n  namespace: kube-system\ndata:\n  setup.sh: |\n    mkdir /etc/certs\n    mkdir -p /etc/containerd/certs.d/${REGISTRY_HOST}\n    echo \"$TRUSTED_CERT\" &gt; /etc/certs/${REGISTRY_HOST}\n    cat &lt;&lt;EOF &gt;  /etc/containerd/certs.d/${REGISTRY_HOST}/hosts.toml\n    [host.\"https://${REGISTRY_HOST}/v2\"]\n    capabilities = [\"pull\", \"resolve\"]\n    ca = \"/etc/certs/${REGISTRY_HOST}\"\n    override_path = true\n</code></pre> </li> <li> <p>Create a DaemonSet manifest file by clicking on  and create a new file with the following name:</p> <pre><code>ca-crt-ds.yaml\n</code></pre> <p>with the following content:</p> <p>Change the registry host name</p> <p>Change the highlighted registry host name to the one you are using.</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  namespace: kube-system\n  name: registry-ca-setup\n  labels:\n    k8s-app: registry-ca-setup\nspec:\n  selector:\n    matchLabels:\n      k8s-app: registry-ca-setup\n  template:\n    metadata:\n      labels:\n        k8s-app: registry-ca-setup\n    spec:\n      hostPID: true\n      hostNetwork: true\n      initContainers:\n      - name: init-node\n        command: [\"nsenter\"]\n        args: [\"--mount=/proc/1/ns/mnt\", \"--\", \"sh\", \"-c\", \"$(SETUP_SCRIPT)\"]\n        image: debian\n        env:\n        - name: TRUSTED_CERT\n          valueFrom:\n            configMapKeyRef:\n              name: trusted-ca\n              key: ca.crt\n        - name: REGISTRY_HOST\n          value:  10.x.x.111\n        - name: SETUP_SCRIPT\n          valueFrom:\n            configMapKeyRef:\n              name: registry-ca-setup-script\n              key: setup.sh\n        securityContext:\n          privileged: true\n      containers:\n      - name: wait\n        image: k8s.gcr.io/pause:3.1\n</code></pre> </li> <li> <p>Open VSCode Terminal</p> </li> <li> <p>Login to the devbox shell</p> <pre><code>cd $HOME/sol-cnai-infra/\ndevbox shell\ncd $HOME/harbor\n</code></pre> </li> <li> <p>Apply the manifests created in the previous steps</p> <pre><code>kubectl apply -f ca-crt-secret.yaml\nkubectl apply -f ca-crt-cm.yaml\nkubectl apply -f ca-crt-ds.yaml\n</code></pre> </li> <li> <p>Verify that the manifests are applied by checking the status of the daemonset and pods</p> CommandCommand output <pre><code>kubectl get ds -n kube-system --selector='k8s-app=registry-ca-setup' -owide\nkubectl get po -n kube-system --selector='k8s-app=registry-ca-setup' -owide\n</code></pre> <pre><code>$ k get ds -n kube-system --selector='k8s-app=registry-ca-setup' -owide\n\nNAME                DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS   IMAGES                 SELECTOR\nregistry-ca-setup   5         5         5       5            5           &lt;none&gt;          25h   wait         k8s.gcr.io/pause:3.1   k8s-app=registry-ca-setup\n</code></pre> <pre><code>$ k get po -n kube-system --selector='k8s-app=registry-ca-setup' -owide\n\nNAME                      READY   STATUS    RESTARTS   AGE   IP             NODE                                    \nregistry-ca-setup-65sgk   1/1     Running   0          23h   10.122.7.224   nkpdev-gpu-nodepool-b57nm-2c8vh-njlnf\nregistry-ca-setup-98mfn   1/1     Running   0          23h   10.122.7.129   nkpdev-md-0-b4z9f-mkrt6-sd7vb\nregistry-ca-setup-gnpkn   1/1     Running   0          23h   10.122.7.112   nkpdev-md-0-b4z9f-mkrt6-9n89d  \nregistry-ca-setup-hdzpl   1/1     Running   0          23h   10.122.7.117   nkpdev-md-0-b4z9f-mkrt6-zz2mk \nregistry-ca-setup-vvhl5   1/1     Running   0          23h   10.122.7.128   nkpdev-md-0-b4z9f-mkrt6-hdrtq\n</code></pre> <p>Note</p> <p>The daemonset's pods will run only on the worker nodes and GPU nodes in the cluster.</p> </li> <li> <p>Now that the manifests are applied, the CA certificate will be added to the trusted CA store on the nodes.</p> </li> </ol>"},{"location":"appendix/opentofu/","title":"Appendix","text":""},{"location":"appendix/opentofu/#preparing-opentofu","title":"Preparing OpenTofu","text":""},{"location":"appendix/opentofu/#install-opentofu-on-linux","title":"Install OpenTofu on Linux","text":"<p>On your Linux workstation run the following scripts to install OpenTofu. See [here]for latest instructions and other platform information. </p> <p>Download the installer script:<pre><code>curl --proto '=https' --tlsv1.2 -fsSL https://get.opentofu.org/install-opentofu.sh -o install-opentofu.sh\n</code></pre> Give it execution permissions:<pre><code>chmod +x install-opentofu.sh\n</code></pre> Run the installer:<pre><code>./install-opentofu.sh --install-method rpm\n</code></pre></p>"},{"location":"appendix/opentofu/#install-opentofu-on-windows","title":"Install OpenTofu on Windows","text":"<p>On your Windows workstation run the following scripts to install OpenTofu.</p> <p>Download the installer script:<pre><code>Invoke-WebRequest -outfile \"install-opentofu.ps1\" -uri \"https://get.opentofu.org/install-opentofu.ps1\"\n</code></pre> Run the installer<pre><code>&amp; .\\install-opentofu.ps1 -installMethod standalone\n</code></pre> Remove the installer<pre><code>Remove-Item install-opentofu.ps1\n</code></pre></p>"},{"location":"conceptual/conceptual/","title":"Conceptual Design","text":"<p>In this section we will discuss conceptual designs of Nutanix infrastructure along with Nutanix Kubernetes Engine (NKE) at a very high level.</p> <p>Note</p> <p>All information for this page comes from Nutanix Validated Designs (NVD) from Nutanix Portal for GPT-in-a-Box and is publicly available. </p>"},{"location":"conceptual/conceptual/#core-infrastructure-design","title":"Core Infrastructure Design","text":"<p>All Nutanix infrastructure designs are considered non-functional components and designed with the following in mind:</p> <ul> <li>Availability (along with the uptime requirement in terms of 9s (Eg. 99.99)</li> <li>Resiliency</li> <li>Scalability</li> <li>Observability and</li> <li>Performance</li> </ul> <p>The Nutanix cluster used in this website to test AI applications is a 4-node Nutanix cluster with GPU. This has provisioning for all non-functional components except regional availability, as it is just a testing environment. It is recommended to have zonal and/or regional availability for production applications.</p> <p>Compute, Storage and Networking are redundant in a 4-node Nutanix cluster used. The NVD referenced in this page also provides details of multi-zone and multi-regional Nutanix clusters. </p> <p></p> <p>Refer to the GPT-in-a-Box  for details about multi-zone/region NVD.</p>"},{"location":"conceptual/conceptual/#core-nke-design","title":"Core NKE Design","text":"<p>The conceptual design outlines a robust, highly available Kubernetes environment on the Nutanix platform, using Nutanix Kubernetes Engine (NKE) features and integrating with essential Kubernetes tools and practices for efficient and secure operations.</p> <p>NKE designs are considered non-functional components and designed with the following in mind:</p> <ul> <li>Availability (along with the uptime requirement in terms of 9s (Eg. 99.99)</li> <li>Resiliency</li> <li>Scalability</li> <li>Observability and</li> <li>Performance</li> </ul> <p>We will use two NKE clusters to eventually deploy AI apps.</p> <p></p> <p>All NKE cluster will use the following components:</p> <ul> <li>Kube-VIP: Manages network connectivity and load balancing.</li> <li>Ingress Controllers: Handle external access to services using HTTP and HTTPS routes.</li> <li>Cert-Manager: Automates TLS certificate management and issuance, enhancing secure communications.</li> </ul>"},{"location":"conceptual/conceptual/#management-kubernetes-cluster","title":"Management Kubernetes Cluster","text":"<p>The Management Kubernetes Clusters is deployed with multiple master and worker nodes distributed across multiple physical nodes.</p> <p>The following management applications are used in the Management kubernetes clusters to automate the lifecycle management of all components.</p> <ul> <li>Flux CD: Manages applications on Kubernetes prod, test, and dev clusters with automated deployment from Git repositories</li> <li>OpenTelemetry: Manages Metrics, Logs, and Traces that are collected across all kubernetes clusters </li> <li>Uptrace: Provides a user-friendly interface for visualizing and querying collected data for monitoring and debugging</li> <li>Kafka: Provides messaging infrastructure to send and receive data from the Retrieval-Augmented Generation (RAG) workload and vector database store</li> </ul>"},{"location":"conceptual/conceptual/#devtestprod-kubernetes-cluster","title":"Dev/Test/Prod Kubernetes Cluster","text":"<p>We will only use Dev cluster in this design. However, the principles apply to Prod and Test clusters as well.</p> <p>These clusters will have access to GPU card using passthrough method. For example: Dev cluster will have only have access to a single GPU while PROD clusters may have access to many. Refer to GPT-in-a-Box NVD for design details based on the resource requirement for the AI application. </p> <p>The workload clusters will host the following:</p> <ul> <li>Inference Endpoint: This is the managed service running on NKE cluster that provides the interface between the AI application and users</li> <li>RAG Workload: All private user/company data additional to basic LLM knowledge is stored in this vector database store</li> </ul>"},{"location":"iep/","title":"Getting Started","text":"<p>In this part of the lab we will deploy LLM on GPU nodes.</p> <p>We will also deploy a Kubernetes cluster so far as per the NVD design requirements.</p> <p>Dev NKP cluster: to host the dev LLM and ChatBot application - this will use GPU passed through to the kubernetes worker node.</p> <p>Deploy the kubernetes cluster with the following components:</p> <ul> <li>3 x Control plane nodes</li> <li>4 x Worker nodes </li> <li>1 x GPU node (with a minimum of 40GB of RAM and 16 vCPUs based on <code>llama3-8B</code> LLM model)</li> </ul> <p>We will deploy the GPT-in-a-Box v2 NVD Reference App - backed by <code>llama3-8B</code> model.</p> <p>The following is the flow of the NAI lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKP {\n        [*] --&gt; CreateNkpMachineImage\n        CreateNkpMachineImage --&gt; CreateNkpSelfManagedCluster\n        CreateNkpSelfManagedCluster --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n\n    state NAIPreRequisites {\n        [*] --&gt; ReserveIPs\n        ReserveIPs --&gt; CreateFilesShare\n        CreateFilesShare --&gt; [*]\n    }\n\n    state DeployNAI {\n        [*] --&gt; BootStrapDevCluster\n        BootStrapDevCluster --&gt; MonitorResourcesDeployment\n        MonitorResourcesDeployment --&gt; [*]\n    }\n\n    state TestLLMApp {\n        [*] --&gt; TestQueryLLM\n        TestQueryLLM --&gt; TestChatApp\n        TestChatApp --&gt; [*]\n    }\n\n    [*] --&gt; DeployNKP\n    DeployNKP --&gt; NAIPreRequisites\n    NAIPreRequisites --&gt; DeployNAI\n    DeployNAI --&gt; TestLLMApp\n    TestLLMApp --&gt; [*]</code></pre>"},{"location":"iep/iep_deploy/","title":"Deploying Nutanix Enterprise AI (NAI) NVD Reference Application","text":"<p>Version 2.3.0</p> <p>This version of the NAI deployment is based on the Nutanix Enterprise AI (NAI) <code>v2.3.0</code> release.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNAI {\n        [*] --&gt; DeployNAIAdmin\n        DeployNAIAdmin --&gt;  InstallSSLCert\n        InstallSSLCert --&gt; DownloadModel\n        DownloadModel --&gt; CreateNAI\n        CreateNAI --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI \n    DeployNAI --&gt; TestNAI : next section\n    TestNAI --&gt; [*]</code></pre>"},{"location":"iep/iep_deploy/#prepare-for-nai-deployment","title":"Prepare for NAI Deployment","text":"<p>Changes in NAI <code>v2.4.0</code></p> <ul> <li>Istio Ingress gateway is replaced with Envoy Gateway</li> <li>Knative is removed from NAI </li> <li>Kserve has been upgraded to 0.15.0</li> </ul>"},{"location":"iep/iep_deploy/#enable-nkp-applications-through-nkp-gui","title":"Enable NKP Applications through NKP GUI","text":"<p>Enable these NKP Operators from NKP GUI.</p> <p>Note</p> <p>In this lab, we will be using the Management Cluster Workspace to deploy our Nutanix Enterprise AI (NAI)</p> <p>However, in a customer environment, it is recommended to use a separate workload NKP cluster.</p> <ol> <li>In the NKP GUI, Go to Clusters</li> <li>Click on Management Cluster Workspace</li> <li>Go to Applications </li> <li> <p>Search and enable the following applications: follow this order to install dependencies for NAI application</p> <ul> <li>Kube-prometheus-stack: version <code>70.4.2</code> or later (pre-installed on NKP cluster)</li> </ul> </li> </ol>"},{"location":"iep/iep_deploy/#enable-pre-requisite-applications","title":"Enable Pre-requisite Applications","text":"<p>We will enable the following pre-requisite applications through command line:</p> <ul> <li>Envoy Gateway <code>v1.3.2</code></li> <li>Kserve: <code>v0.15.0</code> in raw deployment mode</li> </ul> <p>Note</p> <p>The following application are pre-installed on NKP cluster with Pro license</p> <ul> <li>Cert Manager</li> </ul> <p>Check if Cert Manager is installed (pre-installed on NKP cluster)</p> CommandOutput <pre><code>kubectl get deploy -n cert-manager\n</code></pre> <pre><code>$ kubectl get deploy -n cert-manager\n\nNAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ncert-manager              1/1     1            1           145m\ncert-manager-cainjector   1/1     1            1           145m\ncert-manager-webhook      1/1     1            1           145m\n</code></pre> <p>If not installed, use the following command to install it</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.4/cert-manager.yaml\n</code></pre> <ol> <li> <p>Open Terminal  in <code>VSCode</code></p> </li> <li> <p>Run the command to load the environment variables</p> <pre><code>source $HOME/.env\n</code></pre> </li> <li> <p>Install Envoy Gateway <code>v1.3.2</code></p> CommandOutput <pre><code>helm install eg oci://docker.io/envoyproxy/gateway-helm --version v1.3.2 -n envoy-gateway-system --create-namespace\n</code></pre> <pre><code>helm install eg oci://docker.io/envoyproxy/gateway-helm --version v1.3.2 -n envoy-gateway-system --create-namespace\nPulled: docker.io/envoyproxy/gateway-helm:v1.3.2\nDigest: sha256:0070bdddc186e6bd48007a84c6d264b796d14017436f38ccfe5ca621aefc1ca5\nNAME: eg\nLAST DEPLOYED: Mon Aug 25 04:31:06 2025\nNAMESPACE: envoy-gateway-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> </li> <li> <p>Check if Envoy Gateway resources are ready</p> CommandOutput <pre><code>kubectl wait --timeout=5m -n envoy-gateway-system deployment/envoy-gateway --for=condition=Available\n</code></pre> <pre><code>deployment.apps/envoy-gateway condition met\n</code></pre> </li> <li> <p>Open <code>$HOME/.env</code> file in <code>VSCode</code></p> </li> <li> <p>Add (append) the following line and save it</p> <pre><code>export KSERVE_VERSION=v0.15.0\n</code></pre> </li> <li> <p>Install <code>kserve</code> using the following commands</p> CommandOutput <p><pre><code>helm upgrade --install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd --version ${KSERVE_VERSION} -n kserve --create-namespace \n</code></pre> <pre><code>helm upgrade --install kserve oci://ghcr.io/kserve/charts/kserve --version ${KSERVE_VERSION} --namespace kserve --create-namespace \\\n--set kserve.controller.deploymentMode=RawDeployment \\\n--set kserve.controller.gateway.disableIngressCreation=true\n</code></pre></p> <p><pre><code>Pulled: ghcr.io/kserve/charts/kserve-crd:v0.15.0\nDigest: sha256:57ad1a5475fd625cb558214ba711752aa77b7d91686a391a5f5320cfa72f3fa8\nRelease \"kserve-crd\" has been upgraded. Happy Helming!\nNAME: kserve-crd\nLAST DEPLOYED: Mon May 19 06:11:30 2025\nNAMESPACE: kserve\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\n(devbox) \n</code></pre> <pre><code>Pulled: ghcr.io/kserve/charts/kserve:v0.15.0\nDigest: sha256:905abce80e975c53b40fba7a12b0b9a1e24bdf65cceebb88fba4ef62bba01406\nRelease \"kserve\" has been upgraded. Happy Helming!\nNAME: kserve\nLAST DEPLOYED: Mon May 19 05:48:45 2025\nNAMESPACE: kserve\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\n</code></pre></p> </li> <li> <p>Check if <code>kserve</code> pods are running</p> CommandOutput <pre><code>kubens kserve\nkubectl get pods    # (1)!\n</code></pre> <ol> <li>Make sure both the containers are running for <code>kserve-controller-manager</code> pod</li> </ol> <pre><code>NAME                                         READY   STATUS    RESTARTS   AGE\nkserve-controller-manager-58946fd54d-vsxvn   2/2     Running   0          18m\n</code></pre> </li> </ol> <p>Note</p> <p>It may take a few minutes for each application to be up and running. Monitor the deployment to make sure that these applications are running before moving on to the next section.</p>"},{"location":"iep/iep_deploy/#deploy-nai","title":"Deploy NAI","text":"<p>We will use the Docker login credentials we created in the previous section to download the NAI Docker images.</p> <p>Change the Docker login credentials</p> <p>The following Docker based environment variable values need to be changed from your own Docker environment variables to the credentials downloaded from Nutanix Portal.</p> <ul> <li><code>$DOCKER_USERNAME</code></li> <li><code>$DOCKER_PASSWORD</code></li> </ul> <ol> <li> <p>Open <code>$HOME/.env</code> file in <code>VSCode</code></p> </li> <li> <p>Add (append) the following environment variables and save it</p> Template .envSample .env <pre><code>export DOCKER_USERNAME=_GA_release_docker_username\nexport DOCKER_PASSWORD=_GA_release_docker_password\nexport NAI_CORE_VERSION=_GA_release_nai_core_version\nexport NAI_DEFAULT_RWO_STORAGECLASS=_RWO_storage_class_name\nexport NAI_API_RWX_STORAGECLASS=_RWX_storage_class_name\n</code></pre> <pre><code>export DOCKER_USERNAME=ntnxsvcgpt\nexport DOCKER_PASSWORD=dckr_pat_xxxxxxxxxxxxxxxxxxxxxxxx\nexport NAI_CORE_VERSION=2.4.0\nexport NAI_DEFAULT_RWO_STORAGECLASS=nutanix-volume\nexport NAI_API_RWX_STORAGECLASS=nai-nfs-storage\n</code></pre> </li> <li> <p>Source the environment variables (if not done so already)</p> <pre><code>source $HOME/.env\n</code></pre> </li> <li> <p>In <code>VSCode</code> Explorer pane, browse to <code>$HOME/nai</code> folder</p> </li> <li> <p>Click on New File  and create file with the following name:</p> <pre><code>nkp-values.yaml\n</code></pre> <p>with the following content:</p> <pre><code># nai-monitoring stack values for nai-monitoring stack deployment in NKE environment\nnaiMonitoring:\n\n  ## Component scraping node exporter\n  ##\n  nodeExporter:\n    serviceMonitor:\n      enabled: true\n      endpoint:\n        port: http-metrics\n        scheme: http\n        targetPort: 9100\n      namespaceSelector:\n        matchNames:\n        - kommander\n      serviceSelector:\n        matchLabels:\n          app.kubernetes.io/name: prometheus-node-exporter\n          app.kubernetes.io/component: metrics\n\n  ## Component scraping dcgm exporter\n  ##\n  dcgmExporter:\n    podLevelMetrics: true\n    serviceMonitor:\n      enabled: true\n      endpoint:\n        targetPort: 9400\n      namespaceSelector:\n        matchNames:\n        - kommander\n      serviceSelector:\n        matchLabels:\n          app: nvidia-dcgm-exporter\n</code></pre> How to get nkp-values.yaml file? <p>It is possible to get the values file using the following command</p> <pre><code>helm repo add ntnx-charts https://nutanix.github.io/helm-releases\nhelm repo update ntnx-charts\nhelm pull ntnx-charts/nai-core --version=nai-core-version --untar=true\n</code></pre> <p>All the files will be untar'ed to a folder nai-core in the present working directory</p> <p>Use the <code>nkp-values.yaml</code> file in the installation command</p> </li> <li> <p>In <code>VSCode</code>, Under <code>$HOME/nai</code> folder, click on New File  and create a file with the following name:</p> <pre><code>nai-deploy.sh\n</code></pre> <p>with the following content:</p> <pre><code>#!/usr/bin/env bash\n\nset -ex\nset -o pipefail\n\nhelm repo add ntnx-charts https://nutanix.github.io/helm-releases\nhelm repo update ntnx-charts\n\n#NAI-core\nhelm upgrade --install nai-core ntnx-charts/nai-core --version=$NAI_CORE_VERSION -n nai-system --create-namespace --wait \\\n--set imagePullSecret.credentials.username=$DOCKER_USERNAME \\\n--set imagePullSecret.credentials.password=$DOCKER_PASSWORD \\\n--insecure-skip-tls-verify \\\n--set naiApi.storageClassName=$NAI_API_RWX_STORAGECLASS \\\n--set defaultStorageClassName=$NAI_DEFAULT_RWO_STORAGECLASS \\\n-f nkp-values.yaml\n</code></pre> </li> <li> <p>Run the following command to deploy NAI</p> CommandCommand output <pre><code>$HOME/nai/nai-deploy.sh\n</code></pre> <pre><code>$HOME/nai/nai-deploy.sh \n\n+ set -o pipefail\n+ helm repo update ntnx-charts\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"ntnx-charts\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\nhelm upgrade --install nai-core ntnx-charts/nai-core --version=$NAI_CORE_VERSION -n nai-system --create-namespace --wait \\\n--set imagePullSecret.credentials.username=$DOCKER_USERNAME \\\n--set imagePullSecret.credentials.password=$DOCKER_PASSWORD \\\n--insecure-skip-tls-verify \\\n-f nkp-values.yaml\nRelease \"nai-core\" does not exist. Installing it now.\nNAME: nai-core\nLAST DEPLOYED: Mon Aug 25 04:59:28 2025\nNAMESPACE: nai-system\nSTATUS: deployed\nREVISION: 1\n</code></pre> </li> <li> <p>Verify that the NAI Core Pods are running and healthy</p> CommandCommand output <pre><code>kubens nai-system\nkubectl get po,deploy\n</code></pre> <pre><code>$ kubectl get po,deploy\nContext \"nkplb-admin@nkplb\" modified.\nActive namespace is \"nai-system\".\nNAME                                            READY   STATUS      RESTARTS   AGE\npod/nai-api-58cbd47f86-dqt5z                    1/1     Running     0          4m1s\npod/nai-api-db-migrate-q2urg-nb8zc              0/1     Completed   0          4m1s\npod/nai-db-0                                    1/1     Running     0          4m1s\npod/nai-iep-model-controller-64d88cd94f-q85hf   1/1     Running     0          4m1s\npod/nai-ui-dd8fb65c-zthbf                       1/1     Running     0          4m1s\npod/prometheus-nai-0                            2/2     Running     0          4m1s\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nai-api                    1/1     1            1           4m1s\ndeployment.apps/nai-iep-model-controller   1/1     1            1           4m1s\ndeployment.apps/nai-ui                     1/1     1            1           4m1s\n</code></pre> </li> </ol> Uninstall NAI <code>v2.3.0</code> Dependencies <p>If you are upgrading NAI from <code>v2.3.0</code> to <code>v2.4.0</code>, uninstall the following:</p> <p>If Helm was used:</p> <p>Uninstall Istio<pre><code>helm uninstall istio-ingressgateway -n istio-system --wait --ignore-not-found\nhelm uninstall istiod -n istio-system --wait --ignore-not-found\nhelm uninstall istio-base -n istio-system --wait --ignore-not-found\n</code></pre> Uninstall Knative<pre><code>kubectl delete --ignore-not-found=true KnativeServing knative-serving -n knative-serving\nhelm uninstall knative-operator -n knative-serving --wait --ignore-not-found\nkubectl wait --for=delete pod --all -n knative-serving --timeout=300s\n</code></pre></p> <p>If NKP Application were used for installation:</p> <p>Go to NKP Cluster Dashboard &gt; Application &gt; Search and Uninstall the following:</p> <ol> <li>Istio</li> <li>Knative </li> </ol>"},{"location":"iep/iep_deploy/#install-ssl-certificate-and-gateway-elements","title":"Install SSL Certificate and Gateway Elements","text":"<p>In this section we will install SSL Certificate to access the NAI UI. This is required as the endpoint will only work with a ssl endpoint with a valid certificate.</p> <p>NAI UI is accessible using the Ingress Gateway.</p> <p>The following steps show how cert-manager can be used to generate a self signed certificate using the default selfsigned-issuer present in the cluster. </p> <p>If you are using Public Certificate Authority (CA) for NAI SSL Certificate</p> <p>If an organization generates certificates using a different mechanism then obtain the certificate + key and create a kubernetes secret manually using the following command:</p> <pre><code>kubectl -n istio-system create secret tls nai-cert --cert=path/to/nai.crt --key=path/to/nai.key\n</code></pre> <p>Skip the steps in this section to create a self-signed certificate resource.</p> <ol> <li> <p>Get the NAI UI ingress gateway host using the following command:</p> <pre><code>NAI_UI_ENDPOINT=$(kubectl get svc -n envoy-gateway-system -l \"gateway.envoyproxy.io/owning-gateway-name=nai-ingress-gateway,gateway.envoyproxy.io/owning-gateway-namespace=nai-system\" -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}' | grep -v '^$' || kubectl get svc -n envoy-gateway-system -l \"gateway.envoyproxy.io/owning-gateway-name=nai-ingress-gateway,gateway.envoyproxy.io/owning-gateway-namespace=nai-system\" -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')\n</code></pre> </li> <li> <p>Get the value of <code>NAI_UI_ENDPOINT</code> environment variable</p> CommandCommand output <pre><code>echo $NAI_UI_ENDPOINT\n</code></pre> <pre><code>10.x.x.216\n</code></pre> </li> <li> <p>We will use the command output e.g: <code>10.x.x.216</code> as the IP address for NAI as reserved in this section</p> </li> <li> <p>Construct the FQDN of NAI UI using nip.io and we will use this FQDN as the certificate's Common Name (CN).</p> Template URLSample URL <pre><code>nai.${NAI_UI_ENDPOINT}.nip.io\n</code></pre> <pre><code>nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>Create the ingress resource certificate using the following command:</p> <pre><code>cat &lt;&lt; EOF | k apply -f -\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: nai-cert\n  namespace: nai-system\nspec:\n  issuerRef:\n    name: selfsigned-issuer\n    kind: ClusterIssuer\n  secretName: nai-cert\n  commonName: nai.${NAI_UI_ENDPOINT}.nip.io\n  dnsNames:\n  - nai.${NAI_UI_ENDPOINT}.nip.io\n  ipAddresses:\n  - ${NAI_UI_ENDPOINT}\nEOF\n</code></pre> </li> <li> <p>Patch the Envoy gateway with the <code>nai-cert</code> certificate details</p> <pre><code>kubectl patch gateway nai-ingress-gateway -n nai-system --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/listeners/1/tls/certificateRefs/0/name\", \"value\": \"nai-cert\"}]'\n</code></pre> </li> <li> <p>Create EnvoyProxy</p> <pre><code>k apply -f -&lt;&lt;EOF\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyProxy\nmetadata:\n  name: envoy-service-config\n  namespace: nai-system\nspec:\n  provider:\n    type: Kubernetes\n    kubernetes:\n      envoyService:\n        type: LoadBalancer\nEOF\n</code></pre> </li> <li> <p>Patch the <code>nai-ingress-gateway</code> resource with the new <code>EnvoyProxy</code> details</p> <pre><code>kubectl patch gateway nai-ingress-gateway -n nai-system --type=merge \\\n-p '{\n    \"spec\": {\n        \"infrastructure\": {\n            \"parametersRef\": {\n                \"group\": \"gateway.envoyproxy.io\",\n                \"kind\": \"EnvoyProxy\",\n                \"name\": \"envoy-service-config\"\n            }\n        }\n    }\n}'\n</code></pre> </li> </ol>"},{"location":"iep/iep_deploy/#accessing-the-ui","title":"Accessing the UI","text":"<ol> <li> <p>In a browser, open the following URL to connect to the NAI UI</p> <pre><code>https://nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>Change the password for the <code>admin</code> user</p> </li> <li> <p>Login using <code>admin</code> user and password.</p> <p></p> </li> </ol>"},{"location":"iep/iep_deploy/#download-model","title":"Download Model","text":"<p>We will download and user llama3 8B model which we sized for in the previous section.</p> <ol> <li>In the NAI GUI, go to Models</li> <li>Click on Import Model from Hugging Face</li> <li>Choose the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model</li> <li> <p>Input your Hugging Face token that was created in the previous section and click Import</p> </li> <li> <p>Provide the Model Instance Name as <code>Meta-Llama-3.1-8B-Instruct</code> and click Import</p> </li> <li> <p>Go to VSC Terminal to monitor the download</p> CommandCommand output <p>Get jobs in nai-admin namespace<pre><code>kubens nai-admin\n\nkubectl get jobs\n</code></pre> Validate creation of pods and PVC<pre><code>kubectl get po,pvc\n</code></pre> Verify download of model using pod logs<pre><code>kubectl logs -f _pod_associated_with_job\n</code></pre></p> <p>Get jobs in nai-admin namespace<pre><code>kubens nai-admin\n\n\u2714 Active namespace is \"nai-admin\"\n\nkubectl get jobs\n\nNAME                                       COMPLETIONS   DURATION   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-model-job   0/1           4m56s      4m56\n</code></pre> Validate creation of pods and PVC<pre><code>kubectl get po,pvc\n\nNAME                                             READY   STATUS    RESTARTS   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-model-job-9nmff   1/1     Running   0          4m49s\n\nNAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      VOLUMEATTRIBUTESCLASS   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-pvc-claim   Bound    pvc-a63d27a4-2541-4293-b680-514b8b890fe0   28Gi       RWX            nai-nfs-storage   &lt;unset&gt;                 2d\n</code></pre> Verify download of model using pod logs<pre><code>kubectl logs -f nai-c0d6ca61-1629-43d2-b57a-9f-model-job-9nmff \n\n/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.05 MB. The target location /data/model-files only has 0.00 MB free disk space.\nwarnings.warn(\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51.0k/51.0k [00:00&lt;00:00, 3.26MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.09M/9.09M [00:00&lt;00:00, 35.0MB/s]&lt;00:30, 150MB/s]\nmodel-00004-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.17G/1.17G [00:12&lt;00:00, 94.1MB/s]\nmodel-00001-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.98G/4.98G [04:23&lt;00:00, 18.9MB/s]\nmodel-00003-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.92G/4.92G [04:33&lt;00:00, 18.0MB/s]\nmodel-00002-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [04:47&lt;00:00, 17.4MB/s]\nFetching 16 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [05:42&lt;00:00, 21.43s/it]:33&lt;00:52, 9.33MB/s]\n## Successfully downloaded model_files|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [04:47&lt;00:00, 110MB/s] \n\nDeleting directory : /data/hf_cache\n</code></pre></p> </li> <li> <p>Optional - verify the events in the namespace for the pvc creation </p> CommandCommand output <pre><code>k get events | awk '{print $1, $3}'\n</code></pre> <pre><code>$ k get events | awk '{print $1, $3}'\n\n3m43s Scheduled\n3m43s SuccessfulAttachVolume\n3m36s Pulling\n3m29s Pulled\n3m29s Created\n3m29s Started\n3m43s SuccessfulCreate\n90s   Completed\n3m53s Provisioning\n3m53s ExternalProvisioning\n3m45s ProvisioningSucceeded\n3m53s PvcCreateSuccessful\n3m48s PvcNotBound\n3m43s ModelProcessorJobActive\n90s   ModelProcessorJobComplete\n</code></pre> </li> </ol> <p>The model is downloaded to the Nutanix Files <code>pvc</code> volume.</p> <p>After a successful model import, you will see it in Active status in the NAI UI under Models menu</p> <p></p>"},{"location":"iep/iep_deploy/#create-and-test-inference-endpoint","title":"Create and Test Inference Endpoint","text":"<p>In this section we will create an inference endpoint using the downloaded model.</p> <ol> <li>Navigate to Inference Endpoints menu and click on Create Endpoint button</li> <li> <p>Fill the following details based on GPU or CPU availability:</p> <p>Tip</p> <p>NAI <code>v2.3</code> can host a model up to 7 billion parameters on CPU only nodes</p> GPU AccessCPU Access <ul> <li>Endpoint Name: <code>llama-8b</code></li> <li>Model Instance Name: <code>Meta-LLaMA-8B-Instruct</code></li> <li>Use GPUs for running the models : <code>Checked</code></li> <li>No of GPUs (per instance):</li> <li>GPU Card: <code>NVIDIA-L40S</code> (or other available GPU)</li> <li>No of Instances: <code>1</code></li> <li>API Keys: Create a new API key or use an existing one</li> </ul> <ul> <li>Endpoint Name: <code>llama-8b</code></li> <li>Model Instance Name: <code>Meta-LLaMA-8B-Instruct</code></li> <li>Use GPUs for running the models : <code>leave unchecked</code></li> <li>No of Instances: <code>1</code></li> <li>API Keys: Create a new API key or use an existing one</li> </ul> </li> <li> <p>Click on Create</p> </li> <li> <p>Monitor the <code>nai-admin</code> namespace to check if the services are coming up</p> CommandCommand output <pre><code>kubens nai-admin\nkubectl get po,deploy\n</code></pre> <pre><code>kubens nai-admin\nget po,deploy\nNAME                                                     READY   STATUS        RESTARTS   AGE\npod/llama8b-predictor-00001-deployment-9ffd786db-6wkzt   2/2     Running       0          71m\n\nNAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/llama8b-predictor-00001-deployment   1/1     1            0           3d17h\n</code></pre> </li> <li> <p>Check the events in the <code>nai-admin</code> namespace for resource usage to make sure there are no errors</p> CommandCommand output <pre><code>kubectl get events -n nai-admin --sort-by='.lastTimestamp' | awk '{print $1, $3, $5}'\n</code></pre> <pre><code>$ kubectl get events -n nai-admin --sort-by='.lastTimestamp' | awk '{print $1, $3, $5}'\n\n110s FinalizerUpdate Updated\n110s FinalizerUpdate Updated\n110s RevisionReady Revision\n110s ConfigurationReady Configuration\n110s LatestReadyUpdate LatestReadyRevisionName\n110s Created Created\n110s Created Created\n110s Created Created\n110s InferenceServiceReady InferenceService\n110s Created Created\n</code></pre> </li> <li> <p>Once the services are running, check the status of the inference service</p> CommandCommand output <pre><code>kubectl get isvc\n</code></pre> <pre><code>kubectl get isvc\n\nNAME      URL                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllama8b   http://llama8b.nai-admin.svc.cluster.local   True           100                              llama8b-predictor-00001   3d17h\n</code></pre> </li> </ol>"},{"location":"iep/iep_pre_reqs/","title":"Pre-requisites for Deploying NAI","text":"<p>In this part of the lab we will prepare pre-requisites for LLM application on GPU nodes.</p> <p>The following is the flow of the applications lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state PreRequisites {\n        [*] --&gt; CreateFilesShare  \n        CreateFilesShare --&gt; PrepareHuggingFace\n        PrepareHuggingFace --&gt; [*]\n    }\n\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI : next section\n    DeployNAI --&gt; TestNAI\n    TestNAI --&gt; [*]</code></pre> <p>Prepare the following pre-requisites needed to deploy NAI on target kubernetes cluster.</p>"},{"location":"iep/iep_pre_reqs/#create-nutanix-files-storage-class","title":"Create Nutanix Files Storage Class","text":"<p>We will create Nutanix Files storage class which will be used to create a pvc that will store the <code>LLama-3-8B</code> model files.</p> <ol> <li>In Prism Central, choose Files from the menu</li> <li>Choose the file server (e.g. labFS)</li> <li>Click on Shares &amp; Exports</li> <li>Click on +New Share or Export</li> <li> <p>Fill the details of the Share</p> <ul> <li>Name - model_share</li> <li>Description - for NAI model store</li> <li>Share path - leave blank</li> <li>Max Size - 10 GiB (adjust to the model file size)</li> <li>Primary Protocol Access - NFS</li> </ul> </li> <li> <p>Click Next and make sure Enable compression in checked</p> </li> <li>Click Next </li> <li> <p>In NFS Protocol Access, choose the following: </p> <ul> <li>Authentication - System</li> <li>Default Access (for all clients) - Read-Write </li> <li>Squash - Root Squash</li> </ul> <p>Note</p> <p>Consider changing access options for Production environment</p> </li> <li> <p>Click Next</p> </li> <li>Confirm the share details and click on Create</li> </ol>"},{"location":"iep/iep_pre_reqs/#create-the-files-storage-class","title":"Create the Files Storage Class","text":"<ol> <li> <p>Run the following command to check K8S status of the <code>nkpdev</code> cluster</p> <pre><code>kubectl get nodes\n</code></pre> </li> <li> <p>In VSC Explorer, click on New File  and create a config file with the following name:</p> <pre><code>nai-nfs-storage.yaml\n</code></pre> <p>Add the following content and replace the <code>nfsServerName</code> with the name of the Nutanix Files server name .</p> <p></p> Template YAMLSample YAML <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nai-nfs-storage\nparameters:\n  nfsPath: &lt;nfs-path&gt;\n  nfsServer: &lt;nfs-server&gt;\n  storageType: NutanixFiles\nprovisioner: csi.nutanix.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nai-nfs-storage\nparameters:\n  nfsPath: /model_share\n  nfsServer: labFS.ntnxlab.local\n  storageType: NutanixFiles\nprovisioner: csi.nutanix.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n</code></pre> </li> <li> <p>Create the storage class</p> <pre><code>kubectl apply -f nai-nfs-storage.yaml\n</code></pre> </li> <li> <p>Check storage classes in the cluster for the Nutanix Files storage class</p> CommandCommand output <pre><code>kubectl get storageclass\n</code></pre> <pre><code>kubectl get storageclass\n\nNAME                       PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ndkp-object-store           kommander.ceph.rook.io/bucket   Delete          Immediate              false                  28h\nnai-nfs-storage            csi.nutanix.com                 Delete          Immediate              true                   24h\nnutanix-volume (default)   csi.nutanix.com                 Delete          WaitForFirstConsumer   false                  28h\n</code></pre> </li> </ol>"},{"location":"iep/iep_pre_reqs/#request-access-to-model-on-hugging-face","title":"Request Access to Model on Hugging Face","text":"<p>Follow these steps to request access to the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model:</p> <p>LLM Recommendation</p> <p>From testing <code>google/gemma-2-2b-it</code> model is quicker to download and obtain download rights, than <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model.</p> <p>Feel free to use the google/gemma-2-2b-it model if necessary. The procedure to request access to the model is the same.</p> <ol> <li> <p>Sign in to your Hugging Face account:  </p> <ul> <li>Visit Hugging Face and log in to your account.</li> </ul> </li> <li> <p>Navigate to the model page:  </p> <ul> <li>Go to the Meta-Llama-3.1-8B-Instruct model page.</li> </ul> </li> <li> <p>Request access:</p> <ul> <li>On the model page, you will see a section or button labeled Request Access (this is usually near the top of the page or near the \"Files and versions\" section).</li> <li>Click Request Access.</li> </ul> </li> <li> <p>Complete the form:</p> <ul> <li>You may be prompted to fill out a form or provide additional details about your intended use of the model.</li> <li>Complete the required fields and submit the request.</li> </ul> </li> <li> <p>Wait for approval:</p> <ul> <li>After submitting your request, you will receive a notification or email once your access is granted.</li> <li>This process can take some time depending on the approval workflow.</li> </ul> </li> </ol> <p>Once access is granted, there will be an email notification.</p> <p>Note</p> <p>Email from Hugging Face can take a few minutes or hours before it arrives.</p>"},{"location":"iep/iep_pre_reqs/#create-a-hugging-face-token-with-read-permissions","title":"Create a Hugging Face Token with Read Permissions","text":"<p>Follow these steps to create a Hugging Face token with read permissions:</p> <ol> <li> <p>Sign in to your Hugging Face account:  </p> <ul> <li>Visit Hugging Face and log in to your account.</li> </ul> </li> <li> <p>Access your account settings:</p> <ul> <li>Click on your profile picture in the top-right corner.</li> <li>From the dropdown, select Settings.</li> </ul> </li> <li> <p>Navigate to the \"Access Tokens\" section:</p> <ul> <li>In the sidebar, click on Access Tokens.</li> <li>You will see a page where you can create and manage tokens.</li> </ul> </li> <li> <p>Create a new token:</p> <ul> <li>Click the New token button.</li> <li>Enter a name for your token (i.e., <code>read-only-token</code>).</li> </ul> </li> <li> <p>Set token permissions:</p> <ul> <li>Under the permissions dropdown, select Read. For Example:     </li> </ul> </li> <li> <p>Create and copy the token:</p> <ul> <li>After selecting the permissions, click Create.</li> <li>Your token will be generated and displayed only once, so make sure to copy it and store it securely.</li> </ul> </li> </ol> <p>Use this token for accessing Hugging Face resources with read-only permissions.</p>"},{"location":"iep/iep_pre_reqs/#prepare-nai-docker-download-credentials","title":"Prepare NAI Docker Download Credentials","text":"<p>All NAI Docker images will be downloaded from the public Docker Hub registry. In order to download the images, you will need to logon to Nutanix Portal - NAI and create a Docker ID and access token.</p> <ol> <li>Login to Nutanix Portal - NAI using your credentials</li> <li>Click on Generate Access Token option</li> <li>Copy the generated Docker ID and access token to a safe place as we will need it for the Deploy NAI section.</li> </ol> <p>Warning</p> <p>Currently there are issues with the Nutanix Portal to create a Docker ID and access token. This will be fixed soon.</p> <p>Click on the Manage Access Token option and use the credentials listed there until the Nutanix Portal is fixed.</p>"},{"location":"iep/iep_test/","title":"Deploying GPT-in-a-Box NVD Reference Application using GitOps (FluxCD)","text":"<pre><code>stateDiagram-v2\n    direction LR\n\n    state TestNAI {\n        [*] --&gt; CheckInferencingService\n        CheckInferencingService --&gt;  TestChatApp\n        TestChatApp --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI \n    DeployNAI --&gt; TestNAI : previous section\n    TestNAI --&gt; [*]</code></pre>"},{"location":"iep/iep_test/#test-querying-inference-service-api","title":"Test Querying Inference Service API","text":"<ol> <li> <p>Prepare the API key that was created in the previous section</p> Template commandSample command <pre><code>export API_KEY=_your_endpoint_api_key\n</code></pre> <pre><code>export API_KEY=5840a693-254d-41ef-a2d3-1xxxxxxxxxx\n</code></pre> </li> <li> <p>Construct your <code>curl</code> command using the API key obtained above, and run it on the terminal</p> CommandCommand output <pre><code>curl -k -X 'POST' 'https://nai.10.x.x.216.nip.io/api/v1/chat/completions' \\\n-H \"Authorization: Bearer $API_KEY\" \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model\": \"llama-8b\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": \"What is the capital of France?\"\n        }\n    ],\n    \"max_tokens\": 256,\n    \"stream\": false\n}'\n</code></pre> <pre><code>{\n    \"id\": \"9e55abd1-2c91-4dfc-bd04-5db78f65c8b2\",\n    \"object\": \"chat.completion\",\n    \"created\": 1728966493,\n    \"model\": \"llama-8b\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"The capital of France is Paris. It is a historic city on the Seine River in the north-central part of the country. Paris is also the political, cultural, and economic center of France.\"\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 17,\n        \"completion_tokens\": 41,\n        \"total_tokens\": 58\n    },\n    \"system_fingerprint\": \"\"\n}\n</code></pre> </li> </ol> <p>We have a successful NAI deployment.</p>"},{"location":"iep/iep_test/#accessing-llm-frontend-ui","title":"Accessing LLM Frontend UI","text":"<ol> <li> <p>In the NAI GUI, under Endpoints, click on the llama8b</p> </li> <li> <p>Click on Test</p> </li> <li> <p>Provide a sample prompt and check the output</p> <p></p> </li> </ol>"},{"location":"iep/iep_test/#sample-chat-application","title":"Sample Chat Application","text":"<p>We have a sample chat application that uses NAI to provide chatbot capabilities. We will install and use the chat application in this section.</p> <ol> <li> <p>Run the following command to deploy the chat application.</p> <p>Create the namespace</p> <pre><code>kubectl create ns chat\nkubens chat\n</code></pre> <p>Create the application</p> <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nai-chatapp\n  namespace: chat\n  labels:\n    app: nai-chatapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nai-chatapp\n  template:\n    metadata:\n      labels:\n        app: nai-chatapp\n    spec:\n      containers:\n      - name: nai-chatapp\n        image: johnugeorge/nai-chatapp:0.12\n        ports:\n        - containerPort: 8502\nEOF\n</code></pre> <p>Create the service</p> <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: nai-chatapp\n  namespace: chat\nspec:\n  selector:\n    app: nai-chatapp\n  ports:\n    - protocol: TCP\n      port: 8502\nEOF\n</code></pre> </li> <li> <p>Create the route for application access</p> </li> <li> <p>Insert <code>chat</code> as the subdomain in the <code>nai.10.x.x.216.nip.io</code> main domain.</p> <p>Example: complete URL</p> <pre><code>chat.nai.10.x.x.216.nip.io\n</code></pre> <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: nai-chatapp-route\n  namespace: chat                   # Same namespace as your chat app service\nspec:\n  parentRefs:\n  - name: nai-ingress-gateway\n    namespace: nai-system           # Namespace of the Gateway\n  hostnames:\n  - \"chat.nai.10.x.x.216.nip.io\"    # Input Gateway IP address\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: nai-chatapp\n      kind: Service\n      port: 8502\nEOF\n</code></pre> </li> <li> <p>We should be able to see the chat application running on the NAI cluster at the following url</p> <pre><code>chat.nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>In the chat app GUI, input the following:</p> <ul> <li>Endpoint URL - e.g. <code>https://nai.10.x.x.216.nip.io/api/v1/chat/completions</code> (can be found in the Endpoints on NAI GUI)</li> <li>Endpoint Name - e.g. <code>llama-8b</code></li> <li>API key - created during Endpoint creation</li> </ul> <p></p> </li> </ol> <p>We have successfully deployed the following:</p> <ul> <li>Inferencing endpoint</li> <li>A sample chat application that uses NAI to provide chatbot capabilities</li> </ul>"},{"location":"infra/","title":"Getting Started","text":"<p>In this section we will deploy infrastructure to be able to deploy NAI.</p> <p>We will deploy a jumphost to work with. We will deploy all the necessary tools on this jumphost. Once this is done, we will use this jumphost to deploy the NAI platform.</p> <p>This guide covers two different scenarios for deploying Nutanix Enterprise AI [NAI] (previously known as GPT-In-A-Box).  </p> <ol> <li>The first scenario is a walkthrough on How-To Deploy Nutanix Enterprise AI (NAI) with Nutanix Kubernetes Platform (NKP), and </li> <li>The second scenario covers the (soon to be deprecated) option on How-To Deploy GPT-In-A-Box v1 Nutanix Validated Design (NVD) with NKE .</li> </ol> <p>Deprecation Notice!</p> <p>GPT-In-A-Box v1 and NKE will be deprecated soon. We will be removing these options from the this site as soon as it is deprecated.</p> <p>Each scenario goes through four phases to prepare the infrastructure on which you can deploy Nutanix Enterprise AI applications.</p> <ol> <li>Prepare the workstation.</li> <li>Deploy the jumphost<ol> <li>Option A - Deploy NKP to deploy NAI</li> <li>Option B - Deploy NKE to deploy GPT-In-A-Box v1</li> </ol> </li> </ol> <p>Here is a diagram that shows the four phases.</p> <pre><code>stateDiagram-v2\n  direction LR\n\n  state PrepWorkstation {\n    [*] --&gt; GenrateRSAKeys\n    GenrateRSAKeys --&gt; InstallTofu\n    InstallTofu --&gt; InstallVSCode\n    InstallVSCode --&gt; [*]\n  }\n  state DeployJumpHost {\n    [*] --&gt; CreateCloudInit\n    CreateCloudInit --&gt; CreateJumpHostVM\n    CreateJumpHostVM --&gt; DeployNaiUtils\n    DeployNaiUtils --&gt; [*]\n  }\n  PrepWorkstation --&gt; DeployJumpHost\n  DeployJumpHost --&gt; DeployNkp: Option A\n  DeployNkp --&gt; DeployNai\n  DeployJumpHost --&gt; DeployNke: Option B\n  DeployNke --&gt; DeployGiabGitOps</code></pre>"},{"location":"infra/#deploy-nai-with-nkp","title":"Deploy NAI with NKP","text":"<ol> <li>Prepare Local Development Workstation (Mac/Windows)</li> <li>Deploy Jumphost VM</li> <li>Deploy Nutanix Kubernetes Platform (NKP) Management Cluster</li> <li>Deploy Nutanix Enterprise AI (NAI)</li> </ol> <p>Here is a diagram that shows the workflow:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKP {\n        [*] --&gt; CreateNkpMachineImage\n        CreateNkpMachineImage --&gt; CreateNkpSelfManagedCluster\n        CreateNkpSelfManagedCluster --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n    state DeployNai {\n        [*] --&gt; PrepareNai\n        PrepareNai --&gt; DeployNaiHelm \n        DeployNaiHelm --&gt; [*]\n    }\n\n    [*] --&gt; PrepWorkstation\n    PrepWorkstation --&gt; DeployJumpHost\n    DeployJumpHost --&gt; DeployNKP\n    DeployNKP --&gt; DeployNai\n    DeployNai --&gt; [*]</code></pre>"},{"location":"infra/#deploy-gpt-in-a-box-v1-with-nke","title":"Deploy GPT-In-A-Box (v1) with NKE","text":"<ol> <li>Prepare your Local Development Workstation (Mac/Windows)</li> <li>Deploy Jumphost VM</li> <li>Deploy Nutanix Kubernetes Engine (NKE) - Management Cluster</li> <li>Deploy Nutanix Kubernetes Engine (NKE) - Development Workload Cluster</li> <li>Deploy Nutanix GPT-In-A-Box (v1) Validated Design Reference RAG Applications using Flux GitOps</li> </ol> <p>Here is a diagram that shows the workflow:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKE {\n        [*] --&gt; CreateTofuWorkspaces\n        CreateTofuWorkspaces --&gt; CreateMgtK8SCluster\n        CreateMgtK8SCluster --&gt; CreateDevK8SCluster\n        CreateDevK8SCluster --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n    state DeployGiabGitOps {\n        [*] --&gt; BootstrapFlux\n        BootstrapFlux --&gt; DeployAIApps\n        DeployAIApps --&gt; [*]\n    }\n\n    [*] --&gt; PrepWorkstation\n    PrepWorkstation --&gt; DeployJumpHost\n    DeployJumpHost --&gt; DeployNKE\n    DeployNKE --&gt; DeployGiabGitOps\n    DeployGiabGitOps --&gt; [*]</code></pre>"},{"location":"infra/harbor/","title":"Install Harbor","text":"<p>In this section, we will install Harbor container registry in the cluster.</p>"},{"location":"infra/harbor/#prerequisites","title":"Prerequisites","text":"<p>We will use the jumphost to install and host Harbor container registry.</p> <p>Since the jumphost also will host the <code>kind</code> cluster, we will need to ensure that the jumphost has enough resources.</p> # CPU Memory Disk Purpose Before 4 16 GB 300 GB <code>Jumphost</code> + <code>Tools</code> After 8 16 GB 300 GB <code>Jumphost</code> + <code>Tools</code> + <code>Harbor</code> + <code>kind</code> <p>Note</p> <p>If the jumphost does not have the resources, make sure to stop the jumphost and add the resources in Prism Central.</p>"},{"location":"infra/harbor/#install-harbor_1","title":"Install Harbor","text":"<p>We will use the following commands to install Harbor on the jumphost.</p>"},{"location":"infra/harbor/#download-harbor","title":"Download Harbor","text":"<ol> <li> <p>Open new VSCode window on your jumphost</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, Click on New Folder  and name it: <code>harbor</code></p> </li> <li> <p>In <code>VSCode</code> Terminal pane, run the following commands to download Harbor.</p> <pre><code>cd $HOME/harbor\nexport HARBOR_VERSION=v2.9.4\ncurl -sSOL https://github.com/goharbor/harbor/releases/download/${HARBOR_VERSION}/harbor-offline-installer-${HARBOR_VERSION}.tgz\ntar -xzvf harbor-offline-installer-${HARBOR_VERSION}.tgz\n</code></pre> </li> </ol>"},{"location":"infra/harbor/#optional-setup-ssl-certificates-for-harbor","title":"Optional - Setup SSL Certificates for Harbor","text":"<p>Do you have your own certificates for Harbor?</p> <p>If there is domain of your ownership, generate the following:</p> <ul> <li>fullchain.pem - must contain both CA server's public and harbor server's public certificate</li> <li>privatekey.pem - contain Harbor's private certificate</li> </ul> <p>Use these in the <code>harbor.yml</code> file during installation</p> <p>Use the following procedure to create a self-managed Certificate Authority, Certificate Signing Request and a certificate pair for Harbor.</p> <ol> <li> <p>Setup up folders to hold certificates for Harbor</p> <p><pre><code>cd $HOME/harbor\nmkdir certs &amp;&amp; cd certs \nHARBOR_HOST=$(hostname -I | awk '{print $1}')\nHARBOR_HOST_EXTERNAL=$HARBOR_HOST\n</code></pre> Confirm the environment variables values.</p> CommandCommand output <pre><code>echo ${HARBOR_HOST}  \necho ${HARBOR_HOST_EXTERNAL}\n</code></pre> <pre><code>10.x.x.111\n10.x.x.111\n</code></pre> </li> <li> <p>Create a root CA certificate and key</p> <pre><code>openssl genrsa -out ca.key 4096\nopenssl req -x509 -new -nodes -key ca.key -sha256 -days 1024 -out ca.crt -subj \"/CN=harborroot\"\n</code></pre> </li> <li> <p>Install the CA certificate on the jumphost VM trusted root CA store</p> <pre><code>sudo cp ca.crt /usr/local/share/ca-certificates/\nsudo update-ca-certificates\n</code></pre> </li> <li> <p>Create private key for Harbor</p> <pre><code>openssl genrsa -out harbor.key 2048\n</code></pre> </li> <li> <p>Create CSR for Harbor</p> <pre><code>openssl req -new -key harbor.key -subj /CN=${HARBOR_HOST} -out harbor.csr\n</code></pre> </li> <li> <p>Add all possible FQDNs and IPs to the certificate's subjectAltName (SAN) field and generate the certificate for Harbor</p> <pre><code>openssl x509 -req -extfile &lt;(printf \"subjectAltName=IP:${HARBOR_HOST_EXTERNAL},IP:`hostname -I | awk '{print $1}'`,DNS:harbor.${HARBOR_HOST}.nip.io,DNS:${HARBOR_HOST}.nip.io\") -days 1024 -in harbor.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out harbor.crt\n</code></pre> </li> <li> <p>Convert the cert from PEM to DER for Docker</p> <pre><code>cd ..\ncp certs/harbor.crt certs/harbor.crt_convert # To preserve the PEM format\nopenssl x509 -inform PEM -in certs/harbor.crt_convert  -out certs/harbor.cert\n</code></pre> </li> <li> <p>Set Harbor certificate in local Docker instance</p> <pre><code>sudo mkdir -p /etc/docker/certs.d/${HARBOR_HOST}\n</code></pre> </li> <li> <p>Copy to ca.crt,harbor.cert and harbor.key to Docker certs directory </p> <pre><code>sudo cp certs/ca.crt /etc/docker/certs.d/${HARBOR_HOST}/.\n\nsudo cp certs/harbor.cert /etc/docker/certs.d/${HARBOR_HOST}/.\n\nsudo cp certs/harbor.key /etc/docker/certs.d/${HARBOR_HOST}/.\n\nsudo systemctl restart docker\n</code></pre> </li> </ol>"},{"location":"infra/harbor/#add-harbors-ca-certificate-to-the-trusted-ca-store-of-nkp-nodes","title":"Add Harbor's CA Certificate to the Trusted CA Store of NKP Nodes","text":"<p>Warning</p> <p>The local CA certificate that is certifying Harbor's certificate will need to be added to all the NKP air-gapped cluster nodes.</p> <p>Kubernetes nodes will only trust the CA certificates present on the nodes apart from the public CA certificates (Let's Encrypt, Digicert, etc.).</p> <p>Follow the steps in this Deploying Private CA Certificate to NKP Cluster section to add the Harbor container registry's CA certificate <code>ca.crt</code> that you created in the above section to the NKP air-gapped cluster nodes.</p> <p>Best Practice for Self-signed Certificates</p> <p>The best practice is to deploy the NKP air-gapped cluster with a Self-signed Certificates (private) CA certificate at Day 0 using the <code>nkp create cluster nutanix --additional-trust-bundle</code> among other options.</p> <p>For Day 1 and 2 operations, the private CA certificate will need to be added to all the NKP air-gapped cluster nodes.</p>"},{"location":"infra/harbor/#configure-harbor-installation-manifest","title":"Configure Harbor Installation Manifest","text":"<ol> <li> <p>In VSCode Terminal, run the following command to setup and create the manifest file:</p> <pre><code>export OS_USER=ubuntu # Set OS User\nexport CERTS_DIR=/home/${OS_USER}/harbor/certs\nexport HOST=${HARBOR_HOST}\n</code></pre> </li> <li> <p>In VSCode Explorer, create the manifest file</p> <p><pre><code>harbor.yml\n</code></pre>    with the following content: (focus on the hightlighted lines)</p> Template file Sample file <pre><code>hostname: ${HOST}\nhttp:\n  port: 80\nhttps:\n  port: 443\n  certificate: ${CERTS_DIR}/harbor.crt\n  private_key: ${CERTS_DIR}/harbor.key\n# Uncomment external_url if you want to enable external proxy\n# And when it enabled the hostname will no longer used\nexternal_url: https://${HOST}/\nharbor_admin_password: _your_harbor_password\ndatabase:\n  password: _your_harbor_db_password\n  max_idle_conns: 100\n  max_open_conns: 900\n  conn_max_idle_time: 0\ndata_volume: /data\ntrivy:\n  ignore_unfixed: false\n  skip_update: false\njobservice:\n  max_job_workers: 10\n  logger_sweeper_duration: 1 #days \n  job_loggers:\n    - STD_OUTPUT\n    - FILE\n    # - DB\nnotification:\n  webhook_job_max_retry: 3\n  webhook_job_http_client_timeout: 3 #seconds\nchart:\n  absolute_url: disabled\nlog:\n  level: info\n  local:\n    rotate_count: 50\n    rotate_size: 200M\n    location: /var/log/harbor\nproxy:\n  http_proxy:\n  https_proxy:\n# no_proxy endpoints will appended to 127.0.0.1,localhost,.local,.internal,log,db,redis,nginx,core,portal,postgresql,jobservice,registry,registryctl,clair,chartmuseum,notary-server\n  no_proxy:\n  components:\n    - core\n    - jobservice\n    - trivy\n_version: ${HARBOR_VERSION}\n</code></pre> <pre><code>hostname: 10.x.x.111\nhttp:\n  port: 80\nhttps:\n  port: 443\n  certificate: /home/ubuntu/harbor/certs/harbor.crt # Comment this line if using own public key\n  private_key: /home/ubuntu/harbor/certs/harbor.key # Comment this line if using own private key\n  # certificate: /etc/letsencrypt/live/harbor.apj-cxrules.win/fullchain.pem # Uncomment for your own public key\n  # private_key: /etc/letsencrypt/live/harbor.apj-cxrules.win/privkey.pem   # Uncomment for your own private key\n# Uncomment external_url if you want to enable external proxy\n# And when it enabled the hostname will no longer used\nexternal_url: https://harbor.10.x.x.111.nip.io/\nharbor_admin_password: xxxxxxx\ndatabase:\n  password: xxxxxxx\n  max_idle_conns: 100\n  max_open_conns: 900\n  conn_max_idle_time: 0\ndata_volume: /data\ntrivy:\n  ignore_unfixed: false\n  skip_update: false\njobservice:\n  max_job_workers: 10\n  logger_sweeper_duration: 1 #days \n  job_loggers:\n    - STD_OUTPUT\n    - FILE\n    # - DB\nnotification:\n  webhook_job_max_retry: 3\n  webhook_job_http_client_timeout: 3 #seconds\nchart:\n  absolute_url: disabled\nlog:\n  level: info\n  local:\n    rotate_count: 50\n    rotate_size: 200M\n    location: /var/log/harbor\nproxy:\n  http_proxy:\n  https_proxy:\n# no_proxy endpoints will appended to 127.0.0.1,localhost,.local,.internal,log,db,redis,nginx,core,portal,postgresql,jobservice,registry,registryctl,clair,chartmuseum,notary-server\n  no_proxy:\n  components:\n    - core\n    - jobservice\n    - trivy\n_version: v2.9.4\n</code></pre> </li> </ol>"},{"location":"infra/harbor/#install-and-verify","title":"Install and Verify","text":"<ol> <li> <p>Run the installation</p> <pre><code>sudo ./install.sh --with-trivy\n</code></pre> </li> <li> <p>Verify the installation</p> CommandCommand output <pre><code>sudo docker-compose ps\n</code></pre> <pre><code>$ sudo docker-compose ps \n\nName                     Command                  State                                          Ports                                    \n------------------------------------------------------------------------------------------------------------------------------------------------\nharbor-core         /harbor/entrypoint.sh            Up (healthy)                                                                               \nharbor-db           /docker-entrypoint.sh 13 14      Up (healthy)                                                                               \nharbor-jobservice   /harbor/entrypoint.sh            Up (healthy)                                                                               \nharbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcp                                                   \nharbor-portal       nginx -g daemon off;             Up (healthy)                                                                               \nnginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80-&gt;8080/tcp,:::80-&gt;8080/tcp, 0.0.0.0:443-&gt;8443/tcp,:::443-&gt;8443/tcp\nredis               redis-server /etc/redis.conf     Up (healthy)                                                                               \nregistry            /home/harbor/entrypoint.sh       Up (healthy)                                                                               \nregistryctl         /home/harbor/start.sh            Up (healthy)                                                                               \ntrivy-adapter       /home/scanner/entrypoint.sh      Up (healthy)                    \n</code></pre> </li> <li> <p>Login to Harbor Web UI using the following credentials</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>xxxxxxx</code> (password you set in the manifest file)</li> </ul> </li> <li> <p>Go to Projects and create a new project <code>nkp</code></p> </li> </ol> <p>Harbor registry and <code>nkp</code> projects will be used to store the container images for NKP air-gapped deployments.</p>"},{"location":"infra/infra_dkp%20copy/","title":"Deploy NKP Clusters","text":"<p>This section will take you through install NKP(Kubernetes) on Nutanix cluster as we will be deploying AI applications on these kubernetes clusters. </p> <p>This section will expand to other available Kubernetes implementations on Nutanix.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKP {\n        [*] --&gt; CreateBootStrapCluster\n        CreateBootStrapCluster --&gt; CreateNKPCluster\n        CreateNKPCluster --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n\n    PrepWorkstation --&gt; DeployJumpHost \n    DeployJumpHost --&gt; DeployNKP \n    DeployNKP --&gt; DeployNai : Next section</code></pre>"},{"location":"infra/infra_dkp%20copy/#nkp-high-level-cluster-design","title":"NKP High Level Cluster Design","text":"<p>The <code>Bootstrap</code> NKP cluster will be a temporary kind cluster that will be used to deploy the DEV cluster.</p> <p>The <code>DEV</code> NKP cluster will be hosting the LLM model serving endpoints and AI application stack. This cluster and will require a dedicated GPU node pool.</p> <p>Once DEV deployment has been tested successfully, we can deploy applications to optional PROD Workload cluster.</p> <p>Note</p> <p>The <code>Bootstrap</code> cluster's Cluster API (CAPI)  components will be migrated to the DEV cluster and the bootstrap cluster will be deleted.</p>"},{"location":"infra/infra_dkp%20copy/#bootstrap-cluster","title":"Bootstrap Cluster","text":"<p>Since the Bootstrap Cluster will be essential to deploying a workload DEV cluster. We will use <code>kind</code> cluster packaged by Nutanix. <code>kind</code> is already installed on the jumphost VM and be accessed using <code>devbox shell</code>.</p>"},{"location":"infra/infra_dkp%20copy/#dev-workload-cluster","title":"Dev Workload Cluster","text":"<p>For Dev, we will deploy an NKE Cluster of type \"Development\".</p> Role No. of Nodes (VM) vCPU RAM Storage Master 3 4 16 GB 80 GB Worker 4 8 32 GB 80 GB GPU 1 16 64 GB 200 GB"},{"location":"infra/infra_dkp%20copy/#pre-requisites-for-nkp-deployment","title":"Pre-requisites for NKP Deployment","text":"<ol> <li>Download and install dkp and dkp-image-builder binaries (will be documented soon)</li> <li>Find and reserve 3 IPs for control plane and MetalLB access from AHV network</li> <li>Create a base image to use with NKP nodes using <code>dkp-image-builder</code></li> </ol>"},{"location":"infra/infra_dkp%20copy/#install-nkp-binaries","title":"Install NKP Binaries","text":"<p>To be documented.</p>"},{"location":"infra/infra_dkp%20copy/#reserve-control-plane-and-metallb-endpoint-ips","title":"Reserve Control Plane and MetalLB Endpoint IPs","text":"<p>Nutanix AHV IPAM network allows you to black list IPs that needs to be reserved for specific application endpoints. We will use this feature to find and reserve three IPs. </p> <p>We will need a total of three IPs for the following:</p> Cluster Role Cluster Name Control Plane IP MetalLB  IP Dev <code>nkp-dev</code> 1 2 <ol> <li> <p>Get the CIDR range for the AHV network(subnet) where the application will be deployed</p> CIDR example for your Nutanix cluster<pre><code>10.x.x.0/24\n</code></pre> </li> <li> <p>From VSC, logon to your jumpbox VM and open Terminal</p> </li> <li> <p>Install <code>nmap</code> tool (if not already done)</p> <pre><code>cd $HOME/sol-cnai-infra\ndevbox add nmap\n</code></pre> </li> <li> <p>Find three unused static IP addresses in the subnet</p> Template commandSample command <pre><code>nmap -v -sn  &lt;your CIDR&gt;\n</code></pre> <pre><code>nmap -v -sn 10.x.x.0/24\n</code></pre> Sample output - choose the first three consecutive IPs<pre><code>Nmap scan report for 10.x.x.214 [host down]\nNmap scan report for 10.x.x.215 [host down]\nNmap scan report for 10.x.x.216 [host down]\nNmap scan report for 10.x.x.217\nHost is up (-0.098s latency).\n</code></pre> </li> <li> <p>Logon to any CVM in your Nutanix cluster and execute the following to add chosen static IPs to the AHV IPAM network</p> <ul> <li>Username: nutanix</li> <li>Password: your Prism Element password </li> </ul> Template commandSample command <pre><code>acli net.add_to_ip_blacklist &lt;your-ipam-ahv-network&gt; \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> <pre><code>acli net.add_to_ip_blacklist User1 \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> </li> </ol>"},{"location":"infra/infra_dkp%20copy/#create-base-image-for-nkp","title":"Create Base Image for NKP","text":"<p>In this section we will go through creating a base image for all the control plane and worker node VMs on Nutanix.</p> <ol> <li> <p>In VSC Explorer pane, Click on New Folder </p> </li> <li> <p>Call the folder <code>dkp</code> under <code>$HOME</code> directory</p> </li> <li> <p>In the <code>dkp</code> folder, click on New File  with the following name</p> <pre><code>.env\n</code></pre> </li> <li> <p>Fill the following values inside the <code>.env</code> file</p> Template fileSample file <pre><code>export NUTANIX_USER=_your_nutanix_username\nexport NUTANIX_PASSWORD=_your_nutanix_password\nexport NUTANIX_ENDPOINT=_your_prism_central_cluster_name\nexport NUTANIX_CLUSTER=_your_prism_element_cluster_name\nexport NUTANIX_SUBNET_NAME=_your_ahv_ipam_network_name\nexport STORAGE_CONTAINER=_your_storage_container_nmae\nexport SSH_PUBLIC_KEY=_your_path_to_ssh_pub_key\nexport NKP_CLUSTER_NAME=_your_nkp_cluster_name\nexport GPU_NAME=_your_gpu_name\nexport CONTROLPLANE_VIP=_your_nkp_cluster_controlplane_ip\nexport METALLB_IP_RANGE=_your_range_of_three_ips\n</code></pre> <pre><code>export NUTANIX_USER=admin\nexport NUTANIX_PASSWORD=xxxxxxxx\nexport NUTANIX_ENDPOINT=pc.example.com\nexport NUTANIX_CLUSTER=pe\nexport NUTANIX_SUBNET_NAME=User1\nexport STORAGE_CONTAINER=default\nexport SSH_PUBLIC_KEY=$HOME/.ssh/id_rsa.pub\nexport NKP_CLUSTER_NAME=nkp1\nexport GPU_NAME=\"Lovelace 40S\"\nexport CONTROLPLANE_VIP=10.x.x.214\nexport METALLB_IP_RANGE=10.x.x.215-10.x.x.216\n</code></pre> </li> <li> <p>Using VSC Terminal, load the environment variables and its values</p> <pre><code>cd $HOME/dkp\nsource .env\n</code></pre> </li> <li> <p>Create the base image and upload to Prism Central using the following command. </p> <pre><code>dkp-image-builder create image nutanix ubuntu-22.04 --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} --subnet ${NUTANIX_SUBNET_NAME}\n</code></pre> <p>Note</p> <p>Image creation will take up to 5 minutes.</p> <p>dkp-image-builder create image nutanix ubuntu-22.04 \\ --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} \\--subnet ${NUTANIX_SUBNET_NAME} Provisioning and configuring imageManifest files extracted to $HOME/dkp/.dkp-image-builder-3243021807nutanix.kib_image: output will be in this color.==&gt; nutanix.kib_image: Creating Packer Builder virtual machine...    nutanix.kib_image: Virtual machine nkp-ubuntu-22.04-1.29.6-20240717082720 created    nutanix.kib_image: Found IP for virtual machine: 10.122.7.234==&gt; nutanix.kib_image: Running post-processor: packer-manifest (type manifest)Build 'nutanix.kib_image' finished after 4 minutes 55 seconds.==&gt; Wait completed after 4 minutes 55 seconds==&gt; Builds finished. The artifacts of successful builds are:--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720</p> <p>Image name</p> <p>Note image name from the previous <code>dkp-image-builder</code> command output (the last line)</p> <pre><code>==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> </li> <li> <p>Populate the <code>.env</code> file with the NKP image name </p> Template commandSample command <pre><code>echo -e \"export NKP_IMAGE=nkp-image-name\" &gt;&gt; .env\nsource .env\n</code></pre> <pre><code>echo -e \"export NKP_IMAGE=nkp-ubuntu-22.04-1.29.6-20240717082720\" &gt;&gt; .env\nsource .env\n</code></pre> <p>Make sure to use image name that is generated in your environment.</p> </li> </ol>"},{"location":"infra/infra_dkp%20copy/#create-a-bootstrap-k8s-cluster","title":"Create a Bootstrap K8S Cluster","text":"<p>In this section we will create a bootstrap cluster which will be used to deploy the workload <code>DEV</code> cluster.</p> <ol> <li> <p>In VSC, open Terminal, enter the following command</p> <pre><code>dkp create bootstrap\n</code></pre> <p>dkp create bootstrap\u2713 Creating a bootstrap cluster \u2713 Upgrading CAPI components \u2713 Waiting for CAPI components to be upgraded \u2713 Initializing new CAPI components \u2713 Creating ClusterClass resources</p> </li> <li> <p>Store kubeconfig file for bootstrap cluster</p> <pre><code>cp $HOME/.kube/config bs.cfg\nexport KUBECONFIG=bs.cfg\n</code></pre> </li> <li> <p>Check the status of bootstrap K8S cluster</p> <pre><code>kubectl get nodes --kubeconfig=bs.cfg\n</code></pre> <p>kubectl get nodes --kubeconfig=bs.cfgNAME                                     STATUS   ROLES           AGE     VERSIONkonvoy-capi-bootstrapper-control-plane   Ready    control-plane   7m15s   v1.29.6</p> </li> </ol> <p>We are now ready to install the workload <code>DEV</code> cluster</p>"},{"location":"infra/infra_dkp%20copy/#create-nkp-workload-cluster","title":"Create NKP Workload Cluster","text":"<ol> <li> <p>In VSC, open Terminal, enter the following command to create the workload cluster</p> <pre><code>dkp create cluster nutanix -c ${NKP_CLUSTER_NAME} --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n--control-plane-prism-element-cluster ${NUTANIX_CLUSTER}  --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n--control-plane-vm-image ${NKP_IMAGE} --csi-storage-container ${STORAGE_CONTAINER} \\\n--endpoint https://${NUTANIX_ENDPOINT}:9440 --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n--worker-subnets ${NUTANIX_SUBNET_NAME} --worker-vm-image ${NKP_IMAGE} \\\n--ssh-public-key-file ${SSH_PUBLIC_KEY} --kubernetes-service-load-balancer-ip-range ${METALLB_IP_RANGE}\n</code></pre> <p>dkp create cluster nutanix -c ${NKP_CLUSTER_NAME} --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\ --control-plane-prism-element-cluster ${NUTANIX_CLUSTER}  --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\ --control-plane-vm-image ${NKP_IMAGE} --csi-storage-container ${STORAGE_CONTAINER} \\ --endpoint https://${NUTANIX_ENDPOINT}:9440 --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\ --worker-subnets ${NUTANIX_SUBNET_NAME} --worker-vm-image ${NKP_IMAGE} \\ --ssh-public-key-file ${SSH_PUBLIC_KEY} --kubernetes-service-load-balancer-ip-range ${METALLB_IP_RANGE}Generating cluster resourcescluster.cluster.x-k8s.io/nkplb createdsecret/nkplb-pc-credentials createdsecret/nkplb-pc-credentials-for-csi createdconfigmap/kommander-bootstrap-configuration createdsecret/nutanix-license created\u2713 Waiting for cluster infrastructure to be ready \u2713 Waiting for cluster control-planes to be ready \u2713 Waiting for machines to be ready</p> </li> <li> <p>Observe the events in the shell and in Prism Central events</p> </li> <li> <p>Store kubeconfig files for the workload cluster</p> <pre><code>dkp get kubeconfig -c ${NKP_CLUSTER_NAME} &gt; ${NKP_CLUSTER_NAME}.cfg\nexport KUBECONFIG=${PWD}/${NKP_CLUSTER_NAME}.cfg\n</code></pre> </li> <li> <p>Run the following command to check K8S status</p> <pre><code>kubectl get nodes\n</code></pre> <p>kubectl get nodesNAME                                  STATUS   ROLES           AGE     VERSIONnkp3-md-0-x948v-hvxtj-9r698           Ready    &lt;none&gt;          4h49m   v1.29.6nkp3-md-0-x948v-hvxtj-fb75c           Ready    &lt;none&gt;          4h50m   v1.29.6nkp3-md-0-x948v-hvxtj-mdckn           Ready    &lt;none&gt;          4h49m   v1.29.6nkp3-md-0-x948v-hvxtj-shxc8           Ready    &lt;none&gt;          4h49m   v1.29.6nkp3-r4fwl-8q4ch                      Ready    control-plane   4h50m   v1.29.6nkp3-r4fwl-jf2s8                      Ready    control-plane   4h51m   v1.29.6nkp3-r4fwl-q888c                      Ready    control-plane   4h49m   v1.29.6</p> </li> </ol>"},{"location":"infra/infra_dkp%20copy/#create-nkp-gpu-workload-pool","title":"Create NKP GPU Workload Pool","text":"<ol> <li> <p>Combine the bootstrap and workload clusters <code>KUBECONFIG</code> file so that we can use it with <code>kubectx</code></p> <pre><code>export KUBECONFIG=bs.cfg:${NKP_CLUSTER_NAME}.cfg\nkubectl config view --flatten &gt; all-in-one-kubeconfig.yaml\nexport KUBECONFIG=all-in-one-kubeconfig.yaml\n</code></pre> </li> <li> <p>Change KUBECONFIG context to use bootstrap cluster</p> <pre><code>kubectx kind-konvoy-capi-bootstrapper\n</code></pre> </li> <li> <p>Run the following command to create a GPU nodepool manifest</p> <pre><code>dkp create nodepool nutanix \\\n--cluster-name ${NKP_CLUSTER_NAME} \\\n--prism-element-cluster ${NUTANIX_CLUSTER} \\\n--subnets ${NUTANIX_SUBNET_NAME} \\\n--vm-image ${NKP_IMAGE} \\\n--disk-size 200 \\\n--memory 64 \\\n--vcpu-sockets 2 \\\n--vcpus-per-socket 8 \\\ngpu-nodepool  --dry-run -o yaml &gt; gpu-nodepool.yaml\n</code></pre> <p>Note</p> <p>Right now there is no switch for GPU in <code>dkp</code> command so we need to do dry-run into file and then add the necessary GPU specifications</p> </li> <li> <p>Add the necessary gpu section to our new <code>gpu-nodepool.yaml</code> using <code>yq</code> command</p> <pre><code>yq e '(.spec.topology.workers.machineDeployments[] | select(.name == \"gpu-nodepool\").variables.overrides[] | select(.name == \"workerConfig\").value.nutanix.machineDetails) += {\"gpus\": [{\"type\": \"name\", \"name\": strenv(GPU_NAME)}]}' -i gpu-nodepool.yaml\n</code></pre> Successful addtion of GPU specs? <p>You would be able to see the added gpu section at the end of the <code>gpu-nodepool.yaml</code> file</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\n\n&lt;snip&gt;\n\n  name: gpu-nodepool\n  variables:\n    overrides:\n      - name: workerConfig\n        value:\n          nutanix:\n            machineDetails:\n              bootType: legacy\n              cluster:\n                name: romanticism\n                type: name\n              image:\n                name: nkp-ubuntu-22.04-1.29.6-20240718055804\n                type: name\n              memorySize: 64Gi\n              subnets:\n                - name: User1\n                  type: name\n              systemDiskSize: 200Gi\n              vcpuSockets: 2\n              vcpusPerSocket: 8\n              gpus:\n                - type: name\n                  name: Lovelace 40S\n</code></pre> </li> <li> <p>Apply the <code>gpu-nodepool.yaml</code> file to the workload cluster </p> <pre><code>kubectl apply -f gpu-nodepool.yaml\n</code></pre> </li> <li> <p>Monitor the progress of the command and check Prism Central events for creation of the GPU worker node</p> </li> <li> <p>Change to workload <code>DEV</code> cluster context</p> <pre><code>kubectx ${NKP_CLUSTER_NAME}-admin@${NKP_CLUSTER_NAME}\n</code></pre> </li> <li> <p>Check nodes status in workload <code>DEV</code> cluster and note the gpu worker node</p> <pre><code>kubectl get nodes\n</code></pre> <p>kubectl get nodesNAME                                   STATUS   ROLES           AGE     VERSIONnkplb-gpu-nodepool-7g4jt-2p7l7-49wvd   Ready    &lt;none&gt;          5m57s   v1.29.6nkplb-md-0-q679c-khl2n-9k7jk           Ready    &lt;none&gt;          74m     v1.29.6nkplb-md-0-q679c-khl2n-9nk6h           Ready    &lt;none&gt;          74m     v1.29.6nkplb-md-0-q679c-khl2n-nf9p6           Ready    &lt;none&gt;          73m     v1.29.6nkplb-md-0-q679c-khl2n-qgxp9           Ready    &lt;none&gt;          74m     v1.29.6nkplb-ncnww-2dg7h                      Ready    control-plane   73m     v1.29.6nkplb-ncnww-bbm4s                      Ready    control-plane   72m     v1.29.6nkplb-ncnww-hldm9                      Ready    control-plane   75m     v1.29.6</p> </li> </ol>"},{"location":"infra/infra_dkp%20copy/#installing-kommander","title":"Installing Kommander","text":""},{"location":"infra/infra_jumphost/","title":"Manually Creating Ubuntu Linux Jumphost VM on Nutanix AHV","text":"<p>Below is walkthrough for manually creating a Linux VM on Nutanix AHV to support the various deployment scenarios.</p>"},{"location":"infra/infra_jumphost/#prerequisites","title":"Prerequisites","text":"<ul> <li>Existing Nutanix AHV Subnet configured with IPAM</li> <li>Existing Linux OS machine image (i.e., <code>Ubuntu 22.04 LTS</code> ) with cloud-init service enabled. If not existing, See Upload Generic Cloud Image into Prism Central example.</li> <li>SSH Private Key for inital <code>cloud-init</code> bootstrapping. If not existing:</li> <li>On MacOS/Linux machine, See Generate a SSH Key on Linux example.</li> <li>On Windows machine, See Generate a SSH Key on Windows example.</li> </ul>"},{"location":"infra/infra_jumphost/#jumphost-vm-requirements","title":"Jumphost VM Requirements","text":"<p>The following jumphost resrouces are recommended for the jumphost VM:</p> <ul> <li>Supported OS: <code>Ubuntu 22.04 LTS</code></li> <li>Resources:</li> <li>CPU: <code>2 vCPU</code></li> <li>Cores Per CPU: <code>4 Cores</code></li> <li>Memory: <code>16 GiB</code></li> <li>Storage: <code>300 GiB</code></li> </ul>"},{"location":"infra/infra_jumphost/#upload-generic-cloud-image-into-prism-central","title":"Upload Generic Cloud Image into Prism Central","text":"<ul> <li>Navigate to Prism Central &gt; Infrastructure &gt; Compute &amp; Storage &gt; Images</li> <li> <p>On Select Image tab, Click Add Image &gt; Select URL Button &gt; Input Image </p> <pre><code>https://cloud-images.ubuntu.com/releases/22.04/release/ubuntu-22.04-server-cloudimg-amd64.img\n</code></pre> </li> <li> <p>Click Add URL &gt; Click Next &gt; Accept Remaining Defaults </p> </li> <li>Click on Save</li> </ul>"},{"location":"infra/infra_jumphost/#generate-a-ssh-key-on-linux","title":"Generate a SSH Key on Linux","text":"<ul> <li> <p>Run the following command to generate an RSA key pair.</p> <pre><code>ssh-keygen -t rsa\n</code></pre> </li> <li> <p>Accept the default file location as <code>~/.ssh/id_rsa</code></p> </li> <li> <p>The keys will be available in the following locations:</p> <pre><code>~/.ssh/id_rsa.pub \n~/.ssh/id_rsa\n</code></pre> </li> </ul>"},{"location":"infra/infra_jumphost/#create-linux-vm-using-prism-central","title":"Create Linux VM using Prism Central","text":"<p>Run following commands from Prism Central UI.</p> <ul> <li>Select Infrastructure (via App Switcher Dropdown) &gt; Click Compute &amp; Storage &gt; Click VMs &gt; Click Create VM</li> <li>On Configuration Tab:</li> <li>Input Name (i.e., <code>nai-llm-jumphost</code>) </li> <li>Number of VMs (i.e., 1) &gt; Select Cluster &gt; Update VM Properties (See Jumphost VM Requirements) </li> <li>Click Next and enter the following details:</li> <li>CPU: <code>2 vCPU</code></li> <li>Cores Per CPU: <code>4 Cores</code></li> <li>Memory: <code>16 GiB</code></li> <li> <p>Storage:<code>300 GiB</code></p> </li> <li> <p>On the Resources Tab:, do the following:</p> </li> <li>Under Disks &gt; Attach Disk &gt; Select Clone from Image &gt; Select Image (ex. <code>ubuntu-22.04-server-cloudimg-amd64.img</code>) &gt; Update Capacity (See Jumphost VM Requirements) &gt; Click Save</li> <li>Under Networks &gt; Attach to Subnet &gt; Select Subnet &gt; Select DHCP Enabled Network &gt; Select Assign with DHCP &gt; Click Save</li> <li> <p>Select Legacy Bios Mode &gt; Click Next &gt; Accept Remaining Defaults &gt; Click Create VM</p> </li> <li> <p>On Management Tab:</p> </li> <li>Under Guest Customization &gt; Select <code>Cloud-Init (Linux)</code> on Script Type dropdown</li> <li> <p>Copy and Paste cloud-init YAML config script (example below) &gt; Accept Remaining Defaults</p> Tip <p>When copying and pasting <code>&lt;ssh-rsa-public-key&gt;</code> from a terminal, make sure not to include any new lines. Consider the following command to removes new lines: </p> <pre><code>cat ~/.ssh/id_rsa.pub | tr -d '\\n'\n</code></pre> <p>Warning</p> <p>Make sure to update the following attributes of the VM:</p> <ul> <li><code>hostname:</code></li> <li><code>&lt;ssh-rsa-public-key&gt;</code> line under <code>ssh-authorized-keys:</code> attribute</li> </ul> <pre><code>#cloud-config\nhostname: nai-llm-jumphost\npackage_update: true\npackage_upgrade: true\npackage_reboot_if_required: true\npackages:\n  - open-iscsi\n  - nfs-common\n  - linux-headers-generic\nruncmd:\n  - systemctl stop ufw &amp;&amp; systemctl disable ufw\nusers:\n  - default\n  - name: ubuntu\n    groups: sudo\n    shell: /bin/bash\n    sudo: ['ALL=(ALL) NOPASSWD:ALL']\n    ssh-authorized-keys:\n    - &lt;ssh-rsa-public-key&gt;\n</code></pre> </li> <li> <p>Click on Create VM</p> </li> </ul> <p>Once the jumphost VM has been created:</p> <ul> <li>Navigate to Prism Central &gt; Select Infrastructure &gt; Select Compute &amp; Storage </li> <li>Click on VMs</li> <li>Filter VM Name (i.e., <code>nai-llm-jumphost</code>) </li> <li>Select VM &gt; Actions &gt; Power On</li> <li> <p>Click on VM and Find IP Address in the NIC tab</p> </li> <li> <p>Validate that VM is accessible using ssh: </p> <pre><code>ssh -i ~/.ssh/id_rsa ubuntu@&lt;ip-address&gt;\n</code></pre> </li> </ul>"},{"location":"infra/infra_jumphost/#install-nai-llm-utilities","title":"Install nai-llm utilities","text":"<p>We have compiled a list of utilities that needs to be installed on the jumphost VM to use for the rest of the lab. We have affectionately called it as <code>nai-llm</code> utilities. Use the following method to install these utilities:</p> <ol> <li> <p>SSH into Linux VM  </p> <pre><code>ssh -i ~/.ssh/id_rsa ubuntu@&lt;ip-address&gt;\n</code></pre> </li> <li> <p>Clone Git repo and change working directory</p> <pre><code>git clone https://github.com/nutanix-japan/sol-cnai-infra\ncd $HOME/sol-cnai-infra/\n</code></pre> </li> <li> <p>Run Post VM Create - Workstation Bootstrapping Tasks</p> <pre><code>sudo snap install task --classic\ntask ws:install-packages ws:load-dotfiles --yes -d $HOME/sol-cnai-infra/\nsource ~/.bashrc\n</code></pre> </li> <li> <p>Change working directory and see <code>Task</code> help</p> <pre><code>cd $HOME/sol-cnai-infra/ &amp;&amp; task\n</code></pre> <pre><code>$ cd $HOME/sol-cnai-infra/ &amp;&amp; task\n# command output\ntask: bootstrap:silent\n\nSilently initializes cluster configs, git local/remote &amp; fluxcd\n\nSee README.md for additional details on Getting Started\n\nTo see list of tasks, run `task --list` or `task --list-all`\n\ndependencies:\n- bootstrap:default\n\ncommands:\n- Task: bootstrap:generate_local_configs\n- Task: bootstrap:verify-configs\n- Task: bootstrap:generate_cluster_configs\n- Task: nke:download-creds \n- Task: flux:init\n</code></pre> </li> </ol>"},{"location":"infra/infra_jumphost_tofu/","title":"Deploy Jumphost","text":"<p>We will go through three phases in this section to deploy jumphost VM which you will use to deploy AI applications.</p> <ol> <li>Create Cloud-Init: needed to bootstrap JumpHost VM on Nutanix AHV using OpenTofu</li> <li>Create Jumphost VM: needed to remotely connect and run deployment workflows accessible to Nutanix Infrastructure.</li> <li>Deploy Nutanix AI Utils: needed to bootstrap, monitor and troubleshoot Nutanix Cloud-Native AI applications using Gitops across fleet of Nutanix Kubernetes Clusters.</li> </ol> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployJumpHost {\n        [*] --&gt; CreateCloudInit\n        CreateCloudInit --&gt; CreateJumpHostVM\n        CreateJumpHostVM --&gt; DeployNaiUtils\n        DeployNaiUtils --&gt; [*]\n    }\n\n    PrepWorkstation --&gt; DeployJumpHost \n    DeployJumpHost --&gt; DeployNkp : Next Section (Option A)\n    DeployJumpHost --&gt; DeployNke : Next Section (Option B)</code></pre>"},{"location":"infra/infra_jumphost_tofu/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenTofu cli, see installation instructions here.</li> <li>Existing Nutanix AHV Subnet configured with IPAM</li> <li>SSH Public Key needed for initial <code>cloud-init</code> bootstrapping<ul> <li>On MacOS/Linux machine, see Generate a SSH Key on Linux example.</li> <li>On Windows machine, see Generate a SSH Key on Windows example.</li> </ul> </li> </ul>"},{"location":"infra/infra_jumphost_tofu/#jump-host-vm-requirements","title":"Jump Host VM Requirements","text":"<p>Based on the Nutanix GPT-in-a-Box specifications, the following system resources are required for the <code>Jump Host</code> VM:</p> <ul> <li>Target OS: <code>Ubuntu 24.04 LTS</code></li> </ul> <p>Minimum System Requirements:</p> CPU Cores Per CPU Memory Storage 2 vCPU 4 Cores 16 GiB 300 GiB"},{"location":"infra/infra_jumphost_tofu/#create-jump-host-vm","title":"Create Jump Host VM","text":"<p>In the following section, we will create a <code>Jump Host</code> VM on Nutanix AHV using both <code>Visual Studio Code (VSCode)</code> and <code>OpenTofu</code>.</p> <ol> <li> <p>Open <code>VSCode</code>, Go to File -&gt; New Window , Click on Open Folder  and create new workspace folder (i.e., <code>tofu-workspace</code>).</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, Click on New Folder  and name it: <code>jumphost-vm</code></p> </li> <li> <p>In the <code>jumphost-vm</code> folder, click on New File  with the following name</p> <pre><code>cloud-init.yaml\n</code></pre> </li> <li> <p>Paste the following contents inside the file:</p> cloud-init.yaml<pre><code>#cloud-config\nhostname: nai-llm-jumphost\npackage_update: true\npackage_upgrade: true\npackage_reboot_if_required: true\npackages:\n  - open-iscsi\n  - nfs-common\nruncmd:\n  - systemctl stop ufw &amp;&amp; systemctl disable ufw\nusers:\n  - default\n  - name: ubuntu\n    groups: sudo\n    shell: /bin/bash\n    sudo:\n      - 'ALL=(ALL) NOPASSWD:ALL'\n    ssh-authorized-keys: \n    - ssh-rsa XXXXXX.... # (1)    \n</code></pre> <ol> <li> <p> Copy and paste the contents of your <code>~/.ssh/id_rsa.pub</code> file or any public key file that you wish to use.</p> <p>If you are using a Mac, the command <code>pbcopy</code>can be used to copy the contents of a file to clipboard.</p> <pre><code>cat ~/.ssh/id_rsa.pub | tr -d '\\n' | pbcopy\n</code></pre> <p>Cmd+v will paste the contents of clipboard to the console.</p> </li> </ol> <p>Warning</p> <p>If needed, make sure to update the target <code>hostname</code> and copy / paste the value of the RSA public key in the <code>cloudinit.yaml</code> file.</p> </li> <li> <p>In <code>VSCode</code> Explorer, within the <code>jumphost-vm</code> folder, click on New File  and create a config file with the following name:</p> <pre><code>jumphostvm_config.yaml\n</code></pre> <p>Update Nutanix environment access details along with any Jump Host VM configurations. See example file for details</p> Template fileExample file jumphostvm_config.yaml<pre><code>endpoint: \"PC FQDN\"\nuser: \"PC user\"                  \npassword: \"PC password\"          \ncluster_name: \"PE Cluster Name\"  \nsubnet_name: \"PE subnet\"  \nname: \"nai-llm-jumphost\"\nnum_vcpus_per_socket: \"4\"\nnum_sockets: \"2\"\nmemory_size_mib: 16384\ndisk_size_mib: 307200\nsource_uri: \"https://cloud-images.ubuntu.com/releases/24.04/release/ubuntu-24.04-server-cloudimg-amd64.img\"\n</code></pre> jumphostvm_config.yaml<pre><code>endpoint: \"pc.example.com\"    # &lt; Change to PC endpoint &gt;\nuser: \"user01\"                # &lt; Change to PC admin user&gt; \npassword: \"XXXXXXXX\"          # &lt; Change to PC admin pass&gt;\ncluster_name: \"mypecluster\"   # &lt; Change to PE element cluster name &gt;\nsubnet_name: \"VLAN.20\"        # &lt; Change to PE element subnet name &gt;\nname: \"nai-llm-jumphost\" # (1)!\nnum_vcpus_per_socket: \"4\"\nnum_sockets: \"2\"\nmemory_size_mib: 16384\ndisk_size_mib: 307200\nsource_uri: \"https://cloud-images.ubuntu.com/releases/24.04/release/ubuntu-24.04-server-cloudimg-amd64.img\"\n</code></pre> <ol> <li> make sure to update <code>hostname</code> with same name defined within <code>cloudinit.yaml</code>.</li> </ol> <p>Tip</p> <p>If you are using a Mac and <code>pbcopy</code> utility as suggested in the previous command's tip window, Cmd+v will paste the contents of clipboard to the console.</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, navigate to the <code>jumphost-vm</code> folder, click on New File  and create a opentofu manifest file with the following name:</p> <pre><code>jumphostvm.tf\n</code></pre> <p>with the following content:</p> jumphostvm.tf<pre><code>terraform {\n  required_providers {\n    nutanix = {\n      source  = \"nutanix/nutanix\"\n      version = \"1.9.5\"\n    }\n  }\n}\n\nlocals {\n  config = yamldecode(file(\"${path.module}/jumphostvm_config.yaml\"))\n}\n\ndata \"nutanix_cluster\" \"cluster\" {\n  name = local.config.cluster_name\n}\ndata \"nutanix_subnet\" \"subnet\" {\n  subnet_name = local.config.subnet_name\n}\n\nprovider \"nutanix\" {\n  username     = local.config.user\n  password     = local.config.password\n  endpoint     = local.config.endpoint\n  insecure     = true\n  wait_timeout = 60\n}\n\nresource \"nutanix_image\" \"machine-image\" {\n  name        = element(split(\"/\", local.config.source_uri), length(split(\"/\", local.config.source_uri)) - 1)\n  description = \"opentofu managed image\"\n  source_uri  = local.config.source_uri\n}\n\nresource \"nutanix_virtual_machine\" \"nai-llm-jumphost\" {\n  name                 = local.config.name\n  cluster_uuid         = data.nutanix_cluster.cluster.id\n  num_vcpus_per_socket = local.config.num_vcpus_per_socket\n  num_sockets          = local.config.num_sockets\n  memory_size_mib      = local.config.memory_size_mib\n  guest_customization_cloud_init_user_data = base64encode(file(\"${path.module}/cloud-init.yaml\"))\n  disk_list {\n    data_source_reference = {\n      kind = \"image\"\n      uuid = nutanix_image.machine-image.id\n    }\n    disk_size_mib = local.config.disk_size_mib\n  }\n  nic_list {\n    subnet_uuid = data.nutanix_subnet.subnet.id\n  }\n\n  depends_on = [nutanix_image.machine-image]\n}\n\noutput \"nai-llm-jumphost-ip-address\" {\n  value = nutanix_virtual_machine.nai-llm-jumphost.nic_list_status[0].ip_endpoint_list[0].ip\n  description = \"IP address of the Jump Host vm\"\n}\n</code></pre> </li> <li> <p>Open a terminal within <code>VSCode</code>, Terminal &gt; New Terminal </p> </li> <li> <p>Initialize and Validate your tofu code</p> <pre><code>tofu -chdir=tofu-workspace/jumphost-vm init -upgrade\n</code></pre> <pre><code>tofu -chdir=tofu-workspace/jumphost-vm validate\n</code></pre> </li> <li> <p>Apply your tofu code to create Jump Host VM</p> <pre><code>tofu -chdir=tofu-workspace/jumphost-vm apply \n</code></pre> <p>Type <code>yes</code> to confirm</p> </li> <li> <p>Obtain the IP address of the <code>Jump Host</code> VM from the Tofu output</p> <pre><code>Outputs:\n\nnai-llm-jumphost-ip-address = \"10.x.x.x\"\n</code></pre> </li> <li> <p>Run the Terraform state list command to verify what resources have been created</p> <pre><code>tofu state list\n</code></pre> <pre><code># Sample output for the above command\n\ndata.nutanix_cluster.cluster              # &lt; This is your existing Prism Element cluster\ndata.nutanix_subnet.subnet                # &lt; This is your existing primary subnet\nnutanix_image.machine-image               # &lt; This is the image file for `Jump Host` VM\nnutanix_virtual_machine.nai-llm-jumphost  # &lt; This is the `Jump Host` VM\n</code></pre> </li> <li> <p>Validate that the <code>Jump Host</code> VM is accessible using VSCode &gt; Terminal </p> CommandCommand Sample <pre><code>ssh -i ~/.ssh/id_rsa ubuntu@&lt;ip-address-from-tofu-output&gt;\n</code></pre> <pre><code>ssh -i ~/.ssh/id_rsa ubuntu@10.x.x.171\n</code></pre> </li> </ol>"},{"location":"infra/infra_jumphost_tofu/#initiate-remote-ssh-connection-to-jumpbox-using-vscode","title":"Initiate Remote-SSH Connection to Jumpbox using VSCode","text":"<ol> <li>In VSCode, click on Settings menu icon (gear icon)  &gt; Settings &gt; Extensions</li> <li>In the search window search for Remote SSH</li> <li>Install the Remote-SSH Extension from VSCode Marketplace</li> <li> <p>click on the Install button for the extenstion.</p> </li> <li> <p>From your workstation, open Visual Studio Code.</p> </li> <li> <p>Click View &gt; Command Palette.</p> <p></p> </li> <li> <p>Click on + Add New SSH Host</p> <p></p> </li> <li> <p>Type <code>ssh ubuntu@jumphost_VM-IP-ADDRESS</code>and hit Enter.</p> <p></p> </li> <li> <p>Select the location to update the config file.</p> Mac/LinuxWindows <pre><code>/Users/&lt;your-username&gt;/.ssh/config\n</code></pre> <pre><code>C:\\\\Users\\\\&lt;your-username&gt;\\\\.ssh\\\\config\n</code></pre> </li> <li> <p>Open the ssh config file on your workstation to verify the contents. It should be similar to the following content</p> <pre><code>Host jumphost\n    HostName 10.x.x.x # (1)!\n    IdentityFile ~/.ssh/id_rsa # (2)!\n    User ubuntu\n</code></pre> <ol> <li> <p> This is Jumphost VM IP address</p> </li> <li> <p> This would be the path to RSA private key generated in the previous JumpHost section</p> </li> </ol> <p>Now that we have saved the ssh credentials, we are able to connect to the jumphost VM</p> </li> </ol>"},{"location":"infra/infra_jumphost_tofu/#connect-to-you-jumpbox-using-vscode","title":"Connect to you Jumpbox using VSCode","text":"<ol> <li> <p>On <code>VSCode</code>, Click View &gt; Command Palette and Connect to Host</p> </li> <li> <p>Select the IP address of your <code>Jump Host</code> VM</p> </li> <li> <p>A New Window  will open in <code>VSCode</code></p> </li> <li> <p>Click the Explorer button from the left-hand toolbar and select Open Folder.</p> <p></p> </li> <li> <p>Provide the <code>$HOME/</code> as the folder you want to open and click on OK.</p> <p>Note</p> <p>Ensure that bin is NOT highlighted otherwise the editor will attempt to autofill <code>/bin/</code>. You can avoid this by clicking in the path field before clicking OK.</p> <p>Warning</p> <p>The connection may take up to 1 minute to display the root folder structure of the jumphost VM.</p> </li> <li> <p>Accept any warning message about trusting the author of the folder</p> <p></p> </li> </ol>"},{"location":"infra/infra_jumphost_tofu/#install-utilities-on-jumphost-vm","title":"Install Utilities on Jumphost VM","text":"<p>We have compiled a list of utilities that needs to be installed on the jumphost VM to use for the rest of the lab. We have affectionately called it as <code>nai-llm</code> utilities. Use the following method to install these utilities:</p> <ol> <li> <p>Using <code>VSCode</code>, open <code>Terminal</code>  on the <code>Jump Host</code> VM</p> </li> <li> <p>Install <code>devbox</code> using the following command and accept all defaults</p> <pre><code>curl -fsSL https://get.jetpack.io/devbox | bash\n</code></pre> </li> <li> <p>From the <code>$HOME</code> directory, clone the <code>sol-cnai-infra</code> git repo and change working directory</p> <pre><code>git clone https://github.com/nutanix-japan/sol-cnai-infra.git\ncd $HOME/sol-cnai-infra/\n</code></pre> </li> <li> <p>Start the <code>devbox shell</code>. If <code>nix</code> isn't available, you will be prompted to install:</p> <pre><code>devbox init\ndevbox shell\n</code></pre> </li> <li> <p>Run Post VM Create - Workstation Bootstrapping tasks</p> <pre><code>sudo snap install task --classic\ntask ws:install-packages ws:load-dotfiles --yes -d $HOME/sol-cnai-infra/\nsource ~/.bashrc\n</code></pre> </li> <li> <p>Change working directory and see <code>Task</code> help</p> <pre><code>cd $HOME/sol-cnai-infra/ &amp;&amp; task\n</code></pre> <pre><code># command output\ntask: bootstrap:silent\n\nSilently initializes cluster configs, git local/remote &amp; fluxcd\n\nSee README.md for additional details on Getting Started\n\nTo see list of tasks, run `task --list` or `task --list-all`\n\ndependencies:\n- bootstrap:default\n\ncommands:\n- Task: bootstrap:generate_local_configs\n- Task: bootstrap:verify-configs\n- Task: bootstrap:generate_cluster_configs\n- Task: nke:download-creds \n- Task: flux:init\n</code></pre> </li> </ol>"},{"location":"infra/infra_jumphost_tofu/#setup-docker-on-jumphost","title":"Setup Docker on Jumphost","text":"<ol> <li>From VSC, logon to your jumpbox VM</li> <li>Open VSC Terminal</li> <li> <p>Run the following commands to install <code>docker</code> binaries</p> <pre><code>cd $HOME/sol-cnai-infra/; devbox init; devbox shell\ntask workstation:install-docker\n</code></pre> <p>Tip</p> <p>Restart the jumpbox host if <code>ubuntu</code> user has permission issues using <code>docker</code> commands.</p> </li> </ol> <p>Now the jumphost VM is ready with all the tools to deploy other sections on this site. </p>"},{"location":"infra/infra_nke/","title":"Deploy NKE Clusters","text":"<p>This section will take you through install NKE(Kubernetes) on Nutanix cluster as we will be deploying AI applications on these kubernetes clusters. </p> <p>This section will expand to other available Kubernetes implementations on Nutanix.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKE {\n        [*] --&gt; CreateTofuWorkspaces\n        CreateTofuWorkspaces --&gt; CreateMgtK8SCluster\n        CreateMgtK8SCluster --&gt; CreateDevK8SCluster\n        CreateDevK8SCluster --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n\n    PrepWorkstation --&gt; DeployJumpHost \n    DeployJumpHost --&gt; DeployNKE\n    DeployNKE --&gt; DeployGiabGitOps : Next section</code></pre>"},{"location":"infra/infra_nke/#nke-setup","title":"NKE Setup","text":"<p>We will use Infrastructure as Code framework to deploy NKE kubernetes clusters. </p>"},{"location":"infra/infra_nke/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Prism Central is at least version <code>2023.4</code></li> <li>NKE is enabled on Nutanix Prism Central</li> <li>NKE is at at least version <code>2.10</code> (updated through LCM)</li> <li>NKE Node OS is at least version <code>ntnx-1.7</code></li> <li>NKE Kubernetes is at least version <code>1.26.11-0</code></li> <li>Monitoring on each NKE Cluster is ENABLED</li> </ul>"},{"location":"infra/infra_nke/#nke-high-level-cluster-design","title":"NKE High Level Cluster Design","text":"<p>The <code>Management</code> NKE cluster will be a centralized cluster that the AI applications on each Workload NKE cluster will be leveraged for automation and observability.</p> <p>The <code>Workload</code> NKE cluster will be hosting the LLM model serving endpoints and AI application stack. This cluster and will require a dedicated GPU node pool.  </p> <p>We will create a 1 x NKE cluster for Management and at min. 1 x NKE cluster for the DEV Workloads.</p> <p>Once DEV deployment has been tested successfully, we can deploy applications to optional PROD Workload cluster.</p>"},{"location":"infra/infra_nke/#management-cluster","title":"Management Cluster","text":"<p>Since the Management Cluster will be essential to all AI application workloads, we will deploy an NKE cluster of type \"Production\".</p> Role No. of Nodes (VM) vCPU RAM Storage Master 2 8 16 GB 120 GB ETCD 3 4 8 GB 120 GB Worker 3 12 16 GB 300 GB"},{"location":"infra/infra_nke/#dev-workload-cluster","title":"Dev Workload Cluster","text":"<p>For Dev, we will deploy an NKE Cluster of type \"Development\".</p> Role No. of Nodes (VM) vCPU RAM Storage Master 1 8 16 GB 120 GB ETCD 1 4 8 GB 120 GB Worker 3 12 16 GB 300 GB GPU 2 12 40 GB 300 GB"},{"location":"infra/infra_nke/#prod-workload-cluster","title":"Prod Workload Cluster","text":"<p>For Prod, we will deploy an NKE Cluster of type \"Production\".</p> Role No. of Nodes (VM) vCPU RAM Storage Master 2 8 16 GB 120 GB ETCD 3 4 8 GB 120 GB Worker 3 12 16 GB 300 GB GPU 2 12 40 GB 300 GB"},{"location":"infra/infra_nke/#create-tofu-manifest-file","title":"Create TOFU Manifest file","text":"<ol> <li> <p>If you haven't already done so, Open new <code>VSCode</code> window</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, Click on existing <code>tofu-workspace</code> Folder </p> </li> <li> <p>Click on New Folder  name it: <code>nke-tofu</code></p> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>tofu-workspace/nke-tofu</code> folder, click on New File  and create a tofu manifest file with following name:</p> <pre><code>main.tf\n</code></pre> <p>with the following content:</p> main.tf<pre><code>terraform {\n  required_providers {\n    nutanix = {\n      source  = \"nutanix/nutanix\"\n      version = \"1.9.5\"\n    }\n  }\n}\n\nlocals {\n  config = yamldecode(file(\"${path.module}/.env.${terraform.workspace}.yaml\"))\n}\n\ndata \"nutanix_cluster\" \"cluster\" {\n  name = local.config.prism_element.cluster_name\n}\n\ndata \"nutanix_subnet\" \"subnet\" {\n  subnet_name = local.config.prism_element.subnet_name\n}\n\nprovider \"nutanix\" {\n  username     = local.config.prism_central.user\n  password     = local.config.prism_central.password\n  endpoint     = local.config.prism_central.endpoint\n  insecure     = false\n  wait_timeout = 60\n}\n\nresource \"nutanix_karbon_cluster\" \"nke_cluster\" {\n  name       = terraform.workspace\n  version    = local.config.nke.k8s_version\n  storage_class_config {\n    reclaim_policy = \"Delete\"\n    volumes_config {\n      file_system                = \"ext4\"\n      flash_mode                 = false\n      prism_element_cluster_uuid = data.nutanix_cluster.cluster.id\n      storage_container          = local.config.prism_element.storage_container\n      username                   = local.config.prism_element.user\n      password                   = local.config.prism_element.password\n    }\n  }\n\n  cni_config {\n    node_cidr_mask_size = 24\n    pod_ipv4_cidr       = \"172.20.0.0/16\"\n    service_ipv4_cidr   = \"172.19.0.0/16\"\n  }\n\n  worker_node_pool {\n    node_os_version = local.config.nke.node_os_version \n    num_instances   = local.config.nke.worker.num_instances\n    ahv_config {\n      cpu = local.config.nke.worker.cpu_count\n      memory_mib = local.config.nke.worker.memory_gb * 1024\n      disk_mib = local.config.nke.worker.disk_gb * 1024\n      network_uuid               = data.nutanix_subnet.subnet.id\n      prism_element_cluster_uuid = data.nutanix_cluster.cluster.id\n    }\n  }\n\n  etcd_node_pool {\n    node_os_version = local.config.nke.node_os_version \n    num_instances   = local.config.nke.etcd.num_instances\n    ahv_config {\n      cpu = local.config.nke.etcd.cpu_count\n      memory_mib = local.config.nke.etcd.memory_gb * 1024\n      disk_mib = local.config.nke.etcd.disk_gb * 1024\n      network_uuid               = data.nutanix_subnet.subnet.id\n      prism_element_cluster_uuid = data.nutanix_cluster.cluster.id\n    }\n  }\n\n  master_node_pool {\n    node_os_version = local.config.nke.node_os_version \n    num_instances   = local.config.nke.master.num_instances\n    ahv_config {\n      cpu = local.config.nke.master.cpu_count\n      memory_mib = local.config.nke.master.memory_gb * 1024\n      disk_mib = local.config.nke.master.disk_gb * 1024\n      network_uuid               = data.nutanix_subnet.subnet.id\n      prism_element_cluster_uuid = data.nutanix_cluster.cluster.id\n    }\n  }\n\n  timeouts {\n    create = \"1h\"\n    update = \"30m\"\n    delete = \"10m\"\n  }\n\n}\n</code></pre> </li> </ol>"},{"location":"infra/infra_nke/#deploying-management-cluster","title":"Deploying Management Cluster","text":"<ol> <li> <p>In <code>VSCode</code> Terminal, change working directory to the <code>nke-tofu</code> folder</p> <pre><code>cd tofu-workspace/nke-tofu/\n</code></pre> </li> <li> <p>In <code>VSCode</code> Terminal, Create Tofu <code>workspace</code> for target NKE Management Cluster</p> <pre><code>tofu workspace select -or-create mgmt-cluster\n</code></pre> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>tofu-workspace/nke-tofu</code> folder, click on New File  and create a environment config file for target Management Cluster with following name:</p> <pre><code>.env.mgmt-cluster.yaml\n</code></pre> <p>Update Nutanix environment access details along with any NKE specific configurations. See example file for details</p> Management Cluster Config fileManagement Cluster Example file .env.mgmt-cluster.yaml<pre><code>prism_central:\n  endpoint: &lt;PC FQDN&gt;\n  user: &lt;PC admin user&gt;\n  password: &lt;PC admin password&gt;\n\nprism_element:\n  cluster_name: &lt;PE Cluster Name&gt;\n  storage_container: default\n  subnet_name: &lt;PE Subnet&gt;\n  user: &lt;PE admin user&gt;\n  password: &lt;PE admin password&gt;\n\nnke:\n  k8s_version: 1.26.11-0\n  node_os_version: ntnx-1.7\n  master:\n    num_instances: 1\n    cpu_count: 8\n    memory_gb: 16\n    disk_gb: 300\n  etcd:\n    num_instances: 1\n    cpu_count: 4\n    memory_gb: 8\n    disk_gb: 300\n  worker:\n    num_instances: 3\n    cpu_count: 12\n    memory_gb: 16\n    disk_gb: 300\n</code></pre> .env.mgmt-cluster.yaml<pre><code>prism_central:\n  endpoint: \"pc.example.com\"    # &lt; Change to PC endpoint &gt;\n  user: \"admin\"                 # &lt; Change to PC admin user&gt; \n  password: \"XXXXXXXX\"          # &lt; Change to PC admin pass&gt;\n\nprism_element:\n  cluster_name: \"mypecluster\"   # &lt; Change to PE element cluster name &gt;\n  storage_container: default    # &lt; Change to PE element cluster storage container name &gt;\n  subnet_name: \"VLAN.20\"        # &lt; Change to PE element subnet name &gt;\n  user: \"admin\"                 # &lt; Change to PE admin user&gt; \n  password: \"XXXXXXXX\"          # &lt; Change to PE admin pass&gt; \n\nnke:\n  k8s_version: 1.26.11-0\n  node_os_version: ntnx-1.7\n  master:\n    num_instances: 1\n    cpu_count: 8\n    memory_gb: 16\n    disk_gb: 300\n  etcd:\n    num_instances: 1\n    cpu_count: 4\n    memory_gb: 8\n    disk_gb: 300\n  worker:\n    num_instances: 3\n    cpu_count: 12\n    memory_gb: 16\n    disk_gb: 300\n</code></pre> </li> <li> <p>Initialize and Validate your tofu code</p> <pre><code>tofu -chdir=tofu-workspace/nke-tofu init -upgrade\n\n# OpenTofu will initialize the Nutanix provider\n</code></pre> <pre><code>tofu -chdir=tofu-workspace/nke-tofu validate\n\n# OpenTofu will validate configurations\n</code></pre> </li> <li> <p>Apply your tofu code to create NKE cluster, associated virtual machines and other resources</p> <pre><code>tofu -chdir=tofu-workspace/nke-tofu apply \n\n# OpenTofu will show you all resources that it will to create\n# Type yes to confirm \n</code></pre> </li> <li> <p>Run the OpenTofu state list command to verify what resources have been created</p> <pre><code>tofu -chdir=tofu-workspace/nke-tofu state list\n</code></pre> <pre><code># Sample output for the above command\n\ndata.nutanix_cluster.cluster              # &lt; This is your existing Prism Element cluster\ndata.nutanix_subnet.subnet                # &lt; This is your existing primary subnet\nnutanix_image.jumphost-image              # &lt; This is the image file for jump host VM\nnutanix_virtual_machine.nai-llm-jumphost  # &lt; This is the jump host VM\nnutanix_karbon_cluster.mgt_cluster        # &lt; This is your Management NKE cluster\n</code></pre> </li> </ol>"},{"location":"infra/infra_nke/#deploying-dev-cluster","title":"Deploying DEV cluster","text":"<p>The DEV cluster will contain GPU node pool to deploy your AI apps.</p> <ol> <li> <p>Create TOFU workspace for DEV NKE Cluster</p> <pre><code>tofu workspace select -or-create dev-cluster\n</code></pre> </li> <li> <p>Create the Management NKE cluster config.yaml </p> Tip <p>The previous <code>.env.mgmt-cluster.yaml</code> could be copied </p> <pre><code>cp .env.mgmt-cluster.yaml .env.dev-cluster.yaml\n</code></pre> <pre><code>.env.dev-cluster.yaml\n</code></pre> <p>with the following content:</p> .env.dev-cluster.yaml<pre><code>prism_central:\n  endpoint: &lt;PC FQDN&gt;\n  user: &lt;PC user&gt;\n  password: &lt;PC password&gt;\n\nprism_element:\n  cluster_name: &lt;PE Cluster Name&gt;\n  storage_container: default\n  subnet_name: &lt;PE Subnet&gt;\n  user: &lt;PE user&gt;\n  password: &lt;PE password&gt;\n\nnke:\n  k8s_version: 1.26.11-0\n  node_os_version: ntnx-1.7\n  master:\n    num_instances: 1\n    cpu_count: 8\n    memory_gb: 16\n    disk_gb: 300\n  etcd:\n    num_instances: 1\n    cpu_count: 4\n    memory_gb: 8\n    disk_gb: 300\n  worker:\n    num_instances: 3\n    cpu_count: 12\n    memory_gb: 16\n    disk_gb: 300\n</code></pre> </li> <li> <p>Validate your tofu code</p> <pre><code>tofu validate\n\n# OpenTofu will validate configurations\n</code></pre> </li> <li> <p>Apply your tofu code to create NKE cluster, associated virtual machines and other resources</p> <pre><code>tofu apply\n\n# OpenTofu will show you all resources that it will to create\n# Type yes to confirm \n</code></pre> </li> <li> <p>Run the Terraform state list command to verify what resources have been created</p> <pre><code>tofu state list\n</code></pre> <pre><code># Sample output for the above command\n\ndata.nutanix_cluster.cluster              # &lt; This is your existing Prism Element cluster\ndata.nutanix_subnet.subnet                # &lt; This is your existing primary subnet\nnutanix_image.jumphost-image              # &lt; This is the image file for jump host VM\nnutanix_virtual_machine.nai-llm-jumphost  # &lt; This is the jump host VM\nnutanix_karbon_cluster.mgt-cluster        # &lt; This is your Management NKE cluster\nnutanix_karbon_cluster.dev-cluster        # &lt; This is your Dev NKE cluster\n</code></pre> </li> </ol>"},{"location":"infra/infra_nke/#adding-nodepool-with-gpu","title":"Adding NodePool with GPU","text":"<p>In this section we will create a nodepool to host the AI apps with a GPU. </p> <p>Note</p> <p>At this time there is no <code>tofu</code> support for creating a <code>nodepool</code> with GPU parameters. We will use NKE's <code>karbonctl</code> tool. Once tofu nodepool resource is updated with gpu parameters, we will update this section.</p> <p>It is necessary to connect to Prism Central (PC) to be able to access the <code>karbonctl</code> tool.</p> <ol> <li> <p>Login to the ssh session of PC</p> <pre><code>ssh -l admin pc.example.com\n</code></pre> </li> <li> <p>Login to NKE control plane using karbonctl tool</p> <pre><code>alias karbonctl=/home/nutanix/karbon/karbonctl\nkarbonctl login --pc-username admin\n</code></pre> </li> <li> <p>Check the number of available GPUs for Dev NKE cluster</p> <pre><code>karbonctl cluster gpu-inventory list --cluster-name dev-cluster\n</code></pre> Command execution<pre><code>PCVM:~$ karbonctl cluster gpu-inventory list --cluster-name dev-cluster\nName            Total Count    Assignable Count\nLovelace 40S    8              2\n</code></pre> </li> <li> <p>Create a new gpu nodepool and assing it 1 GPU</p> <pre><code>karbonctl cluster node-pool add --cluster-name dev-cluster --count 2 --cpu 12 --memory 40 --disk-size 300 --gpu-count 1 --gpu-name \"Lovelace 40S\" --node-pool-name gpu\n</code></pre> Command execution<pre><code>PCVM:~$ karbonctl cluster node-pool add --cluster-name dev-cluster --count 2 --cpu 12 --memory 40 ---disk-size 300 -gpu-count 1 --gpu-name \"Lovelace 40S\" --node-pool-name gpu\n\nI acknowledge that GPU enablement requires installation of NVIDIA datacenter driver software governed by NVIDIA licensing terms. Y/[N]:Y\n\nSuccessfully submitted request to add a node pool: [POST /karbon/v1-alpha.1/k8s/clusters/{name}/add-node-pool][202] addK8sClusterNodePoolAccepted  &amp;{TaskUUID:0xc001168e50}\n</code></pre> </li> <li> <p>Monitor PC tasks to confirm creation on VM and allocation of GPU to the VM</p> </li> <li> <p>Once nodepool is created, go to PC &gt; Kubernetes Management &gt; dev-cluster &gt; Node Pools and select gpu nodepool</p> </li> <li> <p>Click on update in the drop-down menu</p> </li> <li> <p>You should see that one GPU is assigned to node pool</p> <p></p> </li> </ol> <p>We now have a node that can be used to deploy AI applications and use the GPU.</p>"},{"location":"infra/infra_nkp/","title":"Deploy NKP Clusters","text":"<p>This section will take you through install NKP(Kubernetes) on Nutanix cluster as we will be deploying AI applications on these kubernetes clusters.</p> <p>We will use the CAPI based deployment of NKP. This will automatically deploy the required infrastructure VMs for the cluster by connecting to Nutanix Cluster APIs. There is no requirement to use Terraform or or other IaC tools to deploy NKP.</p> <p>This section will expand to other available Kubernetes implementations on Nutanix.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKP {\n        [*] --&gt; CreateNkpMachineImage\n        CreateNkpMachineImage --&gt; CreateNKPCluster\n        CreateNKPCluster --&gt; GenerateLicense\n        GenerateLicense --&gt; InstallLicense\n        InstallLicense --&gt; DeployGpuNodePool\n        DeployGpuNodePool --&gt; EnableGpuOperator\n        EnableGpuOperator --&gt; [*]\n    }\n\n    PrepWorkstation --&gt; DeployJumpHost \n    DeployJumpHost --&gt; DeployNKP \n    DeployNKP --&gt; DeployNai : Next section</code></pre> <p>Deploying NKP Cluster</p> <p>This lab will focus on deploying NKP to host NAI workloads. However, the steps can also be used deploy a custom NKP deployment if that's the aim.</p> <p>We will only deploy 1 (one) NKP cluster and have the management and workload part in this. For production environments, it is advised to have separate NKP clusters for management and workloads.</p> <p>Consider using NKP The Hard Way section to create a customized version of your NKP cluster with separate NKP clusters for management and workloads.</p> <p>Once you have determined the resource requirements for a custom NKP deployment, modify the environment variables and values in the <code>.env</code> file to suit your resource needs for your NKP cluster.</p>"},{"location":"infra/infra_nkp/#nkp-high-level-cluster-design","title":"NKP High Level Cluster Design","text":"<p>The <code>nkpdev</code> cluster will be hosting the LLM model serving endpoints and AI application stack. This cluster and will require a dedicated GPU node pool.</p>"},{"location":"infra/infra_nkp/#sizing-requirements","title":"Sizing Requirements","text":"<p>Below are the sizing requirements needed to successfully deploy NAI on a NKP Cluster (labeled as <code>nkpdev</code>) and subsequently deploying single LLM inferencing endpoint on NAI using the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> LLM model.</p> Calculating GPU Resources Tips <p>The calculations below assume that you're already aware of how much memory is required to load target LLM model.</p> <p>For a general example:</p> <ul> <li>To host a 8b(illion) parameter model, multiply the parameter number by 2 to get minimum GPU memory requirments.    e.g. 16GB of GPU memory is required for 8b parameter model.</li> </ul> <p>So in the case of the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> model, you'll need a min. 16 GiB GPU vRAM available</p> <p>Below are additional sizing consideration \"Rule of Thumb\" for further calculating min. GPU node resources:</p> <ul> <li>For each GPU node will have 8 CPU cores, 24 GB of memory, and 300 GB of disk space.</li> <li>For each GPU attached to the node, add 16 GiB of memory.</li> <li>For each endpoint attached to the node, add 8 CPU cores.</li> <li>If a model needs multiple GPUs, ensure all GPUs are attached to the same worker node</li> <li>For resiliency, while running multiple instances of the same endpoint, ensure that the GPUs are on different worker nodes.</li> </ul> <p>Since we will be testing with the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> HuggingFace model, we will require a GPU with a min. of 24 GiB GPU vRAM available to support this demo.</p> <p>Note</p> <p>GPU min. vRAM should be 24 GB, such as NVIDIA L4 Model.</p> <p>Below are minimum requirements for deploying NAI on the NKP Demo Cluster.</p> Role No. of Nodes (VM) vCPU per Node Memory per Node Storage per Node Total vCPU Total Memory Control plane 3 4 16 GB 150 GB 12 48 GB Worker 4 8 32 GB 150 GB 32 128 GB GPU 1 16 40 GB 300 GB 16 40 GB Totals 60 216 GB"},{"location":"infra/infra_nkp/#pre-requisites-for-nkp-deployment","title":"Pre-requisites for NKP Deployment","text":"<ol> <li>Existing Ubuntu Linux jumphost VM. See here for jumphost installation steps.</li> <li>Docker or Podman installed on the jumphost VM</li> <li>Nutanix PC is at least <code>2024.3</code></li> <li>Nutanix AOS is at least <code>6.8+</code>, <code>6.10</code></li> <li>Download and install <code>nkp</code> binary from Nutanix Portal</li> <li>Find and reserve 3 IPs for control plane and MetalLB access from AHV network</li> <li>Find GPU details from Nutanix cluster</li> <li>Create a base image to use with NKP nodes using <code>nkp</code> command</li> </ol>"},{"location":"infra/infra_nkp/#install-nkp-binaries","title":"Install NKP Binaries","text":"<ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Downloads &gt; Nutanix Kubernetes Platform (NKP)</li> <li>Select NKP for Linux and copy the download link to the <code>.tar.gz</code> file</li> <li> <p>If you haven't already done so, Open new <code>VSCode</code> window on your jumphost VM</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, click on existing <code>$HOME</code> folder</p> </li> <li> <p>Click on New Folder  name it: <code>nkp</code></p> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>$HOME/nkp</code> folder</p> </li> <li> <p>On <code>VSCode</code> menu, select <code>Terminal</code> &gt; <code>New Terminal</code></p> </li> <li> <p>Browse to <code>nkp</code> directory</p> <pre><code>cd $HOME/nkp\n</code></pre> </li> <li> <p>Download and extract the NKP binary from the link you copied earlier</p> CommandSample command Paste the download URL within double quotes<pre><code>curl -o nkp_v2.16.0_linux_amd64.tar.gz \"_paste_download_URL_here\"\n</code></pre> <pre><code>curl -o nkp_v2.16.0_linux_amd64.tar.gz \"https://download.nutanix.com/downloads/nkp/v2.16.0/nkp_v2.16.0_linux_amd64.tar.gz?Expires=1729016864&amp;........\"\n</code></pre> <pre><code>tar xvfz nkp_v2.16.0_linux_amd64.tar\n</code></pre> </li> <li> <p>Move the <code>nkp</code> binary to a directory that is included in your <code>PATH</code> environment variable</p> <pre><code>sudo cp nkp /usr/local/bin/\n</code></pre> </li> <li> <p>Verify the <code>nkp</code> binary is installed correctly. Ensure the version is latest</p> <p>Note</p> <p>At the time of writing this lab nkp version is <code>v2.16.0</code></p> CommandCommand Output <pre><code>nkp version\n</code></pre> <pre><code>$ nkp version\ncatalog: v0.7.0\ndiagnose: v0.12.0\nimagebuilder: v2.16.0\nkommander: v2.16.0\nkonvoy: v2.16.0\nmindthegap: v1.22.1\nnkp: v2.16.0\n</code></pre> </li> </ol>"},{"location":"infra/infra_nkp/#setup-docker-on-jumphost","title":"Setup Docker on Jumphost","text":"<p>If not already done, follow the steps in Setup Docker on Jumphost section. </p> <ol> <li> <p>Login to docker on the jumphost</p> CommandCommand Output <pre><code>docker login\n</code></pre> <pre><code>~$ docker login\n\nUSING WEB-BASED LOGIN\nTo sign in with credentials on the command line, use 'docker login -u &lt;username&gt;'\n\nYour one-time device confirmation code is: JXXX-VXXX\nPress ENTER to open your browser or submit your device code here: https://login.docker.com/activate\n\nWaiting for authentication in the browser\u2026\nWARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credential-stores\n</code></pre> </li> <li> <p>Open the docker login URL in a browser after you enter the <code>docker login</code> command.</p> </li> </ol>"},{"location":"infra/infra_nkp/#reserve-control-plane-and-metallb-ip","title":"Reserve Control Plane and MetalLB IP","text":"<p>Nutanix AHV IPAM network allows you to black list IPs that needs to be reserved for specific application endpoints. We will use this feature to find and reserve three IPs. </p> <p>We will reserve a total of three IPs for the following:</p> Cluster Role Cluster Name NKP NAI Dev <code>nkpdev</code> 2 1 <ol> <li> <p>Get the CIDR range for the AHV network(subnet) where the application will be deployed</p> CIDR example for your Nutanix cluster<pre><code>10.x.x.0/24\n</code></pre> </li> <li> <p>From VSC, logon to your jumpbox VM and open Terminal</p> </li> <li> <p>Install <code>nmap</code> tool (if not already done)</p> <pre><code>cd $HOME/sol-cnai-infra\ndevbox add nmap\n</code></pre> </li> <li> <p>Find three unused static IP addresses in the subnet</p> CommandSample command <pre><code>nmap -v -sn  &lt;your CIDR&gt;\n</code></pre> <pre><code>nmap -v -sn 10.x.x.0/24\n</code></pre> Sample output - choose the first three consecutive IPs<pre><code>Nmap scan report for 10.x.x.214 [host down]\nNmap scan report for 10.x.x.215 [host down]\nNmap scan report for 10.x.x.216 [host down]\nNmap scan report for 10.x.x.217\nHost is up (-0.098s latency).\n</code></pre> </li> <li> <p>Logon to any CVM in your Nutanix cluster and execute the following to add chosen static IPs to the AHV IPAM network</p> <ul> <li>Username: nutanix</li> <li>Password: your Prism Element password </li> </ul> CommandSample command <pre><code>acli net.add_to_ip_blacklist &lt;your-ipam-ahv-network&gt; \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> <pre><code>acli net.add_to_ip_blacklist User1 \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> </li> </ol>"},{"location":"infra/infra_nkp/#reservation-of-ips","title":"Reservation of IPs","text":"<p>Reserve the firs IPs for NKP control plane  Reserve the second two IPs for MetalLB distributed load balancer - We will use one of these IP for NAI</p> <p>Reserve the third IP for NAI. We will use the NAI IP in the next NAI section to assign the FDQN and install SSL certificate.</p> Component IP FQDN NKP Control Plane VIP <code>10.x.x.214</code> - NKP MetalLB IP Range <code>10.x.x.215-10.x.x.216</code> - NAI <code>10.x.x.216</code> <code>nai.10.x.x.216.nip.io</code>"},{"location":"infra/infra_nkp/#create-base-image-for-nkp","title":"Create Base Image for NKP","text":"<p>About NKP Base Image OS Version on Nutanix Cluster</p> <p>The base image for NKP is a minimal image that contains the required packages and tools to run the Kubernetes cluster. The base image is used to create the worker node VMs and the control plane VMs.</p> <p>NKP base image can be <code>Rocky Linux 9.4</code> image and is part of <code>NKP Starter</code> license. This image is maintained and supported by Nutanix. The image is updated regularly to include the latest security patches and bug fixes. Customers should not modify the base image. </p> <p>Using <code>NKP Pro</code> license also offers choice of using <code>Ubuntu 22.04</code> base image for GPU based workload deployments.</p> <p>In this section we will go through creating a base image for all the control plane and worker node VMs on Nutanix. We will use the <code>Ubuntu 22.04</code> image as the base image as we will need GPU support for AI applications. NVIDIA GPU drivers are not yet available for <code>Rocky Linux 9.4</code> base image. </p> <p>NKP Cloud Support</p> <p>For information about other supported operating systems for Nutanix Kubernetes Platform (NKP), see NKP Cloud Support.</p> <ol> <li> <p>In VSC Explorer pane, Click on New Folder </p> </li> <li> <p>Call the folder <code>nkp</code> under <code>$HOME</code> directory</p> </li> <li> <p>In the <code>nkp</code> folder, click on New File  and create new file with the following name:</p> <pre><code>.env\n</code></pre> </li> <li> <p>Run the following command to generate an new RSA key pair on the jumphost VM. This SSH key pair will be used for authentication between the jumphost and NKP K8S cluster nodes.</p> Do you have existing SSH key pair? <p>Copy the key pair from your workstation (PC/Mac) to <code>~/.ssh/</code> directory on your Jumphost VM.</p> <pre><code>mac/pc $ scp ~/.ssh/id_rsa.pub ubuntu@10.x.x.171:~/.ssh/id_rsa.pub\nmac/pc $ scp ~/.ssh/id_rsa ubuntu@10.x.x.171:~/.ssh/id_rsa\n</code></pre> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>Accept the default file location as <code>~/.ssh/id_rsa</code></p> <p>SSH key pair will stored in the following location:</p> <pre><code>~/.ssh/id_rsa.pub \n~/.ssh/id_rsa\n</code></pre> </li> <li> <p>Fill the following values inside the <code>.env</code> file</p> Template .envSample .env <pre><code>export NUTANIX_USER=_your_nutanix_username\nexport NUTANIX_PASSWORD=_your_nutanix_password\nexport NUTANIX_ENDPOINT=_your_prism_central_fqdn\nexport NUTANIX_CLUSTER=_your_prism_element_cluster_name\nexport NUTANIX_SUBNET_NAME=_your_ahv_ipam_network_name\nexport STORAGE_CONTAINER=_your_storage_container_nmae\nexport SSH_PUBLIC_KEY=_path_to_ssh_pub_key_on_jumphost_vm\nexport NKP_CLUSTER_NAME=_your_nkp_cluster_name\nexport CONTROLPLANE_VIP=_your_nkp_cluster_controlplane_ip\nexport LB_IP_RANGE=_your_range_of_two_ips\nexport DOCKER_USERNAME=_your_docker_username\nexport DOCKER_PASSWORD=_your_docker_password\n</code></pre> <pre><code>export NUTANIX_USER=admin\nexport NUTANIX_PASSWORD=xxxxxxxx\nexport NUTANIX_ENDPOINT=pc.example.com\nexport NUTANIX_CLUSTER=pe\nexport NUTANIX_SUBNET_NAME=User1\nexport STORAGE_CONTAINER=default\nexport SSH_PUBLIC_KEY=$HOME/.ssh/id_rsa.pub\nexport NKP_CLUSTER_NAME=nkpdev\nexport CONTROLPLANE_VIP=10.x.x.214\nexport LB_IP_RANGE=10.x.x.215-10.x.x.216\nexport DOCKER_USERNAME=_your_docker_username\nexport DOCKER_PASSWORD=_your_docker_password_pat\n</code></pre> </li> <li> <p>Using VSC Terminal, load the environment variables and its values</p> <pre><code>source $HOME/nkp/.env\n</code></pre> </li> <li> <p>Create the base image and upload to Prism Central using the following command. </p> <p>Note</p> <p>Image creation will take up to 5 minutes.</p> CommandCommand Output <pre><code>nkp create image nutanix ubuntu-22.04 \\\n  --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} \\\n  --subnet ${NUTANIX_SUBNET_NAME} --insecure\n</code></pre> <pre><code>nkp create image nutanix ubuntu-22.04 \\ \n--endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} \\\n--subnet ${NUTANIX_SUBNET_NAME} --insecure\n\n&gt; Provisioning and configuring image\nManifest files extracted to $HOME/nkp/.nkp-image-builder-3243021807\nnutanix.kib_image: output will be in this color.\n\n==&gt; nutanix.kib_image: Creating Packer Builder virtual machine...\n    nutanix.kib_image: Virtual machine nkp-ubuntu-22.04-1.29.6-20240717082720 created\n    nutanix.kib_image: Found IP for virtual machine: 10.x.x.234\n==&gt; nutanix.kib_image: Running post-processor: packer-manifest (type manifest)\n\n---&gt; 100%\nBuild 'nutanix.kib_image' finished after 4 minutes 55 seconds.\n==&gt; Wait completed after 4 minutes 55 seconds\n\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> <p>Image name - This will be different in your environment</p> <p>Note image name from the previous <code>nkp</code> create image command output</p> <pre><code>==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> <p>Warning</p> <p>Make sure to use image name that is generated in your environment for the next steps.</p> </li> <li> <p>Populate the <code>.env</code> file with the NKP image name by adding (appending) the following environment variables and save it</p> Template .envSample .env <pre><code>export NKP_IMAGE=nkp-image-name\n</code></pre> <pre><code>export NKP_IMAGE=nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> </li> </ol> <p>We are now ready to install the workload <code>nkpdev</code> cluster</p>"},{"location":"infra/infra_nkp/#create-nkp-workload-cluster","title":"Create NKP Workload Cluster","text":"<p>Warning</p> <p>Do not use hyphens <code>-</code> in the nkp cluster name. </p> Clustername Validation Rules<pre><code>a lowercase RFC 1123 subdomain must consist of lower case alphanumeric       \u2502\n\u2502characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com',  \u2502\n\u2502regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')\n</code></pre> <p>Note</p> <p>In this lab the workload cluster will have the Management cluster role as well to reduce resource consumption in a lab environment. </p> <p>However, for production environments, the ideal design is to have a separate management and workload clusters. </p> <ol> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export CONTROL_PLANE_REPLICAS=_no_of_control_plane_replicas\nexport CONTROL_PLANE_VCPUS=_no_of_control_plane_vcpus\nexport CONTROL_PLANE_CORES_PER_VCPU=_no_of_control_plane_cores_per_vcpu\nexport CONTROL_PLANE_MEMORY_GIB=_no_of_control_plane_memory_gib\nexport WORKER_REPLICAS=_no_of_worker_replicas\nexport WORKER_VCPUS=_no_of_worker_vcpus\nexport WORKER_CORES_PER_VCPU=_no_of_worker_cores_per_vcpu\nexport WORKER_MEMORY_GIB=_no_of_worker_memory_gib\nexport CSI_FILESYSTEM=_preferred_filesystem_ext4/xfs\nexport CSI_HYPERVISOR_ATTACHED=_true/false\nexport DOCKER_USERNAME=_your_docker_username\nexport DOCKER_PASSWORD=_your_docker_password\n</code></pre> <pre><code>export CONTROL_PLANE_REPLICAS=3\nexport CONTROL_PLANE_VCPUS=4\nexport CONTROL_PLANE_CORES_PER_VCPU=1\nexport CONTROL_PLANE_MEMORY_GIB=16\nexport WORKER_REPLICAS=4\nexport WORKER_VCPUS=8 \nexport WORKER_CORES_PER_VCPU=1\nexport WORKER_MEMORY_GIB=32\nexport CSI_FILESYSTEM=ext4\nexport CSI_HYPERVISOR_ATTACHED=true\nexport DOCKER_USERNAME=_your_docker_username\nexport DOCKER_PASSWORD=_your_docker_password\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source $HOME/nkp/.env\n</code></pre> </li> <li> <p>In VSC, open Terminal, enter the following command to create the workload cluster</p> Check your command for correct argument values <p>Run the following command to verify your <code>nkp</code> command and associated environment variables and values.</p> <pre><code>echo \"nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n        --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n        --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n        --control-plane-vm-image ${NKP_IMAGE} \\\n        --csi-storage-container ${STORAGE_CONTAINER} \\\n        --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n        --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n        --worker-vm-image ${NKP_IMAGE} \\\n        --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n        --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n        --control-plane-disk-size 150 \\\n        --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} \\\n        --control-plane-vcpus ${CONTROL_PLANE_VCPUS} \\\n        --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n        --worker-disk-size 150 \\\n        --worker-memory ${WORKER_MEMORY_GIB} \\\n        --worker-vcpus ${WORKER_VCPUS} \\\n        --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n        --csi-file-system ${CSI_FILESYSTEM} \\\n        --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n        --registry-mirror-url \"https://registry-1.docker.io\" \\\n        --registry-mirror-username ${DOCKER_USERNAME} \\\n        --registry-mirror-password ${DOCKER_PASSWORD} \\\n        --self-managed \\\n        --insecure\"\n</code></pre> <p>If the values are incorrect, add the correct values to <code>.env</code> and source the  again by running the following command</p> <pre><code>source $HOME/nkp/.env\n</code></pre> <p>Then rerun the <code>echo nkp</code> command to verify the values again before running the <code>nkp create cluster nutanix</code> command.</p> CommandCommand Output <pre><code>nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n    --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n    --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n    --control-plane-vm-image ${NKP_IMAGE} \\\n    --csi-storage-container ${STORAGE_CONTAINER} \\\n    --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n    --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n    --worker-vm-image ${NKP_IMAGE} \\\n    --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n    --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n    --control-plane-disk-size 150 \\\n    --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} \\\n    --control-plane-vcpus ${CONTROL_PLANE_VCPUS} \\\n    --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n    --worker-disk-size 150 \\\n    --worker-memory ${WORKER_MEMORY_GIB} \\\n    --worker-vcpus ${WORKER_VCPUS} \\\n    --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n    --csi-file-system ${CSI_FILESYSTEM} \\\n    --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n    --registry-mirror-url \"https://registry-1.docker.io\" \\\n    --registry-mirror-username ${DOCKER_USERNAME} \\\n    --registry-mirror-password ${DOCKER_PASSWORD} \\\n    --self-managed \\\n    --insecure\n</code></pre> <pre><code>&gt; \u2713 Creating a bootstrap cluster \n\u2713 Upgrading CAPI components \n\u2713 Waiting for CAPI components to be upgraded \n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Creating ClusterClass resources\n&gt; Generating cluster resources\ncluster.cluster.x-k8s.io/nkpdev created\nsecret/nkpdev-pc-credentials created\nsecret/nkpdev-pc-credentials-for-csi created\nsecret/nkpdev-image-registry-credentials created\n\u2713 Waiting for cluster infrastructure to be ready \n\u2713 Waiting for cluster control-planes to be ready \n\u2713 Waiting for machines to be ready\n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Moving cluster resources\n\n&gt; You can now view resources in the moved cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdev.conf\" get nodes\n\n&gt; \u2713 Deleting bootstrap cluster \n\nCluster default/nkpdev kubeconfig was written to to the filesystem.\nYou can now view resources in the new cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdev.conf\" get nodes\n\n&gt; Starting kommander installation\n\u2713 Deploying Flux \n\u2713 Deploying Ingress certificate \n\u2713 Creating kommander-overrides ConfigMap\n\u2713 Deploying Git Operator \n\u2713 Creating GitClaim for management GitRepository \n\u2713 Creating GitClaimUser for accessing management GitRepository \n\u2713 Creating HTTP Proxy configuration\n\u2713 Deploying Flux configuration\n\u2713 Deploying Kommander Operator \n\u2713 Creating KommanderCore resource \n\u2713 Cleaning up kommander bootstrap resources\n\u2713 Deploying Substitution variables\n\u2713 Deploying Flux configuration \n\u2713 Deploying Gatekeeper \n\u2713 Deploying Kommander AppManagement \n\u2713 Creating Core AppDeployments \n\u2713 4 out of 12 core applications have been installed (waiting for dex, dex-k8s-authenticator and 6 more) \n\u2713 5 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 5 more) \n\u2713 7 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 3 more) \n\u2713 8 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 2 more) \n\u2713 9 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 1 more) \n\u2713 10 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, traefik-forward-auth-mgmt) \n\u2713 11 out of 12 core applications have been installed (waiting for traefik-forward-auth-mgmt) \n\u2713 Creating cluster-admin credentials\n\n&gt; Cluster was created successfully! Get the dashboard details with:\n&gt; nkp get dashboard --kubeconfig=\"$HOME/nkp/nkpdev.conf\"\n</code></pre> <p>What is a Self-Manged Cluster?</p> <p>The <code>--self-managed</code> argument of the <code>nkp create cluster nutanix</code> command will deploy bootstrap, and Kommander management automatically. </p> <p>The appendix section has information on how to deploy a cluster without using the <code>--self-managed</code> option. </p> <p>Usually preferred by customer DevOps teams to have more control over the deployment process. This way the customer can do the following:</p> <ul> <li>Deploy bootstrap (<code>Kind</code>) cluster</li> <li>Deploy NKP Management cluster</li> <li>Choose to migrate the CAPI components over to NKP Management cluster</li> <li>Choose to customize Kommander Managment component instllation</li> <li>Choose to deploy workload clusters from NKP Kommander GUI or</li> <li>Choose to deploy workload clusters using scripts to automate the process</li> </ul> <p>See NKP the Hard Way section for more information for customizable NKP cluster deployments. </p> </li> <li> <p>Observe the events in the shell and in Prism Central events</p> </li> <li> <p>Store kubeconfig file for <code>nkpdev</code> cluster</p> <pre><code>export KUBECONFIG=$HOME/nkp/nkpdev.conf\n</code></pre> </li> <li> <p>Confirm the access to <code>nkpdev</code> cluster</p> CommandCommand Output <pre><code>kubectl get nodes\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                  STATUS   ROLES           AGE     VERSION\nnkpdev-md-0-x948v-hvxtj-9r698           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-fb75c           Ready    &lt;none&gt;          4h50m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-mdckn           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-shxc8           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-r4fwl-8q4ch                      Ready    control-plane   4h50m   v1.29.6\nnkpdev-r4fwl-jf2s8                      Ready    control-plane   4h51m   v1.29.6\nnkpdev-r4fwl-q888c                      Ready    control-plane   4h49m   v1.29.6\n</code></pre> </li> </ol>"},{"location":"infra/infra_nkp/#licensing","title":"Licensing","text":"<p>We need to generate a license for the NKP cluster which is the total for all the vCPUs used by worker nodes.</p> <p>For example, in the Sizing Requirements section, the NKP Demo Cluster <code>Total vCPU count</code> is equal to <code>60</code>, whereas the actual worker nodes total vCPU count is only <code>48</code>.</p>"},{"location":"infra/infra_nkp/#generate-nkp-pro-license","title":"Generate NKP Pro License","text":"<p>To generate a NKP Pro License for the NKP cluster:</p> <p>Note</p> <p>Nutanix Internal users should logon using Nutanix SSO</p> <p>Nutanix Partners/Customers should logon to Portal using their Nutanix Portal account credentials</p> <ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Licensing &gt; License Summary</li> <li>Click on the small drop down arrow  on Manage Licenses and choose Nutanix Kubernetes Platform (NKP)</li> <li>Input the NKP cluster name</li> <li>Click on the plus icon </li> <li>Click on Next in the bottom right corner</li> <li>Select NKP Pro License</li> <li>Select Apply to cluster</li> <li>Choose Non-production license and Save</li> <li>Select the cluster name and click on Next</li> <li>Input the number of vCPU (<code>60</code>) from our calculations in the previous section</li> <li>Click on Save</li> <li>Download the csv file and store it in a safe place</li> </ol>"},{"location":"infra/infra_nkp/#applying-nkp-pro-license-to-nkp-cluster","title":"Applying NKP Pro License to NKP Cluster","text":"<ol> <li> <p>Login to the Kommander URL for <code>nkpdev</code> cluster with the generated credentials that was generated in the previous section. The following commands will give you the credentials and URL.</p> CommandCommand Output <pre><code>nkp get dashboard\n</code></pre> <pre><code>nkp get dashboard\n\nUsername: recursing_xxxxxxxxx\nPassword: YHbPsslIDB7p7rqwnfxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nURL: https://10.x.x.215/dkp/kommander/dashboard\n</code></pre> </li> <li> <p>Go to Licensing and click on Remove License to remove the Starter license</p> </li> <li>Type nutanix-license in the confirmation box and click on Remove License</li> <li>Click on Add License, choose Nutanix platform and paste the license key from the previous section</li> <li>Click on Save</li> <li>Confirm the license is applied to the cluster by cheking the License Status in the License menu</li> <li>The license will be applied to the cluster and the license status will reflect NKP Pro in the top right corner of the dashboard</li> </ol>"},{"location":"infra/infra_nkp/#add-nkp-gpu-workload-pool","title":"Add NKP GPU Workload Pool","text":"<p>CPU only LLM Hosting</p> <p>Enabling GPU and NVIDIA GPU Operator in NKP is not essential for NAI <code>v2.3</code></p> <p>NAI <code>v2.3</code> can host LLM up to 7 billion parameters on CPU</p> <p>Warning</p> <p>Skip this section if no GPUs are required</p> <p>The steps below covers the following:     - Retrieving and Applying NKP Pro License     - Identifying the GPU device name     - Deploying the GPU nodepool     - Enabling the NVIDIA GPU Operator</p> <p>Note</p> <p>To Enable the GPU Operator afterwards using the NKP Marketplace, a minimal NKP Pro license is required.</p>"},{"location":"infra/infra_nkp/#find-gpu-device-details","title":"Find GPU Device Details","text":"<p>As we will be deploying Nutanix Enterprise AI (NAI) in the next section, we need to find the GPU details beforehand.</p> <p>Find the details of GPU on the Nutanix cluster while still connected to Prism Central (PC).</p> <ol> <li>Logon to Prism Central GUI</li> <li>On the general search, type GPUs</li> <li> <p>Click on the GPUs result</p> <p></p> </li> <li> <p><code>Lovelace 40s</code> is the GPU available for use</p> </li> <li>Use <code>Lovelace 40s</code> in the evironment variables in the next section.</li> </ol>"},{"location":"infra/infra_nkp/#create-nkp-gpu-workload-pool","title":"Create NKP GPU Workload Pool","text":"<p>In this section we will create a nodepool to host the AI apps with a GPU.</p> <ol> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export GPU_NAME=_name_of_gpu_device_\nexport GPU_REPLICA_COUNT=_no_of_gpu_worker_nodes\nexport GPU_POOL=_name_of_gpu_pool\nexport GPU_NODE_VCPUS=_no_of_gpu_node_vcpus\nexport GPU_NODE_CORES_PER_VCPU=_per_gpu_node_cores_per_vcpu\nexport GPU_NODE_MEMORY_GIB=_per_gpu_node_memory_gib\nexport GPU_NODE_DISK_SIZE_GIB=_per_gpu_node_memory_gib\n</code></pre> <pre><code>export GPU_NAME=\"Lovelace 40S\"\nexport GPU_REPLICA_COUNT=1\nexport GPU_POOL=gpu-nodepool\nexport GPU_NODE_VCPUS=16\nexport GPU_NODE_CORES_PER_VCPU=1\nexport GPU_NODE_MEMORY_GIB=40\nexport GPU_NODE_DISK_SIZE_GIB=200\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source $HOME/nkp/.env\n</code></pre> </li> <li> <p>Run the following command to create a GPU nodepool manifest</p> <pre><code>nkp create nodepool nutanix \\\n    --cluster-name ${NKP_CLUSTER_NAME} \\\n    --prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --subnets ${NUTANIX_SUBNET_NAME} \\\n    --vm-image ${NKP_IMAGE} \\\n    --disk-size ${GPU_NODE_DISK_SIZE_GIB} \\\n    --memory ${GPU_NODE_MEMORY_GIB} \\\n    --vcpus ${GPU_NODE_VCPUS} \\\n    --cores-per-vcpu ${GPU_NODE_CORES_PER_VCPU} \\\n    --replicas ${GPU_REPLICA_COUNT} \\\n    --wait \\\n    ${GPU_POOL} --dry-run -o yaml &gt; gpu-nodepool.yaml\n</code></pre> <p>Note</p> <p>Right now there is no switch for GPU in <code>nkp</code> command. We need to do dry-run the output into a file and then add the necessary GPU specifications</p> </li> <li> <p>Add the necessary gpu section to our new <code>gpu-nodepool.yaml</code> using <code>yq</code> command</p> <pre><code>yq e '(.spec.topology.workers.machineDeployments[] | select(.name == \"gpu-nodepool\").variables.overrides[] | select(.name == \"workerConfig\").value.nutanix.machineDetails) += {\"gpus\": [{\"type\": \"name\", \"name\": strenv(GPU_NAME)}]}' -i gpu-nodepool.yaml\n</code></pre> Successful addtion of GPU specs? <p>You would be able to see the added gpu section at the end of the <code>gpu-nodepool.yaml</code> file</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\n\n&lt;snip&gt;\n\n  name: gpu-nodepool\n  variables:\n    overrides:\n      - name: workerConfig\n        value:\n          nutanix:\n            machineDetails:\n              bootType: legacy\n              cluster:\n                name: romanticism\n                type: name\n              image:\n                name: nkp-ubuntu-22.04-1.29.6-20240718055804\n                type: name\n              memorySize: 40Gi\n              subnets:\n                - name: User1\n                  type: name\n              systemDiskSize: 200Gi\n              vcpuSockets: 16\n              vcpusPerSocket: 1\n              gpus:\n                - type: name\n                  name: Lovelace 40S\n</code></pre> </li> <li> <p>Monitor Cluster-Api resources (on a different shell) to ensure gpu machine will be successfully created</p> <pre><code>watch kubectl get cluster-api\n</code></pre> </li> <li> <p>Apply the <code>gpu-nodepool.yaml</code> file to the workload cluster</p> <pre><code>kubectl apply -f gpu-nodepool.yaml\n</code></pre> </li> <li> <p>Monitor the progress of the command and check Prism Central events for creation of the GPU worker node</p> </li> <li> <p>Check nodes status in workload <code>nkpdev</code> cluster and note the gpu worker node</p> CommandCommand Output <pre><code>kubectl get nodes -w\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                   STATUS   ROLES           AGE     VERSION\nnkpdev-gpu-nodepool-7g4jt-2p7l7-49wvd   Ready    &lt;none&gt;          5m57s   v1.29.6\nnkpdev-md-0-q679c-khl2n-9k7jk           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-md-0-q679c-khl2n-9nk6h           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-md-0-q679c-khl2n-nf9p6           Ready    &lt;none&gt;          73m     v1.29.6\nnkpdev-md-0-q679c-khl2n-qgxp9           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-ncnww-2dg7h                      Ready    control-plane   73m     v1.29.6\nnkpdev-ncnww-bbm4s                      Ready    control-plane   72m     v1.29.6\nnkpdev-ncnww-hldm9                      Ready    control-plane   75m     v1.29.6\n</code></pre> </li> </ol>"},{"location":"infra/infra_nkp/#enable-gpu-operator","title":"Enable GPU Operator","text":"<p>We will need to enable GPU operator for deploying NKP application. </p> <ol> <li>In the NKP GUI, Go to Clusters</li> <li>Click on Kommander Host</li> <li>Go to Applications </li> <li>Search for NVIDIA GPU Operator</li> <li>Click on Enable</li> <li>Click on Configuration tab</li> <li> <p>Click on Workspace Application Configuration Override and paste the following yaml content</p> <pre><code>driver:\n  enabled: true\n</code></pre> <p>As shown here:</p> <p></p> </li> <li> <p>Click on Enable on the top right-hand corner to enable GPU driver on the Ubuntu GPU nodes</p> </li> <li> <p>Check GPU operator resources and make sure they are running</p> CommandCommand Output <pre><code>kubectl get po -A | grep -i nvidia\n</code></pre> <pre><code>kubectl get po -A | grep -i nvidia\n\nnvidia-container-toolkit-daemonset-fjzbt                          1/1     Running     0          28m\nnvidia-cuda-validator-f5dpt                                       0/1     Completed   0          26m\nnvidia-dcgm-exporter-9f77d                                        1/1     Running     0          28m\nnvidia-dcgm-szqnx                                                 1/1     Running     0          28m\nnvidia-device-plugin-daemonset-gzpdq                              1/1     Running     0          28m\nnvidia-driver-daemonset-dzf55                                     1/1     Running     0          28m\nnvidia-operator-validator-w48ms                                   1/1     Running     0          28m\n</code></pre> </li> <li> <p>Run a sample GPU workload to confirm GPU operations</p> CommandCommand Output <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre> <pre><code>pod/cuda-vector-add created\n</code></pre> </li> <li> <p>Follow the logs to check if the GPU operations are successful</p> CommandSample CommandCommand Output <pre><code>kubectl logs _gpu_worload_pod_name\n</code></pre> <pre><code>kubectl logs cuda-vector-add-xxx\n</code></pre> <pre><code>kubectl logs cuda-vector-add\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\n</code></pre> </li> </ol> <p>Now we are ready to deploy our AI workloads.</p>"},{"location":"infra/workstation/","title":"Workstation Setup","text":"<p>We will be going through creating resources and installing tools on your PC/Mac. </p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state PrepWorkstation {\n        [*] --&gt; GenrateRSAKeys\n        GenrateRSAKeys --&gt; InstallTofu\n        InstallTofu --&gt; InstallVSCode\n        InstallVSCode --&gt; [*]\n    }\n\n    [*] --&gt; PrepWorkstation \n    PrepWorkstation --&gt; DeployJumpHost : Next section</code></pre> <p>Note</p> <p>These are the only binaries that you will need to install on your workstation or any other OS to get the jump host VM running.</p> <ol> <li>Create RSA key pair We will need a RSA key pair to connect to the jumphost</li> <li>Install Visual Studio Code (VSC)</li> <li>Install OpenTofu on Linux</li> </ol>"},{"location":"infra/workstation/#generate-a-rsa-key-pair","title":"Generate a RSA Key Pair","text":"<p>Based on your local workstation OS, follow these instuctions to:</p> Mac/LinuxWindows <ol> <li> <p>Run the following command to generate an RSA key pair.</p> <pre><code>ssh-keygen -t rsa\n</code></pre> </li> <li> <p>[Optional] Modify or Accept the default file location as <code>~/.ssh/id_rsa</code></p> </li> <li> <p>If the default file location was accepted, the keys will be in the following location:</p> <pre><code>~/.ssh/id_rsa.pub \n~/.ssh/id_rsa\n</code></pre> </li> </ol> <p>On Windows machine, See Generate a RSA key pair on Windows example.</p>"},{"location":"infra/workstation/#install-opentofu","title":"Install OpenTofu","text":"<p>OpenTofu is a fork of Terraform that is open-source, community-driven, and managed by the Linux Foundation and is used to simplify provisioning resources using the Nutanix Terraform Provider while following Infrastructure as Code (IaC) practices.</p> <p>To Install OpenTofu, follow the steps below for your respective local workstation:</p> Mac/LinuxWindows <p>Download the installer script:<pre><code>curl --proto '=https' --tlsv1.2 -fsSL https://get.opentofu.org/install-opentofu.sh -o install-opentofu.sh\n</code></pre> Give it execution permissions:<pre><code>chmod +x install-opentofu.sh\n</code></pre> Run the installer:<pre><code>./install-opentofu.sh --install-method standalone\n</code></pre> Remove the installer:<pre><code>rm -f install-opentofu.sh\n</code></pre></p> <p>Download the installer script:<pre><code>Invoke-WebRequest -outfile \"install-opentofu.ps1\" -uri \"https://get.opentofu.org/install-opentofu.ps1\"\n</code></pre> Run the installer<pre><code>&amp; .\\install-opentofu.ps1 -installMethod standalone\n</code></pre> Remove the installer<pre><code>Remove-Item install-opentofu.ps1\n</code></pre></p>"},{"location":"infra/workstation/#install-visual-studio-code-vscode","title":"Install Visual Studio Code (VSCode)","text":"<p>We will be doing all the labs by connecting to your jump host using <code>VSCode</code> remote shell environment. This allows for easy browsing and editing of configuration files. For additional details, see Visual Studio Code</p> <p>Having a rich text editor capable of integrating with the rest of our tools, and providing markup to the different source code file types will provide significant value in upcoming exercises and is a much simpler experience for most users compared to command line text editors.</p> <p>If you don't already have <code>VSCode</code> installed on your local, you can either Download and Install VSCode or choose to follow one of the steps belows:</p> Mac/LinuxWindows <pre><code>brew install --cask visual-studio-code # (1)\n</code></pre> <ol> <li> <p>  If you do not have <code>brew</code> macOS package manager installed, use the following command to install it in <code>Terminal</code>. For additional details, see HomeBrew Installation Docs.</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> </li> </ol> <pre><code>choco install vscode # (1)\n</code></pre> <ol> <li> <p>  If you do not have <code>choco</code> Windows package manager installed, use the following command to install it in <code>PowerShell</code>. For additional details, see Chocolatey Installation Docs.</p> <pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> </li> </ol> <p>The steps above leverage HomeBrew Package Manager for macOS and Chocolatey Package Manager for Windows respectively to install <code>VSCode</code>, click on links to see additional details.</p> <p>We will proceed to deploying jumphost VM.</p>"},{"location":"llmmgmt/","title":"Getting Started","text":"<p>Deprecation Notice</p> <p>GPT-in-a-Box V1 NVD will be deprecated and be removed in a future releases. This is replaced by Nutanix Enterprise AI (AI).</p> <p>This part of the lab we will focus on deploying LLM on GPU nodes using the GPT-in-a-Box V1 NVD.</p> <p>We will deploy two Kubernetes clusters so far as per the NVD design requirements</p> <ol> <li>Management cluster: to host the management workloads like flux, kafka, etc</li> <li>Dev cluster: to host the dev LLM and ChatBot application - this will use GPU passed through to the kubernetes worker nodes </li> </ol> <p>We will deploy the following applications one after the other</p> <ol> <li>GPT-in-a-Box v1 NVD Reference App - backed by llama2 model with RAG in Milvus database</li> <li>Support GPT</li> </ol> <p>The following is the flow of the applications lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKE {\n        [*] --&gt; CreateTofuWorkspaces\n        CreateTofuWorkspaces --&gt; CreateMgtK8SCluster\n        CreateMgtK8SCluster --&gt; CreateDevK8SCluster\n        CreateDevK8SCluster --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n\n    state NAIPreRequisites {\n        [*] --&gt; ReserveIPs\n        ReserveIPs --&gt; CreateBuckets\n        CreateBuckets --&gt; CreateFilesShare\n        CreateFilesShare --&gt; [*]\n    }\n\n    state DeployLLMV1 {\n        [*] --&gt; BootStrapMgmtCluster\n        BootStrapMgmtCluster --&gt;  BootStrapDevCluster\n        BootStrapDevCluster --&gt; MonitorResourcesDeployment\n        MonitorResourcesDeployment --&gt; [*]\n    }\n\n    state TestLLMApp {\n        [*] --&gt; TestQueryLLM\n        TestQueryLLM --&gt; TestRAG\n        TestRAG --&gt;  [*]\n    }\n\n    [*] --&gt; DeployNKE\n    DeployNKE --&gt; NAIPreRequisites\n    NAIPreRequisites --&gt; DeployLLMV1\n    DeployLLMV1 --&gt; TestLLMApp\n    TestLLMApp --&gt; [*]</code></pre>"},{"location":"llmmgmt/llm_dev_deploy/","title":"Deploying GPT-in-a-Box NVD Reference Application using GitOps (FluxCD)","text":"<pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployLLMV1 {\n        [*] --&gt; BootStrapMgmtCluster\n        BootStrapMgmtCluster --&gt;  BootStrapDevCluster\n        BootStrapDevCluster --&gt; MonitorResourcesDeployment\n        MonitorResourcesDeployment --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployLLMV1 \n    DeployLLMV1 --&gt; TestLLMApp : next section\n    TestLLMApp --&gt; [*]</code></pre>"},{"location":"llmmgmt/llm_dev_deploy/#bootstrap-dev-cluster","title":"Bootstrap Dev Cluster","text":"<p>A <code>.env</code>file is provided at   <code>$HOME/nai-llm-fleet-infra</code> folder for ease of configuration. We need to make copies of this for m dev-cluster kubernetes cluster.</p> <ol> <li> <p>Set <code>K8S_CLUSTER_NAME</code> environment variable and make a copy of <code>./.env.sample.yaml</code> for <code>dev-cluster</code> kubernetes cluster</p> <pre><code>export K8S_CLUSTER_NAME=dev-cluster\ncp ./.env.sample.yaml ./.env.${K8S_CLUSTER_NAME}.yaml\n</code></pre> </li> <li> <p>Open <code>.env.dev-cluster.yaml</code> file in VSC</p> </li> <li> <p>Change the highlighted fields to match your information</p> <p>Note</p> <p>There are a few yaml key value pair blocks of configuration to be updated in <code>.env.env-cluster.yaml</code> file</p> <p>Remember to use your own information for the following:</p> <ul> <li>Github repo and api token</li> <li>Docker registry information - for container downloads without rate limiting</li> <li>Prism Central/Element details</li> <li>Two IPs for KubeVIP to assign to Ingress and Istio </li> <li>Reference to FQDN reserved for <code>management_cluster_ingress_subdomain</code> (from Create Nginx Ingress and Istio VIP/FDQN section)</li> <li>Nutanix NFS share to store the <code>llama-2-13b-chat</code> model</li> </ul> Template fileExample file .env.sample.yaml<pre><code>k8s_cluster:\n\n  ## kubernetes distribution - supported \"nke\" \"kind\"\n  distribution: nke\n  ## kubernetes cluster name\n  name: _required\n  ## cluster_profile_type - anything under clusters/_profiles (e.g., llm-management, llm-workloads, etc.)\n  profile: _required\n  ## environment name - based on profile selected under clusters/_profiles/&lt;profile&gt;/&lt;environment&gt; (e.g., prod, non-prod, etc.)\n  environment: _required\n\n  ## docker hub registry configs\n  registry:\n    docker_hub:\n      user: _required\n      password: _required\n\n  ## nvidia gpu specific configs\n  gpu_operator:\n    enabled: false\n    version: v23.9.0\n    cuda_toolkit_version: v1.14.3-centos7\n    ## time slicing typically only configured on dev scenarios. \n    ## ideal for jupyter notebooks\n    time_slicing:\n      enabled: false\n      replica_count: 2\n\nflux:\n  ## flux specific configs for github repo\n  github:\n    repo_url: _required\n    repo_user: _required\n    repo_api_token: _required\n\ninfra:\n  ## Global nutanix configs\n  nutanix:\n    ## Nutanix Prism Creds, required to download NKE creds\n    prism_central:\n      enabled: false\n      # endpoint: _required_if_enabled\n      # user: _required_if_enabled\n      # password: _required_if_enabled\n\n    ## Nutanix Objects Store Configs\n    objects:\n      enabled: false\n      # host: _required_if_enabled\n      # port: _required_if_enabled\n      # region: _required_if_enabled\n      # use_ssl: _required_if_enabled\n      # access_key: _required_if_enabled\n      # secret_key: _required_if_enabled\n\nservices:\n  #####################################################\n  ## Required variables for kube-vip and depedent services\n  ## kube-vip specific configs required for any services needing to be configured with LoadBalancer Virtual IP Addresses\n  kube_vip:\n    enabled: false\n    ## Used to configure default global IPAM pool. A minimum of 2 ips should be provide in a range\n    ## For Example: ipam_range: 172.20.0.22-172.20.0.23\n    #ipam_range: _required_if_enabled\n\n  ## required for all platform services that are leveraging nginx-ingress\n  nginx_ingress:\n    enabled: false\n    version: 4.8.3\n    ## Virtual IP Address (VIP) dedicated for nginx-ingress controller. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## Example: vip: 172.20.0.20\n    #vip: _required_if_enabled\n\n    ## NGINX Wildcard Ingress Subdomain used for all default ingress objects created within cluster \n    ## For DEMO purposes, it is common to prefix subdomain with cluster-name as each cluster would require dedicated wildcard domain.\n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.example.com, then value is example.com\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: wildcard_ingress_subdomain:flux-kind-local.172.20.0.20.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n    ## Wildcard Ingress Subdomain for management cluster.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates\n    #management_cluster_ingress_subdomain: _required_if_enabled\n\n  istio:\n    enabled: false\n    version: 1.17.2\n    ## Virtual IP Address (VIP) dedicated for istio ingress gateway. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## This address should be mapped to wildcard_ingress_subdomain defined below. For Example: vip: 172.20.0.21\n    #vip: _required_if_enabled\n\n    ## Istio Ingress Gateway - Wildcard Subdomain used for all knative/kserve llm inference endpoints. \n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.llm.example.com, then value is llm.example.com\n    ## If leveraging AWS Route 53 DNS with Let's Encrypt (below), make sure to enable/configure AWS credentials needed to \n    ## support CertificateSigningRequests using ACME DNS Challenges.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: llm.flux-kind-local.172.20.0.21.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n  cert_manager:\n    ## if enabled - cluster issuer will be self-signed-issuer\n    enabled: false\n    version: v1.13.5\n    ## if aws_route53_acme_dns.enabled - the cluster issuer across all services will be set to \"letsencrypt-issuer\"\n    ## Following AWS Route53 Access Creds required for Lets Encrypt ACME DNS Challenge\n    ## For additional details, https://cert-manager.io/docs/configuration/acme/dns01/route53/\n    ## minimum supported cert-manager version is v1.9.1 https://cert-manager.io/docs/releases/release-notes/release-notes-1.9/#v191\n    aws_route53_acme_dns:\n      enabled: false\n      # email: _required_if_enabled\n      # zone: _required_if_enabled\n      # hosted_zone_id: _required_if_enabled\n      # region: _required_if_enabled\n      # key_id: _required_if_enabled\n      # key_secret: _required_if_enabled\n\n  ## do not disable kyverno unless you know what you're doing\n  ## this is needed to keep docker hub creds synchronized between namespaces.\n  kyverno:\n    enabled: true\n    version: 3.1.4\n\n  ## the following versions and dependencies kserve are aligned with GPT In A Box Opendocs\n  ## the only exception is with cert-manager due to usage of aws route 53\n  ## https://opendocs.nutanix.com/gpt-in-a-box/kubernetes/v0.2/getting_started/\n\n  kserve:\n    enabled: false\n    version: v0.11.2\n\n  knative_serving:\n    enabled: false\n    version: knative-v1.10.1\n\n  knative_istio:\n    enabled: false\n    version: knative-v1.10.0\n\n  ## The following components are leveraged to support Nutanix Validated Designs\n  ## The NVD for GPT in a Box leverages a RAG Pipeline with Serverless Functions \n  ## to demonstrate end to end workflow with Nutanix Integration\n\n  ## Milvus is vector database \n  milvus:\n    enabled: false\n    version: 4.1.13\n    milvus_bucket_name: milvus\n\n  ## Knative Eventing used to receive Event notifications from Nutanix Objects Document Bucket\n  knative_eventing:\n    enabled: false\n    version: knative-v1.10.1\n\n  ## Kafka is messaging broker used by both knative eventing Document Ingestion serverless function\n  ## and integrates with Nutanix Objects Events Notification Kafka Endpoints\n  ## Kafka is also leveraged by Milvus as a Messaging Broker for Milvus related events, as opposed to the default Apache Pulsar\n  kafka:\n    enabled: false\n    version: 26.8.5\n\n  ## OpenTelemetry Collector version is used for both the Deployment and Daemon is used to collect data for monitoring\n  opentelemetry_collector:\n    enabled: false\n    version: 0.80.1\n\n  ## OpenTelemetry Operator is used to deploy opentelemetry components\n  opentelemetry_operator:\n    enabled: false\n    version: 0.47.0\n\n  ## Uptrace is Observability / Monitoring UI\n  uptrace:\n    enabled: false\n    version: 1.5.7\n\n  ## Jupyterhub is deployed on non-prod workload clusters in NVD Reference\n  jupyterhub:\n    enabled: false\n    version: 3.1.0\n\n  redis:\n    enabled: false\n    version: 18.1.6\n\n  elasticsearch:\n    enabled: false\n    version: 19.13.10\n\n  kubernetes_dashboard:\n    enabled: false\n    version: 7.3.2\n\n  weave_gitops:\n    enabled: true\n    version: 4.0.36\n\napps:\n  ## Required GPT NVD Reference Application Helm Chart Configs\n  gptnvd_reference_app:\n    enabled: false\n    version: 0.2.7\n    #documents_bucket_name: documents01\n  ## Required NAI LLM Helm Chart Configs\n  ### huggingFaceToken required when useExistingNFS. This will download model when llm is initialized\n  nai_helm:\n    enabled: false\n    version: 0.1.1\n    #model: llama2_7b_chat\n    #revision: 94b07a6e30c3292b8265ed32ffdeccfdadf434a8\n    #maxTokens: 4000\n    #repPenalty: 1.2\n    #temperature: 0.2\n    #topP: 0.9\n    #useExistingNFS: false\n    #nfs_export: /llm-model-store\n    #nfs_server: _required\n    #huggingFaceToken: _required\n</code></pre> .env.dev-cluster.yaml<pre><code>k8s_cluster:\n\n  ## kubernetes distribution - supported \"nke\" \"kind\"\n  distribution: nke\n  ## kubernetes cluster name\n  name: dev-cluster\n  ## cluster_profile_type - anything under clusters/_profiles (e.g., llm-management, llm-workloads, etc.)\n  profile: llm-workloads\n  ## environment name - based on profile selected under clusters/_profiles/&lt;profile&gt;/&lt;environment&gt; (e.g., prod, non-prod, etc.)\n  environment: non-prod\n\n  ## docker hub registry configs\n  registry:\n    docker_hub:\n      user: your_docker_username\n      password: your_docker_password\n\n  ## nvidia gpu specific configs\n  gpu_operator:\n    enabled: true\n    version: v23.9.0\n    cuda_toolkit_version: v1.14.3-centos7\n    ## time slicing typically only configured on dev scenarios. \n    ## ideal for jupyter notebooks\n    time_slicing:\n      enabled: true\n      replica_count: 2\n\nflux:\n  ## flux specific configs for github repo\n  github:\n    repo_url: https://github.com/&lt;your_github_org&gt;/nai-llm-fleet-infra.git\n    repo_user: your_github_username\n    repo_api_token: your_github_api_token\n\ninfra:\n  ## Global nutanix configs\n  nutanix:\n    ## Nutanix Prism Creds, required to download NKE creds\n    prism_central:\n      enabled: true\n      endpoint: &lt;PC FQDN&gt;\n      user: &lt;PC user&gt;\n      password: &lt;PC password&gt;\n\n    ## Nutanix Objects Store Configs\n    objects:\n      enabled: true\n      host: objects.example.com\n      port: 80\n      region: us-east-1\n      use_ssl: false\n      access_key: your_bucket_access_key\n      secret_key: your_bucket_secret_key\n\nservices:\n  #####################################################\n  ## Required variables for kube-vip and depedent services\n  ## kube-vip specific configs required for any services needing to be configured with LoadBalancer Virtual IP Addresses\n  kube_vip:\n    enabled: true\n    ## Used to configure default global IPAM pool. A minimum of 2 ips should be provide in a range\n    ## For Example: ipam_range: 172.20.0.22-172.20.0.23\n    ipam_range: 10.x.x.216-10.x.x.217\n\n  ## required for all platform services that are leveraging nginx-ingress\n  nginx_ingress:\n    enabled: true\n    version: 4.8.3\n    ## Virtual IP Address (VIP) dedicated for nginx-ingress controller. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## Example: vip: 172.20.0.20\n    vip: 10.x.x.216\n\n    ## NGINX Wildcard Ingress Subdomain used for all default ingress objects created within cluster \n    ## For DEMO purposes, it is common to prefix subdomain with cluster-name as each cluster would require dedicated wildcard domain.\n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.example.com, then value is example.com\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: wildcard_ingress_subdomain:flux-kind-local.172.20.0.20.nip.io\n    wildcard_ingress_subdomain: dev-cluster.10.x.x.216.nip.io #change this to use nginx ingress\n\n    ## Wildcard Ingress Subdomain for management cluster.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates\n    management_cluster_ingress_subdomain: mgmt-cluster.10.x.x.214.nip.io #change this ingress nginx.vip from mgmt cluster\n\n  istio:\n    enabled: true\n    version: 1.17.2\n    ## Virtual IP Address (VIP) dedicated for istio ingress gateway. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## This address should be mapped to wildcard_ingress_subdomain defined below. For Example: vip: 172.20.0.21\n    vip: 10.x.x.217\n\n    ## Istio Ingress Gateway - Wildcard Subdomain used for all knative/kserve llm inference endpoints. \n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.llm.example.com, then value is llm.example.com\n    ## If leveraging AWS Route 53 DNS with Let's Encrypt (below), make sure to enable/configure AWS credentials needed to \n    ## support CertificateSigningRequests using ACME DNS Challenges.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: llm.flux-kind-local.172.20.0.21.nip.io\n    wildcard_ingress_subdomain: dev-cluster.10.x.x.217.nip.io #change this istio.vip\n\n  cert_manager:\n    ## if enabled - cluster issuer will be self-signed-issuer\n    enabled: true\n    version: v1.9.1\n    ## if aws_route53_acme_dns.enabled - the cluster issuer across all services will be set to \"letsencrypt-issuer\"\n    ## Following AWS Route53 Access Creds required for Lets Encrypt ACME DNS Challenge\n    ## For additional details, https://cert-manager.io/docs/configuration/acme/dns01/route53/\n    ## minimum supported cert-manager version is v1.9.1 https://cert-manager.io/docs/releases/release-notes/release-notes-1.9/#v191\n    aws_route53_acme_dns:\n      enabled: false\n      # email: _required_if_enabled\n      # zone: _required_if_enabled\n      # hosted_zone_id: _required_if_enabled\n      # region: _required_if_enabled\n      # key_id: _required_if_enabled\n      # key_secret: _required_if_enabled\n\n  ## do not disable kyverno unless you know what you're doing\n  ## this is needed to keep docker hub creds synchronized between namespaces.\n  kyverno:\n    enabled: true\n    version: 3.1.4\n\n  ## the following versions and dependencies kserve are aligned with GPT In A Box Opendocs\n  ## the only exception is with cert-manager due to usage of aws route 53\n  ## https://opendocs.nutanix.com/gpt-in-a-box/kubernetes/v0.2/getting_started/\n\n  kserve:\n    enabled: true\n    version: v0.11.2\n\n  knative_serving:\n    enabled: true\n    version: knative-v1.10.1\n\n  knative_istio:\n    enabled: true\n    version: knative-v1.10.0\n\n  ## The following components are leveraged to support Nutanix Validated Designs\n  ## The NVD for GPT in a Box leverages a RAG Pipeline with Serverless Functions \n  ## to demonstrate end to end workflow with Nutanix Integration\n\n  ## Milvus is vector database \n  milvus:\n    enabled: false\n    # version: 4.1.13\n    # milvus_bucket_name: romanticism-dev01-mgmt-milvus\n\n  ## Knative Eventing used to receive Event notifications from Nutanix Objects Document Bucket\n  knative_eventing:\n    enabled: true\n    version: knative-v1.10.1\n\n  ## Kafka is messaging broker used by both knative eventing Document Ingestion serverless function\n  ## and integrates with Nutanix Objects Events Notification Kafka Endpoints\n  ## Kafka is also leveraged by Milvus as a Messaging Broker for Milvus related events, as opposed to the default Apache Pulsar\n  kafka:\n    enabled: false\n    # version: 26.8.5\n\n  ## OpenTelemetry Collector version is used for both the Deployment and Daemon is used to collect data for monitoring\n  opentelemetry_collector:\n    enabled: true\n    version: 0.80.1\n\n  ## OpenTelemetry Operator is used to deploy opentelemetry components\n  opentelemetry_operator:\n    enabled: true\n    version: 0.47.0\n\n  ## Uptrace is Observability / Monitoring UI\n  uptrace:\n    enabled: true\n    version: 1.5.7\n\n  ## Jupyterhub is deployed on non-prod workload clusters in NVD Reference\n  jupyterhub:\n    enabled: true\n    version: 3.1.0\n\n  redis:\n    enabled: false\n    version: 18.1.6\n\n  elasticsearch:\n    enabled: false\n    version: 19.13.10\n\n  kubernetes_dashboard:\n    enabled: false\n    version: 7.3.2\n\n  weave_gitops:\n    enabled: true\n    version: 4.0.36\n\napps:\n  ## Required GPT NVD Reference Application Helm Chart Configs\n  gptnvd_reference_app:\n    enabled: true\n    version: 0.2.7\n    documents_bucket_name: documents01\n  ## Required NAI LLM Helm Chart Configs\n  ### huggingFaceToken required when useExistingNFS. This will download model when llm is initialized\n  nai_helm:\n    enabled: true\n    version: 0.1.1\n    model: llama2_7b_chat\n    revision: 94b07a6e30c3292b8265ed32ffdeccfdadf434a8\n    maxTokens: 4000\n    repPenalty: 1.2\n    temperature: 0.2\n    topP: 0.9\n    useExistingNFS: true\n    nfs_export: /llm-model-store\n    nfs_server: files.# Example: files.pe.example.com\n    #huggingFaceToken: hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre> </li> <li> <p>Generate and Validate Configurations</p> <pre><code>task bootstrap:generate_cluster_configs\n</code></pre> <p>Verify the generated cluster configs</p> <pre><code>cat .local/${K8S_CLUSTER_NAME}/.env\ncat clusters/${K8S_CLUSTER_NAME}/platform/cluster-configs.yaml\n</code></pre> </li> <li> <p>Validate Encrypted Secrets and make sure the values you entered in <code>.env.mgmt-cluster.yaml</code> file</p> <pre><code>task sops:decrypt\n</code></pre> </li> <li> <p>Select New (or Switching to Existing) Cluster and Download NKE creds for <code>dev-cluster</code></p> <pre><code>eval $(task nke:switch-shell-env) &amp;&amp; \\\ntask nke:download-creds &amp;&amp; \\\nkubectl get nodes\n</code></pre> <pre><code># command execution example\n\n$ eval $(task nke:switch-shell-env) &amp;&amp; \\\ntask nke:download-creds &amp;&amp; \\\nkubectl get nodes\nSelect existing cluster instance to load from .local/ directory.\n\n&gt; dev-cluster                  &lt;&lt;&lt; choose dev-cluster                          \n  mgmt-cluster\n</code></pre> </li> <li> <p>Taint the GPU nodes. </p> <p><pre><code>task kubectl:taint_gpu_nodes\n</code></pre> [Optional] - If gpu are over utilised, drain the gpu_nodes of workloads</p> <pre><code>task kubectl:drain_gpu_nodes\n</code></pre> </li> <li> <p>Run Flux Bootstrapping - <code>task bootstrap:silent</code></p> <pre><code>task bootstrap:silent\n</code></pre> <p>Note</p> <p>If there are any issues, troubleshoot using <code>task ts:flux-collect</code>. You can re-run task <code>bootstrap:silent</code> as many times as needed.</p> </li> <li> <p>Monitor on New Terminal to make sure <code>READY</code> status is <code>TRUE</code> for all resources using the following command</p> <pre><code>eval $(task nke:switch-shell-env) &amp;&amp; \\\ntask flux:watch\n</code></pre> <p>Note</p> <p>This may take up to 10 minutes.</p> <p>If there are any issues, update local git repo, push up changes and run <code>task flux:reconcile</code></p> </li> <li> <p>[Optional] Post Install - Taint GPU Nodepool with <code>dedicated=gpu:NoSchedule</code></p> <p>Note</p> <p>If undesired workloads already running on gpu nodepools, drain nodes using <code>task kubectl:drain_gpu_nodes</code></p> <pre><code>## taint gpu nodes with label nvidia.com/gpu.present=true\ntask kubectl:taint_gpu_nodes\n\n## to view taint configurations on all nodes\nkubectl get nodes -o='custom-columns=NodeName:.metadata.name,TaintKey:.spec.taints[*].key,TaintValue:.spec.taints[*].value,TaintEffect:.spec.taints[*].effect'\n</code></pre> </li> <li> <p>Once all workloads are in <code>TRUE</code> state except GPT based workloads, untaint the GPU nodes to schedule GPU based workloads, use the following commands.</p> <p><pre><code>task kubectl:untaint_gpu_nodes\n#\n# to view taint configurations on all nodes\nkubectl get nodes -o='custom-columns=NodeName:.metadata.name,TaintKey:.spec.taints[*].key,TaintValue:.spec.taints[*].value,TaintEffect:.spec.taints[*].effect'\n</code></pre> Wait for all GPU based services are in TRUE state, we are ready to test LLM App. </p> </li> </ol>"},{"location":"llmmgmt/llm_mgt_deploy/","title":"Deploying GPT-in-a-Box NVD Reference Application using GitOps (FluxCD)","text":"<pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployLLMV1 {\n        [*] --&gt; BootStrapMgmtCluster\n        BootStrapMgmtCluster --&gt;  BootStrapDevCluster\n        BootStrapDevCluster --&gt; MonitorResourcesDeployment\n        MonitorResourcesDeployment --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployLLMV1 \n    DeployLLMV1 --&gt; TestLLMApp : next section\n    TestLLMApp --&gt; [*]</code></pre>"},{"location":"llmmgmt/llm_mgt_deploy/#bootstrap-management-cluster","title":"Bootstrap Management Cluster","text":"<p>A <code>.env</code>file is provided at   <code>$HOME/nainai-llm-fleet-infra</code> folder for ease of configuration. We need to make copies of this for mgmt-cluster and dev-cluster kubernetes clusters that you deployed in the previous sections. </p> <ol> <li> <p>Set <code>K8S_CLUSTER_NAME</code> environment variable and make a copy of <code>./.env.sample.yaml</code> for <code>mgmt-cluster</code> kubernetes cluster</p> <pre><code>export K8S_CLUSTER_NAME=mgmt-cluster\ncp ./.env.sample.yaml ./.env.${K8S_CLUSTER_NAME}.yaml\n</code></pre> </li> <li> <p>Open <code>.env.mgmt-cluster.yaml</code> file in VSC</p> </li> <li> <p>Change the highlighted fields to match your information (see Example file)</p> <p>Note</p> <p>There are a few yaml key value pair blocks of configuration to be updated in <code>.env.mgmt-cluster.yaml</code> file</p> <p>Remember to use your own information for the following:</p> <ul> <li>Github repo and api token</li> <li>Docker registry information - for container downloads without rate limiting</li> <li>Prism Central/Element details</li> <li>Nutanix Objects store and bucket details (for Milvus)</li> <li>Two IPs for KubeVIP to assign to Ingress and Istio </li> <li>Nutanix NFS share to store the <code>llama-2-13b-chat</code> model</li> </ul> Template fileExample file .env.sample.yaml<pre><code>k8s_cluster:\n\n  ## kubernetes distribution - supported \"nke\" \"kind\"\n  distribution: nke\n  ## kubernetes cluster name\n  name: _required\n  ## cluster_profile_type - anything under clusters/_profiles (e.g., llm-management, llm-workloads, etc.)\n  profile: _required\n  ## environment name - based on profile selected under clusters/_profiles/&lt;profile&gt;/&lt;environment&gt; (e.g., prod, non-prod, etc.)\n  environment: _required\n\n  ## docker hub registry configs\n  registry:\n    docker_hub:\n      user: _required\n      password: _required\n\n  ## nvidia gpu specific configs\n  gpu_operator:\n    enabled: false\n    version: v23.9.0\n    cuda_toolkit_version: v1.14.3-centos7\n    ## time slicing typically only configured on dev scenarios. \n    ## ideal for jupyter notebooks\n    time_slicing:\n      enabled: false\n      replica_count: 2\n\nflux:\n  ## flux specific configs for github repo\n  github:\n    repo_url: _required\n    repo_user: _required\n    repo_api_token: _required\n\ninfra:\n  ## Global nutanix configs\n  nutanix:\n    ## Nutanix Prism Creds, required to download NKE creds\n    prism_central:\n      enabled: false\n      # endpoint: _required_if_enabled\n      # user: _required_if_enabled\n      # password: _required_if_enabled\n\n    ## Nutanix Objects Store Configs\n    objects:\n      enabled: false\n      # host: _required_if_enabled\n      # port: _required_if_enabled\n      # region: _required_if_enabled\n      # use_ssl: _required_if_enabled\n      # access_key: _required_if_enabled\n      # secret_key: _required_if_enabled\n\nservices:\n  #####################################################\n  ## Required variables for kube-vip and depedent services\n  ## kube-vip specific configs required for any services needing to be configured with LoadBalancer Virtual IP Addresses\n  kube_vip:\n    enabled: false\n    ## Used to configure default global IPAM pool. A minimum of 2 ips should be provide in a range\n    ## For Example: ipam_range: 172.20.0.22-172.20.0.23\n    #ipam_range: _required_if_enabled\n\n  ## required for all platform services that are leveraging nginx-ingress\n  nginx_ingress:\n    enabled: false\n    version: 4.8.3\n    ## Virtual IP Address (VIP) dedicated for nginx-ingress controller. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## Example: vip: 172.20.0.20\n    #vip: _required_if_enabled\n\n    ## NGINX Wildcard Ingress Subdomain used for all default ingress objects created within cluster \n    ## For DEMO purposes, it is common to prefix subdomain with cluster-name as each cluster would require dedicated wildcard domain.\n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.example.com, then value is example.com\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: wildcard_ingress_subdomain:flux-kind-local.172.20.0.20.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n    ## Wildcard Ingress Subdomain for management cluster.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates\n    #management_cluster_ingress_subdomain: _required_if_enabled\n\n  istio:\n    enabled: false\n    version: 1.17.2\n    ## Virtual IP Address (VIP) dedicated for istio ingress gateway. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## This address should be mapped to wildcard_ingress_subdomain defined below. For Example: vip: 172.20.0.21\n    #vip: _required_if_enabled\n\n    ## Istio Ingress Gateway - Wildcard Subdomain used for all knative/kserve llm inference endpoints. \n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.llm.example.com, then value is llm.example.com\n    ## If leveraging AWS Route 53 DNS with Let's Encrypt (below), make sure to enable/configure AWS credentials needed to \n    ## support CertificateSigningRequests using ACME DNS Challenges.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: llm.flux-kind-local.172.20.0.21.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n  cert_manager:\n    ## if enabled - cluster issuer will be self-signed-issuer\n    enabled: false\n    version: v1.13.5\n    ## if aws_route53_acme_dns.enabled - the cluster issuer across all services will be set to \"letsencrypt-issuer\"\n    ## Following AWS Route53 Access Creds required for Lets Encrypt ACME DNS Challenge\n    ## For additional details, https://cert-manager.io/docs/configuration/acme/dns01/route53/\n    ## minimum supported cert-manager version is v1.9.1 https://cert-manager.io/docs/releases/release-notes/release-notes-1.9/#v191\n    aws_route53_acme_dns:\n      enabled: false\n      # email: _required_if_enabled\n      # zone: _required_if_enabled\n      # hosted_zone_id: _required_if_enabled\n      # region: _required_if_enabled\n      # key_id: _required_if_enabled\n      # key_secret: _required_if_enabled\n\n  ## do not disable kyverno unless you know what you're doing\n  ## this is needed to keep docker hub creds synchronized between namespaces.\n  kyverno:\n    enabled: true\n    version: 3.1.4\n\n  ## the following versions and dependencies kserve are aligned with GPT In A Box Opendocs\n  ## the only exception is with cert-manager due to usage of aws route 53\n  ## https://opendocs.nutanix.com/gpt-in-a-box/kubernetes/v0.2/getting_started/\n\n  kserve:\n    enabled: false\n    version: v0.11.2\n\n  knative_serving:\n    enabled: false\n    version: knative-v1.10.1\n\n  knative_istio:\n    enabled: false\n    version: knative-v1.10.0\n\n  ## The following components are leveraged to support Nutanix Validated Designs\n  ## The NVD for GPT in a Box leverages a RAG Pipeline with Serverless Functions \n  ## to demonstrate end to end workflow with Nutanix Integration\n\n  ## Milvus is vector database \n  milvus:\n    enabled: false\n    version: 4.1.13\n    milvus_bucket_name: milvus\n\n  ## Knative Eventing used to receive Event notifications from Nutanix Objects Document Bucket\n  knative_eventing:\n    enabled: false\n    version: knative-v1.10.1\n\n  ## Kafka is messaging broker used by both knative eventing Document Ingestion serverless function\n  ## and integrates with Nutanix Objects Events Notification Kafka Endpoints\n  ## Kafka is also leveraged by Milvus as a Messaging Broker for Milvus related events, as opposed to the default Apache Pulsar\n  kafka:\n    enabled: false\n    version: 26.8.5\n\n  ## OpenTelemetry Collector version is used for both the Deployment and Daemon is used to collect data for monitoring\n  opentelemetry_collector:\n    enabled: false\n    version: 0.80.1\n\n  ## OpenTelemetry Operator is used to deploy opentelemetry components\n  opentelemetry_operator:\n    enabled: false\n    version: 0.47.0\n\n  ## Uptrace is Observability / Monitoring UI\n  uptrace:\n    enabled: false\n    version: 1.5.7\n\n  ## Jupyterhub is deployed on non-prod workload clusters in NVD Reference\n  jupyterhub:\n    enabled: false\n    version: 3.1.0\n\n  redis:\n    enabled: false\n    version: 18.1.6\n\n  elasticsearch:\n    enabled: false\n    version: 19.13.10\n\n  kubernetes_dashboard:\n    enabled: false\n    version: 7.3.2\n\n  weave_gitops:\n    enabled: true\n    version: 4.0.36\n\napps:\n  ## Required GPT NVD Reference Application Helm Chart Configs\n  gptnvd_reference_app:\n    enabled: false\n    version: 0.2.7\n    #documents_bucket_name: documents01\n  ## Required NAI LLM Helm Chart Configs\n  ### huggingFaceToken required when useExistingNFS. This will download model when llm is initialized\n  nai_helm:\n    enabled: false\n    version: 0.1.1\n    #model: llama2_7b_chat\n    #revision: 94b07a6e30c3292b8265ed32ffdeccfdadf434a8\n    #maxTokens: 4000\n    #repPenalty: 1.2\n    #temperature: 0.2\n    #topP: 0.9\n    #useExistingNFS: false\n    #nfs_export: /llm-model-store\n    #nfs_server: _required\n    #huggingFaceToken: _required\n</code></pre> .env.mgmt-cluster.yaml<pre><code>k8s_cluster:\n\n  ## kubernetes distribution - supported \"nke\" \"kind\"\n  distribution: nke\n  ## kubernetes cluster name\n  name: mgmt-cluster\n  ## cluster_profile_type - anything under clusters/_profiles (e.g., llm-management, llm-workloads, etc.)\n  profile: llm-management\n  ## environment name - based on profile selected under clusters/_profiles/&lt;profile&gt;/&lt;environment&gt; (e.g., prod, non-prod, etc.)\n  environment: non-prod\n\n  ## docker hub registry configs\n  registry:\n    docker_hub:\n      user: your_docker_username\n      password: your_docker_password\n\n  ## nvidia gpu specific configs\n  gpu_operator:\n    enabled: false\n    version: v23.9.0\n    cuda_toolkit_version: v1.14.3-centos7\n    ## time slicing typically only configured on dev scenarios. \n    ## ideal for jupyter notebooks\n    time_slicing:\n      enabled: false\n      replica_count: 2\n\nflux:\n  ## flux specific configs for github repo\n  github:\n    repo_url: https://github.com/&lt;your_github_org&gt;/nai-llm-fleet-infra.git\n    repo_user: your_github_username\n    repo_api_token: your_github_api_token\n\ninfra:\n  ## Global nutanix configs\n  nutanix:\n    ## Nutanix Prism Creds, required to download NKE creds\n    prism_central:\n      enabled: true\n      endpoint: &lt;PC FQDN&gt;\n      user: &lt;PC user&gt;\n      password: &lt;PC password&gt;\n\n    ## Nutanix Objects Store Configs\n    objects:\n      enabled: true\n      host: objects.example.com\n      port: 80\n      region: us-east-1\n      use_ssl: false\n      access_key: your_bucket_access_key\n      secret_key: your_bucket_secret_key\n\nservices:\n  #####################################################\n  ## Required variables for kube-vip and depedent services\n  ## kube-vip specific configs required for any services needing to be configured with LoadBalancer Virtual IP Addresses\n  kube_vip:\n    enabled: true\n    ## Used to configure default global IPAM pool. A minimum of 2 ips should be provide in a range\n    ## For Example: ipam_range: 172.20.0.22-172.20.0.23\n    ipam_range: 10.x.x.214-10.x.x.215\n\n  ## required for all platform services that are leveraging nginx-ingress\n  nginx_ingress:\n    enabled: true\n    version: 4.8.3\n    ## Virtual IP Address (VIP) dedicated for nginx-ingress controller. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## Example: vip: 172.20.0.20\n    vip: 10.x.x.214\n\n    ## NGINX Wildcard Ingress Subdomain used for all default ingress objects created within cluster \n    ## For DEMO purposes, it is common to prefix subdomain with cluster-name as each cluster would require dedicated wildcard domain.\n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.example.com, then value is example.com\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: wildcard_ingress_subdomain:flux-kind-local.172.20.0.20.nip.io\n    wildcard_ingress_subdomain: mgmt-cluster.10.x.x.214.nip.io\n\n    ## Wildcard Ingress Subdomain for management cluster.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates\n    management_cluster_ingress_subdomain: mgmt-cluster.10.x.x.214.nip.io\n\n  istio:\n    enabled: false\n    version: 1.17.2\n    ## Virtual IP Address (VIP) dedicated for istio ingress gateway. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## This address should be mapped to wildcard_ingress_subdomain defined below. For Example: vip: 172.20.0.21\n    #vip: _required_if_enabled\n\n    ## Istio Ingress Gateway - Wildcard Subdomain used for all knative/kserve llm inference endpoints. \n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.llm.example.com, then value is llm.example.com\n    ## If leveraging AWS Route 53 DNS with Let's Encrypt (below), make sure to enable/configure AWS credentials needed to \n    ## support CertificateSigningRequests using ACME DNS Challenges.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: llm.flux-kind-local.172.20.0.21.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n  cert_manager:\n    ## if enabled - cluster issuer will be self-signed-issuer\n    enabled: false\n    version: v1.13.5\n    ## if aws_route53_acme_dns.enabled - the cluster issuer across all services will be set to \"letsencrypt-issuer\"\n    ## Following AWS Route53 Access Creds required for Lets Encrypt ACME DNS Challenge\n    ## For additional details, https://cert-manager.io/docs/configuration/acme/dns01/route53/\n    ## minimum supported cert-manager version is v1.9.1 https://cert-manager.io/docs/releases/release-notes/release-notes-1.9/#v191\n    aws_route53_acme_dns:\n      enabled: false\n      # email: _required_if_enabled\n      # zone: _required_if_enabled\n      # hosted_zone_id: _required_if_enabled\n      # region: _required_if_enabled\n      # key_id: _required_if_enabled\n      # key_secret: _required_if_enabled\n\n  ## do not disable kyverno unless you know what you're doing\n  ## this is needed to keep docker hub creds synchronized between namespaces.\n  kyverno:\n    enabled: true\n    version: 3.1.4\n\n  ## the following versions and dependencies kserve are aligned with GPT In A Box Opendocs\n  ## the only exception is with cert-manager due to usage of aws route 53\n  ## https://opendocs.nutanix.com/gpt-in-a-box/kubernetes/v0.2/getting_started/\n\n  kserve:\n    enabled: false\n    version: v0.11.2\n\n  knative_serving:\n    enabled: false\n    version: knative-v1.10.1\n\n  knative_istio:\n    enabled: false\n    version: knative-v1.10.0\n\n  ## The following components are leveraged to support Nutanix Validated Designs\n  ## The NVD for GPT in a Box leverages a RAG Pipeline with Serverless Functions \n  ## to demonstrate end to end workflow with Nutanix Integration\n\n  ## Milvus is vector database \n  milvus:\n    enabled: true\n    version: 4.1.13\n    milvus_bucket_name: mgmt-cluster-milvus\n\n  ## Knative Eventing used to receive Event notifications from Nutanix Objects Document Bucket\n  knative_eventing:\n    enabled: false\n    version: knative-v1.10.1\n\n  ## Kafka is messaging broker used by both knative eventing Document Ingestion serverless function\n  ## and integrates with Nutanix Objects Events Notification Kafka Endpoints\n  ## Kafka is also leveraged by Milvus as a Messaging Broker for Milvus related events, as opposed to the default Apache Pulsar\n  kafka:\n    enabled: true\n    version: 26.8.5\n\n  ## OpenTelemetry Collector version is used for both the Deployment and Daemon is used to collect data for monitoring\n  opentelemetry_collector:\n    enabled: true\n    version: 0.80.1\n\n  ## OpenTelemetry Operator is used to deploy opentelemetry components\n  opentelemetry_operator:\n    enabled: true\n    version: 0.47.0\n\n  ## Uptrace is Observability / Monitoring UI\n  uptrace:\n    enabled: true\n    version: 1.5.7\n\n  ## Jupyterhub is deployed on non-prod workload clusters in NVD Reference\n  jupyterhub:\n    enabled: false\n    version: 3.1.0\n\n  redis:\n    enabled: false\n    version: 18.1.6\n\n  elasticsearch:\n    enabled: false\n    version: 19.13.10\n\n  kubernetes_dashboard:\n    enabled: false\n    version: 7.3.2\n\n  weave_gitops:\n    enabled: true\n    version: 4.0.36\n\napps:\n  ## Required GPT NVD Reference Application Helm Chart Configs\n  gptnvd_reference_app:\n    enabled: false\n    version: 0.2.7\n    #documents_bucket_name: documents01\n  ## Required NAI LLM Helm Chart Configs\n  ### huggingFaceToken required when useExistingNFS. This will download model when llm is initialized\n  nai_helm:\n    enabled: false\n    version: 0.1.1\n    #model: llama2_7b_chat\n    #revision: 94b07a6e30c3292b8265ed32ffdeccfdadf434a8\n    #maxTokens: 4000\n    #repPenalty: 1.2\n    #temperature: 0.2\n    #topP: 0.9\n    #useExistingNFS: false\n    #nfs_export: /llm-model-store\n    #nfs_server: _required\n    #huggingFaceToken: _required\n</code></pre> </li> <li> <p>Install workstation packages and export <code>krew</code> path</p> <pre><code>task workstation:install-packages\n</code></pre> <pre><code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre> </li> <li> <p>Generate and Validate Configurations</p> <pre><code>task bootstrap:generate_cluster_configs\n</code></pre> <p>Verify the generated cluster configs</p> <pre><code>cat .local/${K8S_CLUSTER_NAME}/.env\ncat clusters/${K8S_CLUSTER_NAME}/platform/cluster-configs.yaml\n</code></pre> </li> <li> <p>Validate Encrypted Secrets and make sure the values match what you entered in <code>.env.mgmt-cluster.yaml</code> file</p> <pre><code>task sops:decrypt\n</code></pre> </li> <li> <p>Select New (or Switching to Existing) Cluster and Download NKE creds for <code>mgmt-cluster</code></p> <p><pre><code>eval $(task nke:switch-shell-env) &amp;&amp; \\\ntask nke:download-creds &amp;&amp; \\\nkubectl get nodes\n</code></pre> <pre><code># command execution example\n\n$ eval $(task nke:switch-shell-env) &amp;&amp; \\\ntask nke:download-creds &amp;&amp; \\\nkubectl get nodes\nSelect existing cluster instance to load from .local/ directory.\n\n&gt; mgmt-cluster                          &lt;&lt;&lt; choose mgmt-cluster.   \n</code></pre></p> </li> <li> <p>Run Flux Bootstrapping - <code>task bootstrap:silent</code></p> <pre><code>task bootstrap:silent\n</code></pre> <p>Note</p> <p>This may take up to 10 minutes.</p> <p>If there are any issues, update local git repo, push up changes and run <code>task flux:reconcile</code></p> </li> <li> <p>Monitor on New Terminal to make sure <code>READY</code> status is <code>TRUE</code> for all resources using the following command</p> <pre><code>eval $(task nke:switch-shell-env) &amp;&amp; \\\ntask flux:watch\n</code></pre> <p>Note</p> <p>If there are any issues, update local git repo, push up changes and run <code>task flux:reconcile</code></p> </li> </ol>"},{"location":"llmmgmt/llm_mgt_deploy/#set-kafka-endpoint-in-nutanix-objects","title":"Set Kafka Endpoint in Nutanix Objects","text":"<p>After successful bootstrap of the <code>mgmt-cluster</code>, get the Kafka ingress endpoint to set the value in Nutanix Objects store. </p> <p>Nutanix Objects store will send a message to kafka endpoint if an object gets stored in the bucket.</p> <ol> <li> <p>On VSC terminal on the jumpbox VM, get the ingress endpoints</p> <pre><code>kubectl get ingress -A | grep kafka\n</code></pre> <pre><code>NAMESPACE      NAME                          CLASS   HOSTS                                         ADDRESS      PORTS     AGE\nkafka          kafka-ingress                 nginx   kafka.mgmt-cluster.10.x.x.214.nip.io          10.x.x.214   80        3h2m         \n</code></pre> </li> <li> <p>Copy the URL value in HOSTS column (note this will be different for you) and add the port number <code>9096</code> as follows</p> <pre><code>kafka.mgmt-cluster.10.x.x.214.nip.io:9096\n</code></pre> </li> <li> <p>Check if the Kafka endpoint is alive and well</p> <p><pre><code>nc -zv kafka.mgmt-cluster.10.x.x.214.nip.io 9096\n</code></pre> <pre><code># command output\nConnection to kafka.mgmt-cluster.10.x.x.214.nip.io port 9096 [tcp/*] succeeded!`\n</code></pre></p> </li> <li> <p>Login to Prism Central, go to Objects and choose the ntnx-objects store (Objects store name could be different for you)</p> </li> <li>Go to Settings &gt; Notification Endpoints</li> <li>Choose the Kafka tab</li> <li>Toggle the Enable button  to enabled</li> <li> <p>Paste the ingress endpoint of your Kafka instance</p> <p> 9.  Click on Save</p> </li> </ol>"},{"location":"llmmgmt/llm_mgt_deploy/#configure-documents01-bucket-to-send-messages-to-kafka-endpoint","title":"Configure documents01 Bucket to send Messages to Kafka Endpoint","text":"<ol> <li>Go to Buckets</li> <li>Click on <code>documents01</code> bucket and choose Data Event Notification from the top menu</li> <li>Click on <code>Add Rule</code> </li> <li>Choose the following:</li> <li>Endpoint - Kafka</li> <li>Scope - All Objects</li> <li>Data Events - All Events</li> <li>Click on Save</li> </ol>"},{"location":"llmmgmt/llm_mgt_deploy/#check-milvus-database-status","title":"Check Milvus Database Status","text":"<p>To make sure Milvus database and associated components are running. </p> <ol> <li> <p>On VSC terminal, check if the Kafka endpoint is alive and well</p> <p><pre><code>nc -zv milvus.mgmt-cluster.10.x.x.214.nip.io 19530\n</code></pre> <pre><code># command output\nConnection to milvus.mgmt-cluster.10.x.x.214.nip.io port 19530 [tcp/*] succeeded!`\n</code></pre></p> </li> <li> <p>Get the Milvus ingress endpoint</p> <pre><code>kubectl get ingress -A | grep attu\n</code></pre> <pre><code>NAMESPACE      NAME                          CLASS   HOSTS                                       ADDRESS      PORTS     AGE\nmilvus         milvus-milvus-vectordb-attu   nginx   attu.mgmt-cluster.10.x.x.214.nip.io         10.x.x.214   80, 443   3h2m\n</code></pre> </li> <li> <p>Copy the URL value in HOSTS column (note this will be different for you)</p> <pre><code>attu.mgmt-cluster.10.x.x.214.nip.io\n</code></pre> </li> <li> <p>Paste the URL in the browser and you should be able to see Milvus database management page. </p> </li> <li> <p>There is no user name and password for Milvus database as this is a test environment. Feel free to update password for <code>root</code> user in the user settings.</p> <p></p> </li> </ol>"},{"location":"llmmgmt/llm_pre_reqs/","title":"Pre-requisites for MGMT and DEV Cluster","text":"<p>In this part of the lab we will prepare pre-requisites for LLM application on GPU nodes.</p> <p>The following is the flow of the applications lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state PreRequisites {\n        [*] --&gt; ReserveIPs\n        ReserveIPs --&gt; CreateBuckets\n        CreateBuckets --&gt; CreateFilesShare\n        CreateFilesShare --&gt; [*]\n    }\n\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployLLMV1 : next section\n    DeployLLMV1 --&gt; TestLLMApp\n    TestLLMApp --&gt; [*]</code></pre> <p>Prepare the following pre-requisites for mgmt-cluster and dev-cluster kubernetes clusters. </p>"},{"location":"llmmgmt/llm_pre_reqs/#fork-and-clone-giab-nvd-gitops-repository","title":"Fork and Clone GiaB NVD Gitops Repository","text":"<p>Warning</p> <p>The following steps are only required if Deploying GPT-In-A-Box v1 using NVD GitOps workflow</p> <ol> <li> <p>Open the following URL and fork the repo to your Github org</p> <p><pre><code>https://github.com/jesse-gonzalez/nai-llm-fleet-infra.git\n</code></pre> 2. From VSC, logon to your jumphost VM (if not already done) 3. Open Terminal</p> </li> <li> <p>From the <code>$HOME</code> directory, clone the fork of your <code>sol-cnai-infra</code> git repo and change working directory</p> CommandSample command <pre><code>git clone https://github.com/_your_github_org/sol-cnai-infra.git\ncd $HOME/sol-cnai-infra/\n</code></pre> <pre><code>git clone https://github.com/rahuman/sol-cnai-infra.git\ncd $HOME/sol-cnai-infra/\n</code></pre> </li> <li> <p>Finally set your github config</p> <pre><code>git config --user.email \"your_github_email\"\ngit config --user.name \"your_github_username\"\n</code></pre> </li> <li> <p>In <code>VSCode</code> &gt; <code>Terminal</code> Login to your Github account using the following command:</p> <pre><code>gh auth login # (1)\n</code></pre> <ol> <li>  If you do not have <code>gh</code> client installed, see Github CLI Installation Docs.</li> </ol> <pre><code># Execution example\n\n\u276f gh auth login                                                                                                               \u2500\u256f\n? What account do you want to log into? GitHub.com\n? What is your preferred protocol for Git operations on this host? HTTPS \n? Authenticate Git with your GitHub credentials? Yes\n? How would you like to authenticate GitHub CLI?  [Use arrows to move, type to filter]\n    Login with a web browser\n&gt;   Paste an authentication token\n\nSuccessfully logged in to Github.\n</code></pre> </li> </ol> <p>Now the jumphost VM is ready for deploying our app. We will do this in the next section.</p>"},{"location":"llmmgmt/llm_pre_reqs/#reserve-ingress-and-istio-endpoint-ips","title":"Reserve Ingress and Istio Endpoint IPs","text":"<p>Nutanix AHV IPAM network allows you to black list IPs that needs to be reserved for specific application endpoints. We will use this feature to find and reserve four IPs. </p> <p>We will need a total of four IPs for the following:</p> Cluster Role Cluster Name Ingress IP Istio  IP Management <code>mgmt-cluster</code> 1 1 Dev <code>dev-cluster</code> 1 1 <ol> <li> <p>Get the CIDR range for the AHV network(subnet) where the application will be deployed</p> CIDR example for your Nutanix cluster<pre><code>10.x.x.0/24\n</code></pre> </li> <li> <p>From VSC, logon to your jumpbox VM (if not already done)</p> </li> <li> <p>Open Terminal</p> </li> <li> <p>Install <code>nmap</code> tool (if not already done)</p> <pre><code>cd $HOME/nai-llm-fleet-infra\ndevbox add nmap\n</code></pre> </li> <li> <p>Find four unused static IP addresses in the subnet</p> Template commandSample command <pre><code>nmap -v -sn  &lt;your CIDR&gt;\n</code></pre> <pre><code>nmap -v -sn 10.x.x.0/24\n</code></pre> Sample output - choose the first four consecutive IPs<pre><code>Nmap scan report for 10.x.x.214 [host down]\nNmap scan report for 10.x.x.215 [host down]\nNmap scan report for 10.x.x.216 [host down]\nNmap scan report for 10.x.x.217 [host down]\nNmap scan report for 10.x.x.218\nHost is up (-0.098s latency).\n</code></pre> </li> <li> <p>Logon to any CVM in your Nutanix cluster and execute the following to add chosen static IPs to the AHV IPAM network</p> <ul> <li>Username: nutanix</li> <li>Password: your Prism Element password </li> </ul> Template commandSample command <pre><code>acli net.add_to_ip_blacklist &lt;your-ipam-ahv-network&gt; \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216,10.x.x.217\n</code></pre> <pre><code>acli net.add_to_ip_blacklist User1 \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216,10.x.x.217\n</code></pre> </li> </ol>"},{"location":"llmmgmt/llm_pre_reqs/#create-nginx-ingress-and-istio-vipfdqn","title":"Create Nginx Ingress and Istio VIP/FDQN","text":"<p>We will use nip.io address to assign FQDNs for our Nginx Ingress and Istio by using the 4 IPs that we just reserved in the previous section for use in the next section. We will leverage the <code>NIP.IO</code> with the vip and self-signed certificates.</p> <p>We will need a total of four IPs for the following:</p>"},{"location":"llmmgmt/llm_pre_reqs/#management-cluster","title":"Management Cluster","text":"<p>Assign the first two reserved IPs to Management cluster.</p> Component Sub-component IP/FQDN Ingress <code>vip</code> <code>10.x.x.214</code> Ingress  Wildcard <code>wildcard_ingress_subdomain</code> <code>mgmt-cluster.10.x.x.214.nip.io</code> Ingress  Subdomain <code>management_cluster_ingress_subdomain</code> <code>mgmt-cluster.10.x.x.214.nip.io</code> Reserved for future <code>troubleshooting or debugging</code> <code>10.x.x.215</code> <p>Note</p> <p>We only need 1 IP for Management cluster. However, KubeVIP needs a range of at least two IPs. We will reserve the second IP for future use and/or troubleshooting purposes. </p>"},{"location":"llmmgmt/llm_pre_reqs/#dev-cluster","title":"Dev Cluster","text":"<p>Assign the next two reserved IPs to Dev cluster.</p> <p>Note</p> <p>The <code>management_cluster_ingress_subdomain</code> appears in this table once again and it is just a reference for <code>dev-cluster</code> to <code>mgmt-cluster</code>. This entry will be used in the <code>.env.dev-cluster.yaml</code> file during the Deploy Dev Cluster section.</p> Component Sub-component IP/FQDN Nginx Ingress <code>vip</code> <code>10.x.x.216</code> Nginx Ingress <code>wildcard_ingress_subdomain</code> <code>dev-cluster.10.x.x.216.nip.io</code> Nginx Ingress <code>management_cluster_ingress_subdomain</code> <code>mgmt-cluster.10.x.x.214.nip.io</code> Istio <code>vip</code> <code>10.x.x.217</code> Istio <code>wildcard_ingress_subdomain</code> <code>dev-cluster.10.x.x.217.nip.io</code>"},{"location":"llmmgmt/llm_pre_reqs/#create-buckets-in-nutanix-objects","title":"Create Buckets in Nutanix Objects","text":"<p>We will create access keys to buckets that we will be using in the project.</p>"},{"location":"llmmgmt/llm_pre_reqs/#generating-access-keys-for-buckets","title":"Generating Access Keys for Buckets","text":"<p>Note</p> <p>Follow instructions here to create a Nutanix Objects Store (if you do not have it)</p> <p>We are assuming that the name of the Objects Store is <code>ntnx-objects</code>.</p> <ol> <li> <p>Go to Prism Central &gt; Objects &gt; ntnx-objects</p> </li> <li> <p>On the right-hand pane, click on Access Keys</p> </li> <li> <p>Click on + Add people</p> </li> <li> <p>Select Add people not in a directory service</p> </li> <li> <p>Enter an email <code>llm-admin@example.com</code> and name <code>llm-admin</code></p> </li> <li> <p>Click on Next</p> </li> <li> <p>Click on Generate Keys</p> </li> <li> <p>Once generated, click on Download Keys</p> </li> <li> <p>Once downloaded, click on Close</p> </li> <li> <p>Open the downloaded file to verify contents</p> <pre><code>Username: llm-admin@example.com\nAccess Key: 1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nSecret Key: gxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nDisplay Name: llm-admin\n</code></pre> </li> <li> <p>Store the access key and secret key in a safe place for access </p> </li> </ol>"},{"location":"llmmgmt/llm_pre_reqs/#create-buckets","title":"Create Buckets","text":"<p>We will create buckets for Milvus database store and document store for uploaded files for querying will be stored.</p> <ol> <li> <p>On the top menu, click on Object Stores</p> </li> <li> <p>Click on ntnx-objects</p> </li> <li> <p>Click on Create Bucket</p> </li> <li> <p>Enter mgmt-cluster-milvus as the bucket name</p> </li> <li> <p>Click on Create</p> </li> <li> <p>Follow the same steps to create another bucket called documents01</p> </li> </ol>"},{"location":"llmmgmt/llm_pre_reqs/#provide-access-to-buckets","title":"Provide Access to Buckets","text":"<ol> <li> <p>In the list of buckets, click on the mgmt-cluster-milvus bucket</p> </li> <li> <p>Click on User Access menu and Edit User Access</p> </li> <li> <p>In the mgmt-cluster-milvus window, type in the <code>llm-admin@example.com</code> email that you configured in the Generating Access Keys for Buckets section</p> </li> <li> <p>Give Full Access permissions</p> </li> <li> <p>Click on Save</p> </li> <li> <p>Follow the same steps to give Full Access to the <code>llm-admin@example.com</code> email for documents01 bucket</p> </li> </ol>"},{"location":"llmmgmt/llm_pre_reqs/#create-nutanix-files-share","title":"Create Nutanix Files Share","text":"<p>Create NFS share for hosting the LLM model file <code>llama-2-13b-chat</code> and model archive file</p> <p>Note</p> <p>Follow instructions here to create a Nutanix Files cluster (if you do not have it)</p> <p>We are assuming that the name of the Files cluster is <code>ntnx-files</code>.</p> <ol> <li> <p>Go to Prism Central &gt; Files &gt; ntnx-files</p> </li> <li> <p>Click on Shares &amp; Exports</p> </li> <li>Click on + New Share or Export</li> <li> <p>Enter the following details:</p> <ul> <li>Name - llm-model-store</li> <li>Enable compression - checked</li> <li>Authentication - system</li> <li>Default Access - Read-Write</li> <li>Squash - Root Squash</li> </ul> </li> <li> <p>Click on Create</p> </li> <li> <p>Copy the Share/Export Path from the list of shares and note it down for later use (e.g: <code>/llm-model-store</code>)</p> </li> </ol>"},{"location":"llmmgmt/llm_pre_reqs/#extract-the-model-archive-file-to-files-share","title":"Extract the Model Archive file to Files Share","text":"<p>The LLM application will use the model archive file (MAR) stored in the file share. A few commands need to be executed to download and extract the model file from Hugging Face to the Files share. </p> <p>Note</p> <p>The following steps are directly from opendocs.nutanix.com GPT-in-a-Box documentation.</p> <ol> <li> <p>Logon to the jumphost VM you created in the previous section</p> <pre><code>ssh -l ubuntu &lt;jumphost vm IP&gt;\n</code></pre> </li> <li> <p>Download nutanix package and extract it. </p> <pre><code>curl -LO https://github.com/nutanix/nai-llm-k8s/archive/refs/tags/v0.2.2.tar.gz\ntar xvf v0.2.2.tar.gz --strip-components=1\n</code></pre> </li> <li> <p>Install pip</p> <pre><code>sudo apt-get install python3-pip\n</code></pre> </li> <li> <p>Install the python library requirements</p> <pre><code>cd llm\npip install -r requirements.txt\n</code></pre> </li> <li> <p>Mount the file share created in the previous section</p> Template commandExample command <pre><code>sudo mount -t nfs &lt;files server fqdn&gt;:&lt;share path&gt; &lt;NFS_LOCAL_MOUNT_LOCATION&gt;\n</code></pre> <pre><code>sudo mount -t nfs ntnx-files.pe.example.com:/llm-model-store /mnt/llm-model-store\n</code></pre> </li> <li> <p>Download and extract the model file to the local mount of file share</p> Template commandExample command <pre><code>python3 generate.py [--hf_token &lt;HUGGINGFACE_HUB_TOKEN&gt; \\\n--repo_version &lt;REPO_COMMIT_ID&gt;] --model_name &lt;MODEL_NAME&gt; \\\n--output &lt;NFS_LOCAL_MOUNT_LOCATION&gt;\n</code></pre> <pre><code>python3 generate.py --model_name llama2_7b_chat \\\n--output /mnt/llm-model-store \\\n--hf_token hf_xxxxxxxxxxxxxxxxxxxxxxxxxxx \n</code></pre> <pre><code># Sample output\n\n## Starting model files download\n\nDeleted all contents from '/mnt/llm-model-store/llama2_7b_chat/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/download' \n\nThe new directory is created! - /mnt/llm-model-store/llama2_7b_chat/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/download \n\nThe new directory is created! - /mnt/llm-model-store/llama2_7b_chat/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/download/tmp_hf_cache \n\ngeneration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 188/188 [00:00&lt;00:00, 1.15MB/s]\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 614/614 [00:00&lt;00:00, 7.51MB/s]\nLICENSE.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.02k/7.02k [00:00&lt;00:00, 81.3MB/s]\nUSE_POLICY.md: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.77k/4.77k [00:00&lt;00:00, 11.2MB/s]\n.gitattributes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.52k/1.52k [00:00&lt;00:00, 10.1MB/s]\nREADME.md: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4k/10.4k [00:00&lt;00:00, 113MB/s]\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.62k/1.62k [00:00&lt;00:00, 13.5MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 414/414 [00:00&lt;00:00, 1.22MB/s]\nmodel.safetensors.index.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.8k/26.8k [00:00&lt;00:00, 13.5MB/s]\npytorch_model.bin.index.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.8k/26.8k [00:00&lt;00:00, 12.4MB/s]\ntokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k/500k [00:00&lt;00:00, 6.12MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.84M/1.84M [00:00&lt;00:00, 7.42MB/s]\nmodel-00002-of-00002.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.50G/3.50G [00:23&lt;00:00, 149MB/s]\nmodel-00001-of-00002.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.98G/9.98G [01:01&lt;00:00, 163MB/s]\nFetching 14 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [01:02&lt;00:00,  4.47s/it]\nDeleted all contents from '/mnt/llm-model-store/llama2_7b_chat/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/download/tmp_hf_cache' MB/s]\n\n## Successfully downloaded model_files\n\n\n## Generating MAR file for custom model files: llama2_7b_chat \n\nThe new directory is created! - /mnt/llm-model-store/llama2_7b_chat/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/model-store \n\n## Generating MAR file, will take few mins.\n## Successfully generated MAR files\n\n## Generating MAR file, will take few mins.\n\nModel Archive File is Generating...\n\nCreating Model Archive:  42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                      | 4.97G/11.7G [10:40&lt;12:56, 8.69MB/s]Creating Model Archive: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.7G/11.7G [21:01&lt;00:00, 9.29MB/s]\n\nModel Archive file size: 9.66 GB\n\n## llama2_7b_chat.mar is generated.\n\nThe new directory is created! - /mnt/llm-model-store/llama2_7b_chat/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/config \n</code></pre> </li> </ol>"},{"location":"llmmgmt/llm_pre_reqs/#prepare-github-repository-and-api-token","title":"Prepare Github Repository and API Token","text":"<p>We need to fork this projects Github repository to you github organization(hadle). This repository will be used to hold the flux files for GitOps sections. </p> <p>A <code>repo_api_token</code> needs to be created to allow for git changes.</p> <ol> <li> <p>Log in to GitHub:    go to GitHub and log in to your account.</p> </li> <li> <p>Fork the following source repository    <pre><code>https://github.com/jesse-gonzalez/nai-llm-fleet-infra\n</code></pre></p> </li> <li> <p>Access Settings:    click on your profile picture in the top-right corner and select Settings from the dropdown menu.</p> </li> <li> <p>Developer Settings:    scroll down in the left sidebar and click on Developer settings.</p> </li> <li> <p>Personal Access Tokens:    in the left sidebar, click on Personal access tokens and then Tokens (classic).</p> </li> <li> <p>Generate New Token:    click on the Generate new token button. You might need to re-enter your GitHub password.</p> </li> <li> <p>Configure Token:</p> <ul> <li>Give your token a descriptive name: <code>for nai-llm-fleet-infra actions</code></li> <li>Expiration: choose an expiration period of <code>7 days</code></li> <li>Scopes: </li> <li>Select <code>repo</code> (and every option under it) </li> <li>Select <code>write:packages</code> and <code>read:packages</code></li> <li>Select <code>admin:org</code> and <code>read:org</code> under it</li> <li>Select <code>gist</code></li> </ul> </li> <li> <p>Generate Token:    after selecting the scopes, click on the Generate token button at the bottom of the page.</p> </li> <li> <p>Copy Token:    GitHub will generate the token and display it. Copy this token and store it securely. It can't be seen again.</p> </li> </ol> Single repository access? <p>Restricting Token to a Single Repository</p> <p>GitHub's PATs are account-wide and cannot be restricted to a single repository directly. However, you can control access using repository permissions and organizational policies. Here are some options:</p> <ul> <li> <p>Create a Dedicated GitHub User:</p> <p>Create a new GitHub user specifically for accessing the repository. Invite this user to your repository with the necessary permissions, then generate a PAT for this user.</p> </li> </ul> <p>We will use this token in the next section. </p>"},{"location":"llmmgmt/llm_pre_reqs/#prepare-docker-hub-credentials","title":"Prepare Docker Hub Credentials","text":"<p>Docker hub credentials are required to prevent rate limits on image downloads. </p> <p>If you do not have docker account, please create it here. </p> <p>Store the docker username and password securely for use in the next section.</p> <p>We are now ready to deploy the LLM app.</p>"},{"location":"llmmgmt/llm_test/","title":"Deploying GPT-in-a-Box NVD Reference Application using GitOps (FluxCD)","text":"<pre><code>stateDiagram-v2\n    direction LR\n\n    state TestLLMApp {\n        [*] --&gt; CheckInferencingService\n        CheckInferencingService --&gt;  TestFrontEndApp\n        TestFrontEndApp --&gt; TestRAG\n        TestRAG --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployLLMV1 \n    DeployLLMV1 --&gt; TestLLMApp : previous section\n    TestLLMApp --&gt; [*]</code></pre>"},{"location":"llmmgmt/llm_test/#accessing-llm-frontend","title":"Accessing LLM Frontend","text":"<p>Once the bootstrapping is done in the previous section. We can access and test our LLM application.</p> <ol> <li> <p>In VSC Terminal, check the status of inferencing service</p> <p><pre><code>kubectl get isvc -A\n</code></pre> <pre><code>NAMESPACE   NAME      URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllm         llm-llm   http://llm-llm.llm.dev-cluster.10.x.x.217.nip.io   True           100                              llm-llm-predictor-00001   4h9m\n</code></pre></p> </li> <li> <p>Access the URL to check status and make sure it is alive and well </p> <p><pre><code>$ curl http://llm-llm.llm.dev-cluster.10.x.x.217.nip.io\n</code></pre> <pre><code>$ curl http://llm-llm.llm.dev-cluster.10.x.x.217.nip.io\n{\"status\":\"alive\"} \n</code></pre></p> </li> <li> <p>On VSC terminal, get the LLM Frontend ingress endpoints</p> <pre><code>kubectl get ingress -A | grep frontend\n</code></pre> <pre><code>kubectl get ingress -A | grep frontend\nNAMESPACE              NAME                                                      CLASS   HOSTS                                      ADDRESS        PORTS     AGE\ngptnvd-reference-app   gptnvd-reference-app-gptnvd-ref-app-gptnvd-referenceapp   nginx   frontend.dev-cluster.10.x.x.216.nip.io   10.x.x.216   80, 443   4h9m      \n</code></pre> </li> <li> <p>Copy the HOSTS address <code>frontend.dev-cluster.10.x.x.216.nip.io</code> from the above output and paste it in your browser. You should be able to see the LLM chat interface. Start asking away. </p> <p></p> </li> </ol>"},{"location":"llmmgmt/llm_test/#testing-llm-frontend-chat-app","title":"Testing LLM Frontend Chat App","text":"<ol> <li> <p>Type any question in the chat box. For example: give me a python program to print the fibonacci series?</p> <p></p> </li> </ol>"},{"location":"nai/","title":"Getting Started","text":"<p>In this part of the lab we will deploy LLM on GPU nodes.</p> <p>We will also deploy a Kubernetes (NKP) cluster so far as per the NVD design requirements.</p> <p>NKP cluster: to host the dev LLM and ChatBot application - this will use GPU passed through to the kubernetes worker node.</p> <p>Deploy the kubernetes cluster with the following components:</p> <ul> <li>3 x Control plane nodes</li> <li>4 x Worker nodes </li> <li>1 x GPU node (with a minimum of 40GB of RAM and 16 vCPUs based on <code>llama3-8B</code> LLM model)</li> </ul> <p>We will deploy the NAI NVD Reference App - backed by <code>llama3-8B</code> model.</p> <p>The following is the flow of the NAI lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKP {\n        [*] --&gt; CreateNkpMachineImage\n        CreateNkpMachineImage --&gt; CreateNkpSelfManagedCluster\n        CreateNkpSelfManagedCluster --&gt; DeployGPUNodePool\n        DeployGPUNodePool --&gt; [*]\n    }\n\n    state NAIPreRequisites {\n        [*] --&gt; ReserveIPs\n        ReserveIPs --&gt; CreateFilesShare\n        CreateFilesShare --&gt; [*]\n    }\n\n    state DeployNAI {\n        [*] --&gt; DeployNAIfromAppCatalog\n        DeployNAIfromAppCatalog --&gt; MonitorResourcesDeployment\n        MonitorResourcesDeployment --&gt; [*]\n    }\n\n    state TestLLMApp {\n        [*] --&gt; TestQueryLLM\n        TestQueryLLM --&gt; TestChatApp\n        TestChatApp --&gt; [*]\n    }\n\n    [*] --&gt; DeployNKP\n    DeployNKP --&gt; NAIPreRequisites\n    NAIPreRequisites --&gt; DeployNAI\n    DeployNAI --&gt; TestLLMApp\n    TestLLMApp --&gt; [*]</code></pre>"},{"location":"nai/deploy_nai/","title":"Deploying Nutanix Enterprise AI (NAI) NVD Reference Application","text":"<p>Version 2.1.0</p> <p>This version of the NAI deployment is based on the Nutanix Enterprise AI (NAI) <code>v2.1.0</code> release.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNAI {\n        [*] --&gt; DeployNAIAdmin\n        DeployNAIAdmin --&gt;  InstallSSLCert\n        InstallSSLCert --&gt; DownloadModel\n        DownloadModel --&gt; CreateNAI\n        CreateNAI --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI \n    DeployNAI --&gt; TestNAI : next section\n    TestNAI --&gt; [*]</code></pre>"},{"location":"nai/deploy_nai/#prepare-nai-docker-download-credentials","title":"Prepare NAI Docker Download Credentials","text":"<p>All NAI Docker images will be downloaded from the public Docker Hub registry. In order to download the images, you will need to logon to Nutanix Portal - NAI and create a Docker ID and access token.</p> <ol> <li>Login to Nutanix Portal - NAI using your credentials</li> <li>Click on Generate Access Token option</li> <li>Copy the generated Docker ID and access token</li> </ol> <p>Warning</p> <p>Currently there are issues with the Nutanix Portal to create a Docker ID and access token. This will be fixed soon.</p> <p>Click on the Manage Access Token option and use the credentials listed there until the Nutanix Portal is fixed.</p>"},{"location":"nai/deploy_nai/#deploy-nai","title":"Deploy NAI","text":"<p>In this section we will use the NKP Applications Catalog to deploy NAI. </p> <p>Warning</p> <p>NAI catalog has to be added manually for now to NKP Application Catalog. However future releases of NKP v2.15.0 and beyond may be released with NAI application ready to setup.</p> <ol> <li> <p>Add the <code>nutanix-apps-catalog</code> to NKP </p> CommandCommand output <pre><code>nkp create catalog nutanix-apps-catalog -w kommander-workspace --branch main \\\n--url https://github.com/nutanix-cloud-native/nkp-nutanix-product-catalog\n</code></pre> <pre><code>Catalog GitRepository nutanix-apps-catalog created. Use 'nkp edit gitrepository -n kommander nutanix-apps-catalog' to change its configuration.\n</code></pre> </li> <li> <p>Validate if the catalog is added. Make sure the status of the<code>nutanix-apps-catalog</code> is <code>Ready</code> as shown in the command output</p> CommandCommand output <pre><code>kubectl get gitrepo -A\n</code></pre> <pre><code>NAMESPACE        NAME                   URL                                                                                                   AGE     READY   STATUS\nkommander-flux   management             https://git-operator-git.git-operator-system.svc.cluster.local/repositories/kommander/kommander.git   5h47m   True    stored artifact for revision 'main@sha1:d5638636da96c752eac79b0d1710eea3f6b57215'\nkommander        nutanix-apps-catalog   https://github.com/nutanix-cloud-native/nkp-nutanix-product-catalog                                   2s      True    stored artifact for revision 'main@sha1:662482cce8cc5f22189e09a85a0dce5d69266145'\n</code></pre> </li> <li> <p>In the NKP GUI, Go to Clusters</p> </li> <li>Click on Management Cluster Workspace</li> <li>Go to Applications </li> <li>Search for Nutanix Enterprise AI</li> <li>Click on Enable</li> <li>Click on Configuration tab</li> <li> <p>Click on Workspace Application Configuration Override and paste the following yaml content</p> Template yamlSample yaml <pre><code>imagePullSecret:\n  # Name of the image pull secret\n  name: nai-iep-secret\n  # Image registry credentials\n  credentials:\n    registry: https://index.docker.io/v1/\n    username: _GA_release_docker_username\n    password: _GA_release_docker_password\n    email: _Email_associated_with_service_account\nstorageClassName: nai-nfs-storage\n</code></pre> <pre><code>imagePullSecret:\n  # Name of the image pull secret\n  name: nai-iep-secret\n  # Image registry credentials\n  credentials:\n    registry: https://index.docker.io/v1/\n    username: ntnxsvcgpt\n    password: dckr_pat_Yxxxxxxxxxxxxxxxxxxxx\n    email: first.last@company.com\nstorageClassName: nai-nfs-storage\n</code></pre> </li> <li> <p>Click on Enable on the top right-hand corner to enable Nutanix Enterprise AI</p> </li> <li> <p>Wait until Nutanix Enterprise AI operator is Enabled. </p> <p></p> </li> <li> <p>Verify that the NAI Core Pods are running and healthy</p> CommandCommand output <pre><code>kubens nai-system\nkubectl get po,deploy\n</code></pre> <pre><code>kubens nai-system\nkubectl get po,deploy\n\u2714 Active namespace is \"nai-system\"\nNAME                                            READY   STATUS      RESTARTS   AGE\npod/nai-api-5bc49f5445-2bsrj                    1/1     Running     0          119m\npod/nai-api-db-migrate-9z4s5-fsks5              0/1     Completed   0          119m\npod/nai-db-0                                    1/1     Running     0          119m\npod/nai-iep-model-controller-7fd4cfc5cc-cxh2g   1/1     Running     0          119m\npod/nai-ui-77b7b789d5-wf67d                     1/1     Running     0          119m\npod/prometheus-nai-0                            2/2     Running     0          119m\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nai-api                    1/1     1            1           119m\ndeployment.apps/nai-iep-model-controller   1/1     1            1           119m\ndeployment.apps/nai-ui                     1/1     1            1           119m\n</code></pre> </li> </ol>"},{"location":"nai/deploy_nai/#install-ssl-certificate","title":"Install SSL Certificate","text":"<p>In this section we will install SSL Certificate to access the NAI UI. This is required as the endpoint will only work with a ssl endpoint with a valid certificate. </p> <p>NAI UI is accessible using the Ingress Gateway.</p> <p>The following steps show how cert-manager can be used to generate a self signed certificate using the default selfsigned-issuer present in the cluster. </p> <p>If you are using Public Certificate Authority (CA) for NAI SSL Certificate</p> <p>If an organization generates certificates using a different mechanism then obtain the certificate + key and create a kubernetes secret manually using the following command:</p> <pre><code>kubectl -n istio-system create secret tls nai-cert --cert=path/to/nai.crt --key=path/to/nai.key\n</code></pre> <p>Skip the steps in this section to create a self-signed certificate resource.</p> <ol> <li> <p>Get the Ingress host using the following command:</p> <pre><code>INGRESS_HOST=$(kubectl get svc -n istio-system istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre> </li> <li> <p>Get the value of <code>INGRESS_HOST</code> environment variable</p> CommandCommand output <pre><code>echo $INGRESS_HOST\n</code></pre> <pre><code>10.x.x.216\n</code></pre> </li> <li> <p>We will use the command output e.g: <code>10.x.x.216</code> as the IP address for NAI as reserved in this section</p> </li> <li> <p>Construct the FQDN of NAI UI using nip.io and we will use this FQDN as the certificate's Common Name (CN).</p> Template URLSample URL <pre><code>nai.${INGRESS_HOST}.nip.io\n</code></pre> <pre><code>nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>Create the ingress resource certificate using the following command:</p> <pre><code>cat &lt;&lt; EOF | k apply -f -\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: nai-cert\n  namespace: istio-system\nspec:\n  issuerRef:\n    name: selfsigned-issuer\n    kind: ClusterIssuer\n  secretName: nai-cert\n  commonName: nai.${INGRESS_HOST}.nip.io\n  dnsNames:\n  - nai.${INGRESS_HOST}.nip.io\n  ipAddresses:\n  - ${INGRESS_HOST}\nEOF\n</code></pre> </li> <li> <p>Create the certificate using the following command</p> <pre><code>kubectl apply -f $HOME/airgap-nai/nai-cert.yaml\n</code></pre> </li> <li> <p>Patch the ingress gateway's IP address to the certificate file.</p> CommandCommand output <pre><code>kubectl patch  gateways.networking.istio.io -n knative-serving knative-ingress-gateway --type merge --patch-file=/dev/stdin &lt;&lt;EOF\nspec:\n  servers:\n  - hosts:\n    - '*'\n    port:\n      name: https\n      number: 443\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: nai-cert\nEOF\n</code></pre> <pre><code>gateway.networking.istio.io/knative-ingress-gateway patched \n</code></pre> </li> </ol>"},{"location":"nai/deploy_nai/#accessing-the-ui","title":"Accessing the UI","text":"<ol> <li> <p>In a browser, open the following URL to connect to the NAI UI</p> <pre><code>https://nai.10.x.x.216.nip.io\n</code></pre> </li> <li> <p>Change the password for the <code>admin</code> user</p> </li> <li> <p>Login using <code>admin</code> user and password.</p> <p></p> </li> </ol>"},{"location":"nai/deploy_nai/#download-model","title":"Download Model","text":"<p>We will download and user llama3 8B model which we sized for in the previous section.</p> <ol> <li>In the NAI GUI, go to Models</li> <li>Click on Import Model from Hugging Face</li> <li>Choose the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model</li> <li> <p>Input your Hugging Face token that was created in the previous section and click Import</p> </li> <li> <p>Provide the Model Instance Name as <code>Meta-Llama-3.1-8B-Instruct</code> and click Import</p> </li> <li> <p>Go to VSC Terminal to monitor the download</p> CommandCommand output <p>Get jobs in nai-admin namespace<pre><code>kubens nai-admin\n\nkubectl get jobs\n</code></pre> Validate creation of pods and PVC<pre><code>kubectl get po,pvc\n</code></pre> Verify download of model using pod logs<pre><code>kubectl logs -f _pod_associated_with_job\n</code></pre></p> <p>Get jobs in nai-admin namespace<pre><code>kubens nai-admin\n\n\u2714 Active namespace is \"nai-admin\"\n\nkubectl get jobs\n\nNAME                                       COMPLETIONS   DURATION   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-model-job   0/1           4m56s      4m56\n</code></pre> Validate creation of pods and PVC<pre><code>kubectl get po,pvc\n\nNAME                                             READY   STATUS    RESTARTS   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-model-job-9nmff   1/1     Running   0          4m49s\n\nNAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      VOLUMEATTRIBUTESCLASS   AGE\nnai-c0d6ca61-1629-43d2-b57a-9f-pvc-claim   Bound    pvc-a63d27a4-2541-4293-b680-514b8b890fe0   28Gi       RWX            nai-nfs-storage   &lt;unset&gt;                 2d\n</code></pre> Verify download of model using pod logs<pre><code>kubectl logs -f nai-c0d6ca61-1629-43d2-b57a-9f-model-job-9nmff \n\n/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.05 MB. The target location /data/model-files only has 0.00 MB free disk space.\nwarnings.warn(\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51.0k/51.0k [00:00&lt;00:00, 3.26MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.09M/9.09M [00:00&lt;00:00, 35.0MB/s]&lt;00:30, 150MB/s]\nmodel-00004-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.17G/1.17G [00:12&lt;00:00, 94.1MB/s]\nmodel-00001-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.98G/4.98G [04:23&lt;00:00, 18.9MB/s]\nmodel-00003-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.92G/4.92G [04:33&lt;00:00, 18.0MB/s]\nmodel-00002-of-00004.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [04:47&lt;00:00, 17.4MB/s]\nFetching 16 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [05:42&lt;00:00, 21.43s/it]:33&lt;00:52, 9.33MB/s]\n## Successfully downloaded model_files|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [04:47&lt;00:00, 110MB/s] \n\nDeleting directory : /data/hf_cache\n</code></pre></p> </li> <li> <p>Optional - verify the events in the namespace for the pvc creation </p> CommandCommand output <pre><code>k get events | awk '{print $1, $3}'\n</code></pre> <pre><code>$ k get events | awk '{print $1, $3}'\n\n3m43s Scheduled\n3m43s SuccessfulAttachVolume\n3m36s Pulling\n3m29s Pulled\n3m29s Created\n3m29s Started\n3m43s SuccessfulCreate\n90s   Completed\n3m53s Provisioning\n3m53s ExternalProvisioning\n3m45s ProvisioningSucceeded\n3m53s PvcCreateSuccessful\n3m48s PvcNotBound\n3m43s ModelProcessorJobActive\n90s   ModelProcessorJobComplete\n</code></pre> </li> </ol> <p>The model is downloaded to the Nutanix Files <code>pvc</code> volume.</p> <p>After a successful model import, you will see it in Active status in the NAI UI under Models menu</p> <p></p>"},{"location":"nai/deploy_nai/#create-and-test-inference-endpoint","title":"Create and Test Inference Endpoint","text":"<p>In this section we will create an inference endpoint using the downloaded model.</p> <ol> <li>Navigate to Inference Endpoints menu and click on Create Endpoint button</li> <li> <p>Fill the following details:</p> <ul> <li>Endpoint Name: <code>llama-8b</code></li> <li>Model Instance Name: <code>Meta-LLaMA-8B-Instruct</code></li> <li>Use GPUs for running the models : <code>Checked</code></li> <li>No of GPUs (per instance):</li> <li>GPU Card: <code>NVIDIA-L40S</code> (or other available GPU)</li> <li>No of Instances: <code>1</code></li> <li>API Keys: Create a new API key or use an existing one</li> </ul> </li> <li> <p>Click on Create</p> </li> <li> <p>Monitor the <code>nai-admin</code> namespace to check if the services are coming up</p> CommandCommand output <pre><code>kubens nai-admin\nkubectl get po,deploy\n</code></pre> <pre><code>kubens nai-admin\nget po,deploy\nNAME                                                     READY   STATUS        RESTARTS   AGE\npod/llama8b-predictor-00001-deployment-9ffd786db-6wkzt   2/2     Running       0          71m\n\nNAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/llama8b-predictor-00001-deployment   1/1     1            0           3d17h\n</code></pre> </li> <li> <p>Check the events in the <code>nai-admin</code> namespace for resource usage to make sure all </p> CommandCommand output <pre><code>kubectl get events -n nai-admin --sort-by='.lastTimestamp' | awk '{print $1, $3, $5}'\n</code></pre> <pre><code>$ kubectl get events -n nai-admin --sort-by='.lastTimestamp' | awk '{print $1, $3, $5}'\n\n110s FinalizerUpdate Updated\n110s FinalizerUpdate Updated\n110s RevisionReady Revision\n110s ConfigurationReady Configuration\n110s LatestReadyUpdate LatestReadyRevisionName\n110s Created Created\n110s Created Created\n110s Created Created\n110s InferenceServiceReady InferenceService\n110s Created Created\n</code></pre> </li> <li> <p>Once the services are running, check the status of the inference service</p> CommandCommand output <pre><code>kubectl get isvc\n</code></pre> <pre><code>kubectl get isvc\n\nNAME      URL                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION       AGE\nllama8b   http://llama8b.nai-admin.svc.cluster.local   True           100                              llama8b-predictor-00001   3d17h\n</code></pre> </li> </ol>"},{"location":"nai/infra_nkp_nai/","title":"Deploy NKP Clusters","text":"<p>This section will take you through install NKP(Kubernetes) on Nutanix cluster as we will be deploying AI applications on these kubernetes clusters.</p> <p>We will use the CAPI based deployment of NKP. This will automatically deploy the required infrastructure VMs for the cluster by connecting to Nutanix Cluster APIs. There is no requirement to use Terraform or or other IaC tools to deploy NKP.</p> <p>This section will expand to other available Kubernetes implementations on Nutanix.</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state DeployNKP {\n        [*] --&gt; CreateNkpMachineImage\n        CreateNkpMachineImage --&gt; CreateNKPCluster\n        CreateNKPCluster --&gt; GenerateLicense\n        GenerateLicense --&gt; InstallLicense\n        InstallLicense --&gt; DeployGpuNodePool\n        DeployGpuNodePool --&gt; EnableGpuOperator\n        EnableGpuOperator --&gt; [*]\n    }\n\n    PrepWorkstation --&gt; DeployJumpHost \n    DeployJumpHost --&gt; DeployNKP \n    DeployNKP --&gt; DeployNai : Next section</code></pre> <p>Deploying NKP Cluster</p> <p>This lab will focus on deploying NKP to host NAI workloads. However, the steps can also be used deploy a custom NKP deployment if that's the aim.</p> <p>Consider using NKP The Hard Way section to create a customized version of your NKP cluster.</p> <p>Once you have determined the resource requirements for a custom NKP deployment, modify the environment variables and values in the <code>.env</code> file to suit your resource needs for your NKP cluster.</p>"},{"location":"nai/infra_nkp_nai/#nkp-high-level-cluster-design","title":"NKP High Level Cluster Design","text":"<p>The <code>nkpdev</code> cluster will be hosting the LLM model serving endpoints and AI application stack. This cluster and will require a dedicated GPU node pool.</p>"},{"location":"nai/infra_nkp_nai/#sizing-requirements","title":"Sizing Requirements","text":"<p>Below are the sizing requirements needed to successfully deploy NAI on a NKP Cluster (labeled as <code>nkpdev</code>) and subsequently deploying single LLM inferencing endpoint on NAI using the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> LLM model.</p> Calculating GPU Resources Tips <p>The calculations below assume that you're already aware of how much memory is required to load target LLM model.</p> <p>For a general example:</p> <ul> <li>To host a 8b(illion) parameter model, multiply the parameter number by 2 to get minimum GPU memory requirements.    e.g. 16GB of GPU memory is required for 8b parameter model.</li> </ul> <p>So in the case of the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> model, you'll need a min. 16 GiB GPU vRAM available</p> <p>Below are additional sizing consideration \"Rule of Thumb\" for further calculating min. GPU node resources:</p> <ul> <li>For each GPU node will have 8 CPU cores, 24 GB of memory, and 300 GB of disk space.</li> <li>For each GPU attached to the node, add 16 GiB of memory.</li> <li>For each endpoint attached to the node, add 8 CPU cores.</li> <li>If a model needs multiple GPUs, ensure all GPUs are attached to the same worker node</li> <li>For resiliency, while running multiple instances of the same endpoint, ensure that the GPUs are on different worker nodes.</li> </ul> <p>Since we will be testing with the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> HuggingFace model, we will require a GPU with a min. of 24 GiB GPU vRAM available to support this demo.</p> <p>Note</p> <p>GPU min. vRAM should be 24 GB, such as NVIDIA L4 Model.</p> <p>Below are minimum requirements for deploying NAI on the NKP Demo Cluster.</p> Role No. of Nodes (VM) vCPU per Node Memory per Node Storage per Node Total vCPU Total Memory Control plane 3 4 16 GB 150 GB 12 48 GB Worker 4 8 32 GB 150 GB 32 128 GB GPU 1 16 40 GB 300 GB 16 40 GB Totals 60 216 GB"},{"location":"nai/infra_nkp_nai/#pre-requisites-for-nkp-deployment","title":"Pre-requisites for NKP Deployment","text":"<ol> <li>Existing Ubuntu Linux jumphost VM. See here for jumphost installation steps.</li> <li>Docker or Podman installed on the jumphost VM</li> <li>Nutanix PC is at least <code>2024.1</code></li> <li>Nutanix AOS is at least <code>6.5</code>,<code>6.8+</code></li> <li>Download and install <code>nkp</code> binary from Nutanix Portal</li> <li>Find and reserve 3 IPs for control plane and MetalLB access from AHV network</li> <li>Find GPU details from Nutanix cluster</li> <li>Create a base image to use with NKP nodes using <code>nkp</code> command</li> </ol>"},{"location":"nai/infra_nkp_nai/#install-nkp-binaries","title":"Install NKP Binaries","text":"<ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Downloads &gt; Nutanix Kubernetes Platform (NKP)</li> <li>Select NKP for Linux and copy the download link to the <code>.tar.gz</code> file</li> <li> <p>If you haven't already done so, Open new <code>VSCode</code> window on your jumphost VM</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, click on existing <code>$HOME</code> folder</p> </li> <li> <p>Click on New Folder  name it: <code>nkp</code></p> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>$HOME/nkp</code> folder</p> </li> <li> <p>On <code>VSCode</code> menu, select <code>Terminal</code> &gt; <code>New Terminal</code></p> </li> <li> <p>Browse to <code>nkp</code> directory</p> <pre><code>cd $HOME/nkp\n</code></pre> </li> <li> <p>Download and extract the NKP binary from the link you copied earlier</p> CommandSample command Paste the download URL within double quotes<pre><code>curl -o nkp_v2.15.0_linux_amd64.tar.gz \"_paste_download_URL_here\"\n</code></pre> <pre><code>curl -o nkp_v2.15.0_linux_amd64.tar.gz \"https://download.nutanix.com/downloads/nkp/v2.15.0/nkp_v2.15.0_linux_amd64.tar.gz?Expires=1729016864&amp;........\"\n</code></pre> <pre><code>tar xvfz nkp_v2.15.0_linux_amd64.tar\n</code></pre> </li> <li> <p>Move the <code>nkp</code> binary to a directory that is included in your <code>PATH</code> environment variable</p> <pre><code>sudo cp nkp /usr/local/bin/\n</code></pre> </li> <li> <p>Verify the <code>nkp</code> binary is installed correctly. Ensure the version is latest</p> <p>Note</p> <p>At the time of writing this lab nkp version is <code>v2.15.0</code></p> CommandCommand output <pre><code>nkp version\n</code></pre> <pre><code>$ nkp version\ndiagnose: v0.11.0\nimagebuilder: v2.15.0\nkommander: v2.15.0\nkonvoy: v2.15.0\nmindthegap: v1.16.0\nnkp: v2.15.0\n</code></pre> </li> </ol>"},{"location":"nai/infra_nkp_nai/#setup-docker-on-jumphost","title":"Setup Docker on Jumphost","text":"<p>If not already done, follow the steps in Setup Docker on Jumphost section. </p>"},{"location":"nai/infra_nkp_nai/#reserve-control-plane-and-metallb-ip","title":"Reserve Control Plane and MetalLB IP","text":"<p>Nutanix AHV IPAM network allows you to black list IPs that needs to be reserved for specific application endpoints. We will use this feature to find and reserve three IPs. </p> <p>We will reserve a total of three IPs for the following:</p> Cluster Role Cluster Name NKP NAI Dev <code>nkpdev</code> 2 1 <ol> <li> <p>Get the CIDR range for the AHV network(subnet) where the application will be deployed</p> CIDR example for your Nutanix cluster<pre><code>10.x.x.0/24\n</code></pre> </li> <li> <p>From VSC, logon to your jumpbox VM and open Terminal</p> </li> <li> <p>Install <code>nmap</code> tool (if not already done)</p> <pre><code>cd $HOME/sol-cnai-infra\ndevbox add nmap\n</code></pre> </li> <li> <p>Find three unused static IP addresses in the subnet</p> CommandSample command <pre><code>nmap -v -sn  &lt;your CIDR&gt;\n</code></pre> <pre><code>nmap -v -sn 10.x.x.0/24\n</code></pre> Sample output - choose the first three consecutive IPs<pre><code>Nmap scan report for 10.x.x.214 [host down]\nNmap scan report for 10.x.x.215 [host down]\nNmap scan report for 10.x.x.216 [host down]\nNmap scan report for 10.x.x.217\nHost is up (-0.098s latency).\n</code></pre> </li> <li> <p>Logon to any CVM in your Nutanix cluster and execute the following to add chosen static IPs to the AHV IPAM network</p> <ul> <li>Username: nutanix</li> <li>Password: your Prism Element password </li> </ul> CommandSample command <pre><code>acli net.add_to_ip_blacklist &lt;your-ipam-ahv-network&gt; \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> <pre><code>acli net.add_to_ip_blacklist User1 \\\nip_list=10.x.x.214,10.x.x.215,10.x.x.216\n</code></pre> </li> </ol>"},{"location":"nai/infra_nkp_nai/#reservation-of-ips","title":"Reservation of IPs","text":"<p>Reserve the firs IPs for NKP control plane  Reserve the second two IPs for MetalLB distributed load balancer - We will use one of these IP for NAI</p> <p>Reserve the third IP for NAI. We will use the NAI IP in the next NAI section to assign the FDQN and install SSL certificate.</p> Component IP FQDN NKP Control Plane VIP <code>10.x.x.214</code> - NKP MetalLB IP Range <code>10.x.x.215-10.x.x.216</code> - NAI <code>10.x.x.216</code> <code>nai.10.x.x.216.nip.io</code>"},{"location":"nai/infra_nkp_nai/#create-base-image-for-nkp","title":"Create Base Image for NKP","text":"<p>About NKP Base Image OS Version on Nutanix Cluster</p> <p>The base image for NKP is a minimal image that contains the required packages and tools to run the Kubernetes cluster. The base image is used to create the worker node VMs and the control plane VMs.</p> <p>NKP base image can be <code>Rocky Linux 9.4</code> image and is part of <code>NKP Starter</code> license. This image is maintained and supported by Nutanix. The image is updated regularly to include the latest security patches and bug fixes. Customers should not modify the base image. </p> <p>Using <code>NKP Pro</code> license also offers choice of using <code>Ubuntu 22.04</code> base image for GPU based workload deployments.</p> <p>In this section we will go through creating a base image for all the control plane and worker node VMs on Nutanix. We will use the <code>Ubuntu 22.04</code> image as the base image as we will need GPU support for AI applications. NVIDIA GPU drivers are not yet available for <code>Rocky Linux 9.4</code> base image. </p> <p>NKP Cloud Support</p> <p>For information about other supported operating systems for Nutanix Kubernetes Platform (NKP), see NKP Cloud Support.</p> <ol> <li> <p>In VSC Explorer pane, Click on New Folder </p> </li> <li> <p>Call the folder <code>nkp</code> under <code>$HOME</code> directory</p> </li> <li> <p>In the <code>nkp</code> folder, click on New File  and create new file with the following name:</p> <pre><code>.env\n</code></pre> </li> <li> <p>Run the following command to generate an new RSA key pair on the jumphost VM. This SSH key pair will be used for authentication between the jumphost and NKP K8S cluster nodes.</p> Do you have existing SSH key pair? <p>Copy the key pair from your workstation (PC/Mac) to <code>~/.ssh/</code> directory on your Jumphost VM.</p> <pre><code>mac/pc $ scp ~/.ssh/id_rsa.pub ubuntu@10.x.x.171:~/.ssh/id_rsa.pub\nmac/pc $ scp ~/.ssh/id_rsa ubuntu@10.x.x.171:~/.ssh/id_rsa\n</code></pre> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>Accept the default file location as <code>~/.ssh/id_rsa</code></p> <p>SSH key pair will stored in the following location:</p> <pre><code>~/.ssh/id_rsa.pub \n~/.ssh/id_rsa\n</code></pre> </li> <li> <p>Fill the following values inside the <code>.env</code> file</p> Template .envSample .env <pre><code>export NUTANIX_USER=_your_nutanix_username\nexport NUTANIX_PASSWORD=_your_nutanix_password\nexport NUTANIX_ENDPOINT=_your_prism_central_fqdn\nexport NUTANIX_CLUSTER=_your_prism_element_cluster_name\nexport NUTANIX_SUBNET_NAME=_your_ahv_ipam_network_name\nexport STORAGE_CONTAINER=_your_storage_container_nmae\nexport SSH_PUBLIC_KEY=_path_to_ssh_pub_key_on_jumphost_vm\nexport NKP_CLUSTER_NAME=_your_nkp_cluster_name\nexport CONTROLPLANE_VIP=_your_nkp_cluster_controlplane_ip\nexport LB_IP_RANGE=_your_range_of_two_ips\n</code></pre> <pre><code>export NUTANIX_USER=admin\nexport NUTANIX_PASSWORD=xxxxxxxx\nexport NUTANIX_ENDPOINT=pc.example.com\nexport NUTANIX_CLUSTER=pe\nexport NUTANIX_SUBNET_NAME=User1\nexport STORAGE_CONTAINER=default\nexport SSH_PUBLIC_KEY=$HOME/.ssh/id_rsa.pub\nexport NKP_CLUSTER_NAME=nkpdev\nexport CONTROLPLANE_VIP=10.x.x.214\nexport LB_IP_RANGE=10.x.x.215-10.x.x.216\n</code></pre> </li> <li> <p>Using VSC Terminal, load the environment variables and its values</p> <pre><code>source $HOME/nkp/.env\n</code></pre> </li> <li> <p>Create the base image and upload to Prism Central using the following command. </p> <p>Note</p> <p>Image creation will take up to 5 minutes.</p> CommandExample CommandCommand output <pre><code>nkp create image nutanix ubuntu-22.04 \\\n  --endpoint ${NUTANIX_ENDPOINT} --cluster ${NUTANIX_CLUSTER} \\\n  --subnet ${NUTANIX_SUBNET_NAME} --insecure\n</code></pre> <pre><code>nkp create image nutanix ubuntu-22.04 \\\n  --endpoint pc.example.com --cluster pe \\\n  --subnet User1 --insecure\n</code></pre> <pre><code>nkp create image nutanix ubuntu-22.04 \\\n  --endpoint pc.example.com --cluster pe \\\n  --subnet User1 --insecure\n\n&gt; Provisioning and configuring image\nManifest files extracted to $HOME/nkp/.nkp-image-builder-3243021807\nnutanix.kib_image: output will be in this color.\n\n==&gt; nutanix.kib_image: Creating Packer Builder virtual machine...\n    nutanix.kib_image: Virtual machine nkp-ubuntu-22.04-1.29.6-20240717082720 created\n    nutanix.kib_image: Found IP for virtual machine: 10.x.x.234\n==&gt; nutanix.kib_image: Running post-processor: packer-manifest (type manifest)\n\n---&gt; 100%\nBuild 'nutanix.kib_image' finished after 4 minutes 55 seconds.\n==&gt; Wait completed after 4 minutes 55 seconds\n\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.29.6-20240717082720\n</code></pre> <p>Image name - This will be different in your environment</p> <p>Note image name from the previous <code>nkp</code> create image command output</p> <pre><code>==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix.kib_image: nkp-ubuntu-22.04-1.31.4-20250320042646\n</code></pre> <p>Warning</p> <p>Make sure to use image name that is generated in your environment for the next steps.</p> </li> <li> <p>Populate the <code>.env</code> file with the NKP image name by adding (appending) the following environment variables and save it</p> Template .envSample .env <pre><code>export NKP_IMAGE=nkp-image-name\n</code></pre> <pre><code>export NKP_IMAGE=nkp-ubuntu-22.04-1.31.4-20250320042646\n</code></pre> </li> </ol> <p>We are now ready to install the workload <code>nkpdev</code> cluster</p>"},{"location":"nai/infra_nkp_nai/#create-nkp-workload-cluster","title":"Create NKP Workload Cluster","text":"<ol> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export CONTROL_PLANE_REPLICAS=_no_of_control_plane_replicas\nexport CONTROL_PLANE_VCPUS=_no_of_control_plane_vcpus\nexport CONTROL_PLANE_CORES_PER_VCPU=_no_of_control_plane_cores_per_vcpu\nexport CONTROL_PLANE_MEMORY_GIB=_no_of_control_plane_memory_gib\nexport WORKER_REPLICAS=_no_of_worker_replicas\nexport WORKER_VCPUS=_no_of_worker_vcpus\nexport WORKER_CORES_PER_VCPU=_no_of_worker_cores_per_vcpu\nexport WORKER_MEMORY_GIB=_no_of_worker_memory_gib\nexport CSI_FILESYSTEM=_preferred_filesystem_ext4/xfs\nexport CSI_HYPERVISOR_ATTACHED=_true/false\nexport DOCKER_USERNAME=_your_docker_username\nexport DOCKER_PASSWORD=_your_docker_password\nexport NUTANIX_PROJECT_NAME=_your_pc_project_name\n</code></pre> <pre><code>export CONTROL_PLANE_REPLICAS=3\nexport CONTROL_PLANE_VCPUS=4\nexport CONTROL_PLANE_CORES_PER_VCPU=1\nexport CONTROL_PLANE_MEMORY_GIB=16\nexport WORKER_REPLICAS=4\nexport WORKER_VCPUS=8 \nexport WORKER_CORES_PER_VCPU=1\nexport WORKER_MEMORY_GIB=32\nexport CSI_FILESYSTEM=ext4\nexport CSI_HYPERVISOR_ATTACHED=true\nexport DOCKER_USERNAME=_your_docker_username\nexport DOCKER_PASSWORD=_your_docker_password\nexport NUTANIX_PROJECT_NAME=dev-lab\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source $HOME/nkp/.env\n</code></pre> </li> <li> <p>In VSC, open Terminal, enter the following command to create the workload cluster</p> Check your command for correct argument values <p>Run the following command to verify your <code>nkp</code> command and associated environment variables and values.</p> <pre><code>echo \"nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n        --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n        --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n        --control-plane-vm-image ${NKP_IMAGE} \\\n        --csi-storage-container ${STORAGE_CONTAINER} \\\n        --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n        --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n        --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n        --worker-vm-image ${NKP_IMAGE} \\\n        --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n        --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n        --control-plane-disk-size 150 \\\n        --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} \\\n        --control-plane-vcpus ${CONTROL_PLANE_VCPUS} \\\n        --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n        --worker-disk-size 150 \\\n        --worker-memory ${WORKER_MEMORY_GIB} \\\n        --worker-vcpus ${WORKER_VCPUS} \\\n        --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n        --csi-file-system ${CSI_FILESYSTEM} \\\n        --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n        --registry-mirror-url \"https://registry-1.docker.io\" \\\n        --registry-mirror-username ${DOCKER_USERNAME} \\\n        --registry-mirror-password ${DOCKER_PASSWORD} \\\n        --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n        --self-managed \\\n        --insecure\"\n</code></pre> <p>If the values are incorrect, add the correct values to <code>.env</code> and source the  again by running the following command</p> <pre><code>source $HOME/nkp/.env\n</code></pre> <p>Then rerun the <code>echo nkp</code> command to verify the values again before running the <code>nkp create cluster nutanix</code> command.</p> CommandCommand output <pre><code>nkp create cluster nutanix -c ${NKP_CLUSTER_NAME} \\\n    --control-plane-endpoint-ip ${CONTROLPLANE_VIP} \\\n    --control-plane-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --control-plane-subnets ${NUTANIX_SUBNET_NAME} \\\n    --control-plane-vm-image ${NKP_IMAGE} \\\n    --csi-storage-container ${STORAGE_CONTAINER} \\\n    --endpoint https://${NUTANIX_ENDPOINT}:9440 \\\n    --worker-prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --worker-subnets ${NUTANIX_SUBNET_NAME} \\\n    --worker-vm-image ${NKP_IMAGE} \\\n    --ssh-public-key-file ${SSH_PUBLIC_KEY} \\\n    --kubernetes-service-load-balancer-ip-range ${LB_IP_RANGE} \\\n    --control-plane-disk-size 150 \\\n    --control-plane-memory ${CONTROL_PLANE_MEMORY_GIB} \\\n    --control-plane-vcpus ${CONTROL_PLANE_VCPUS} \\\n    --control-plane-cores-per-vcpu ${CONTROL_PLANE_CORES_PER_VCPU} \\\n    --worker-disk-size 150 \\\n    --worker-memory ${WORKER_MEMORY_GIB} \\\n    --worker-vcpus ${WORKER_VCPUS} \\\n    --worker-cores-per-vcpu ${WORKER_CORES_PER_VCPU} \\\n    --csi-file-system ${CSI_FILESYSTEM} \\\n    --csi-hypervisor-attached-volumes=${CSI_HYPERVISOR_ATTACHED} \\\n    --registry-mirror-url \"https://registry-1.docker.io\" \\\n    --registry-mirror-username ${DOCKER_USERNAME} \\\n    --registry-mirror-password ${DOCKER_PASSWORD} \\\n    --control-plane-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --worker-pc-project ${NUTANIX_PROJECT_NAME} \\\n    --self-managed \\\n    --insecure\n</code></pre> <pre><code>&gt; \u2713 Creating a bootstrap cluster \n\u2713 Upgrading CAPI components \n\u2713 Waiting for CAPI components to be upgraded \n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Creating ClusterClass resources\n&gt; Generating cluster resources\ncluster.cluster.x-k8s.io/nkpdev created\nsecret/nkpdev-pc-credentials created\nsecret/nkpdev-pc-credentials-for-csi created\nsecret/nkpdev-image-registry-credentials created\n\u2713 Waiting for cluster infrastructure to be ready \n\u2713 Waiting for cluster control-planes to be ready \n\u2713 Waiting for machines to be ready\n\u2713 Initializing new CAPI components \n\u2713 Creating ClusterClass resources \n\u2713 Moving cluster resources\n\n&gt; You can now view resources in the moved cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdev.conf\" get nodes\n\n&gt; \u2713 Deleting bootstrap cluster \n\nCluster default/nkpdev kubeconfig was written to to the filesystem.\nYou can now view resources in the new cluster by using the --kubeconfig flag with kubectl.\nFor example: kubectl --kubeconfig=\"$HOME/nkp/nkpdev.conf\" get nodes\n\n&gt; Starting kommander installation\n\u2713 Deploying Flux \n\u2713 Deploying Ingress certificate \n\u2713 Creating kommander-overrides ConfigMap\n\u2713 Deploying Git Operator \n\u2713 Creating GitClaim for management GitRepository \n\u2713 Creating GitClaimUser for accessing management GitRepository \n\u2713 Creating HTTP Proxy configuration\n\u2713 Deploying Flux configuration\n\u2713 Deploying Kommander Operator \n\u2713 Creating KommanderCore resource \n\u2713 Cleaning up kommander bootstrap resources\n\u2713 Deploying Substitution variables\n\u2713 Deploying Flux configuration \n\u2713 Deploying Gatekeeper \n\u2713 Deploying Kommander AppManagement \n\u2713 Creating Core AppDeployments \n\u2713 4 out of 12 core applications have been installed (waiting for dex, dex-k8s-authenticator and 6 more) \n\u2713 5 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 5 more) \n\u2713 7 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander and 3 more) \n\u2713 8 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 2 more) \n\u2713 9 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, kommander-ui and 1 more) \n\u2713 10 out of 12 core applications have been installed (waiting for dex-k8s-authenticator, traefik-forward-auth-mgmt) \n\u2713 11 out of 12 core applications have been installed (waiting for traefik-forward-auth-mgmt) \n\u2713 Creating cluster-admin credentials\n\n&gt; Cluster was created successfully! Get the dashboard details with:\n&gt; nkp get dashboard --kubeconfig=\"$HOME/nkp/nkpdev.conf\"\n</code></pre> <p>What is a Self-Manged Cluster?</p> <p>The <code>--self-managed</code> argument of the <code>nkp create cluster nutanix</code> command will deploy bootstrap, and Kommander management automatically. </p> <p>The appendix section has information on how to deploy a cluster without using the <code>--self-managed</code> option. </p> <p>Usually preferred by customer DevOps teams to have more control over the deployment process. This way the customer can do the following:</p> <ul> <li>Deploy bootstrap (<code>kind</code>) cluster</li> <li>Deploy NKP Management cluster</li> <li>Choose to migrate the CAPI components over to NKP Management cluster</li> <li>Choose to customize Kommander Managment component instllation</li> <li>Choose to deploy workload clusters from NKP Kommander GUI or</li> <li>Choose to deploy workload clusters using scripts if they wish to automate the process</li> </ul> <p>See NKP the Hard Way section for more information for customizable NKP cluster deployments. </p> </li> <li> <p>Observe the events in the shell and in Prism Central events</p> </li> <li> <p>Store kubeconfig files for the workload cluster</p> <pre><code>nkp get kubeconfig -c ${NKP_CLUSTER_NAME} &gt; ${NKP_CLUSTER_NAME}.cfg\n</code></pre> </li> <li> <p>Combine the bootstrap and workload clusters <code>KUBECONFIG</code> file so that we can use it with <code>kubectx</code>command to change context between clusters</p> <pre><code>export KUBECONFIG=${NKP_CLUSTER_NAME}.cfg\n</code></pre> </li> <li> <p>Run the following command to check K8S status of the <code>nkpdev</code> cluster</p> CommandCommand output <pre><code>kubectl get nodes\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                  STATUS   ROLES           AGE     VERSION\nnkpdev-md-0-x948v-hvxtj-9r698           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-fb75c           Ready    &lt;none&gt;          4h50m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-mdckn           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-md-0-x948v-hvxtj-shxc8           Ready    &lt;none&gt;          4h49m   v1.29.6\nnkpdev-r4fwl-8q4ch                      Ready    control-plane   4h50m   v1.29.6\nnkpdev-r4fwl-jf2s8                      Ready    control-plane   4h51m   v1.29.6\nnkpdev-r4fwl-q888c                      Ready    control-plane   4h49m   v1.29.6\n</code></pre> </li> </ol>"},{"location":"nai/infra_nkp_nai/#add-nkp-gpu-workload-pool","title":"Add NKP GPU Workload Pool","text":"<p>Are you just deploying NKP?</p> <p>If you are doing this lab only to deploy NKP, then you can skip this GPU section.</p> <p>The steps below covers the following:     - Retrieving and Applying NKP Pro License     - Identifying the GPU device name     - Deploying the GPU nodepool     - Enabling the NVIDIA GPU Operator</p> <p>Note</p> <p>To Enable the GPU Operator afterwards using the NKP Marketplace, a minimal NKP Pro license is required.</p>"},{"location":"nai/infra_nkp_nai/#find-gpu-device-details","title":"Find GPU Device Details","text":"<p>As we will be deploying Nutanix Enterprise AI (NAI) in the next section, we need to find the GPU details beforehand.</p> <p>Find the details of GPU on the Nutanix cluster while still connected to Prism Central (PC).</p> <ol> <li>Logon to Prism Central GUI</li> <li>On the general search, type GPUs</li> <li> <p>Click on the GPUs result</p> <p></p> </li> <li> <p><code>Lovelace 40s</code> is the GPU available for use</p> </li> <li>Use <code>Lovelace 40s</code> in the evironment variables in the next section.</li> </ol>"},{"location":"nai/infra_nkp_nai/#create-nkp-gpu-workload-pool","title":"Create NKP GPU Workload Pool","text":"<p>In this section we will create a nodepool to host the AI apps with a GPU.</p> <ol> <li> <p>Open .env file in VSC and add (append) the following environment variables to your <code>.env</code> file and save it</p> Template .envSample .env <pre><code>export GPU_NAME=_name_of_gpu_device_\nexport GPU_REPLICA_COUNT=_no_of_gpu_worker_nodes\nexport GPU_POOL=_name_of_gpu_pool\nexport GPU_NODE_VCPUS=_no_of_gpu_node_vcpus\nexport GPU_NODE_CORES_PER_VCPU=_per_gpu_node_cores_per_vcpu\nexport GPU_NODE_MEMORY_GIB=_per_gpu_node_memory_gib\nexport GPU_NODE_DISK_SIZE_GIB=_per_gpu_node_memory_gib\n</code></pre> <pre><code>export GPU_NAME=\"Lovelace 40S\"\nexport GPU_REPLICA_COUNT=1\nexport GPU_POOL=gpu-nodepool\nexport GPU_NODE_VCPUS=16\nexport GPU_NODE_CORES_PER_VCPU=1\nexport GPU_NODE_MEMORY_GIB=40\nexport GPU_NODE_DISK_SIZE_GIB=200\n</code></pre> </li> <li> <p>Source the new variables and values to the environment</p> <pre><code>source $HOME/nkp/.env\n</code></pre> </li> <li> <p>Run the following command to create a GPU nodepool manifest</p> <pre><code>nkp create nodepool nutanix \\\n    --cluster-name ${NKP_CLUSTER_NAME} \\\n    --prism-element-cluster ${NUTANIX_CLUSTER} \\\n    --pc-project ${NUTANIX_PROJECT_NAME} \\\n    --subnets ${NUTANIX_SUBNET_NAME} \\\n    --vm-image ${NKP_IMAGE} \\\n    --disk-size ${GPU_NODE_DISK_SIZE_GIB} \\\n    --memory ${GPU_NODE_MEMORY_GIB} \\\n    --vcpus ${GPU_NODE_VCPUS} \\\n    --cores-per-vcpu ${GPU_NODE_CORES_PER_VCPU} \\\n    --replicas ${GPU_REPLICA_COUNT} \\\n    --wait \\\n    ${GPU_POOL} --dry-run -o yaml &gt; gpu-nodepool.yaml\n</code></pre> <p>Note</p> <p>Right now there is no switch for GPU in <code>nkp</code> command. We need to do dry-run the output into a file and then add the necessary GPU specifications</p> </li> <li> <p>Add the necessary gpu section to our new <code>gpu-nodepool.yaml</code> using <code>yq</code> command</p> <pre><code>yq e '(.spec.topology.workers.machineDeployments[] | select(.name == \"gpu-nodepool\").variables.overrides[] | select(.name == \"workerConfig\").value.nutanix.machineDetails) += {\"gpus\": [{\"type\": \"name\", \"name\": strenv(GPU_NAME)}]}' -i gpu-nodepool.yaml\n</code></pre> Successful addtion of GPU specs? <p>You would be able to see the added gpu section at the end of the <code>gpu-nodepool.yaml</code> file</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\n\n&lt;snip&gt;\n\n  name: gpu-nodepool\n  variables:\n    overrides:\n      - name: workerConfig\n        value:\n          nutanix:\n            machineDetails:\n              bootType: legacy\n              cluster:\n                name: romanticism\n                type: name\n              image:\n                name: nkp-ubuntu-22.04-1.29.6-20240718055804\n                type: name\n              memorySize: 40Gi\n              subnets:\n                - name: User1\n                  type: name\n              systemDiskSize: 200Gi\n              vcpuSockets: 16\n              vcpusPerSocket: 1\n              gpus:\n                - type: name\n                  name: Lovelace 40S\n</code></pre> </li> <li> <p>Monitor Cluster-Api resources to ensure gpu machine will be successful</p> CommandScaling output <pre><code>watch kubectl get cluster-api\n</code></pre> <pre><code>NAME                                                          CLUSTER   REPLICAS   READY   UPDATED   UNA\nVAILABLE   PHASE       AGE    VERSION\nmachinedeployment.cluster.x-k8s.io/nkplb-gpu-nodepool-mpr4d   nkplb     1                  1         1\n           ScalingUp   12s    v1.31.4\nmachinedeployment.cluster.x-k8s.io/nkplb-md-0-d6cm7           nkplb     4          4       4         0\n           Running     159m   v1.31.4\n</code></pre> </li> <li> <p>Apply the <code>gpu-nodepool.yaml</code> file to the workload cluster</p> <pre><code>kubectl apply -f gpu-nodepool.yaml\n</code></pre> </li> <li> <p>Monitor the progress of the command and check Prism Central events for creation of the GPU worker node</p> <p>Change to workload <code>nkpdev</code> cluster context</p> <pre><code>kubectx ${NKP_CLUSTER_NAME}-admin@${NKP_CLUSTER_NAME}\n</code></pre> </li> <li> <p>Check nodes status in workload <code>nkpdev</code> cluster and note the gpu worker node</p> CommandCommand output <pre><code>kubectl get nodes -w\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                   STATUS   ROLES           AGE     VERSION\nnkpdev-gpu-nodepool-7g4jt-2p7l7-49wvd   Ready    &lt;none&gt;          5m57s   v1.29.6\nnkpdev-md-0-q679c-khl2n-9k7jk           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-md-0-q679c-khl2n-9nk6h           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-md-0-q679c-khl2n-nf9p6           Ready    &lt;none&gt;          73m     v1.29.6\nnkpdev-md-0-q679c-khl2n-qgxp9           Ready    &lt;none&gt;          74m     v1.29.6\nnkpdev-ncnww-2dg7h                      Ready    control-plane   73m     v1.29.6\nnkpdev-ncnww-bbm4s                      Ready    control-plane   72m     v1.29.6\nnkpdev-ncnww-hldm9                      Ready    control-plane   75m     v1.29.6\n</code></pre> </li> </ol>"},{"location":"nai/infra_nkp_nai/#licensing","title":"Licensing","text":"<p>We need to generate a license for the NKP cluster which is the total for all the vCPUs used by worker nodes.</p> <p>For example, in the Sizing Requirements section, the NKP Demo Cluster <code>Total vCPU count</code> is equal to <code>60</code>, whereas the actual worker nodes total vCPU count is only <code>48</code>.</p>"},{"location":"nai/infra_nkp_nai/#generate-nkp-pro-license","title":"Generate NKP Pro License","text":"<p>To generate a NKP Pro License for the NKP cluster:</p> <p>Note</p> <p>Nutanix Internal users should logon using Nutanix SSO</p> <p>Nutanix Partners/Customers should logon to Portal using their Nutanix Portal account credentials</p> <ol> <li>Login to Nutanix Portal using your credentials</li> <li>Go to Licensing &gt; License Summary</li> <li>Click on the small drop down arrow  on Manage Licenses and choose Nutanix Kubernetes Platform (NKP)</li> <li>Input the NKP cluster name</li> <li>Click on the plus icon </li> <li>Click on Next in the bottom right corner</li> <li>Select NKP Pro License</li> <li>Select Apply to cluster</li> <li>Choose Non-production license and Save</li> <li>Select the cluster name and click on Next</li> <li>Input the number of vCPU (<code>60</code>) from our calculations in the previous section</li> <li>Click on Save</li> <li>Download the csv file and store it in a safe place</li> </ol>"},{"location":"nai/infra_nkp_nai/#applying-nkp-pro-license-to-nkp-cluster","title":"Applying NKP Pro License to NKP Cluster","text":"<ol> <li> <p>Login to the Kommander URL for <code>nkpdev</code> cluster with the generated credentials that was generated in the previous section. The following commands will give you the credentials and URL.</p> CommandCommand output <pre><code>nkp get dashboard\n</code></pre> <pre><code>nkp get dashboard\n\nUsername: recursing_xxxxxxxxx\nPassword: YHbPsslIDB7p7rqwnfxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nURL: https://10.x.x.215/dkp/kommander/dashboard\n</code></pre> </li> <li> <p>Go to Licensing and click on Remove License to remove the Starter license</p> </li> <li>Type nutanix-license in the confirmation box and click on Remove License</li> <li>Click on Add License, choose Nutanix platform and paste the license key from the previous section</li> <li>Click on Save</li> <li>Confirm the license is applied to the cluster by cheking the License Status in the License menu</li> <li>The license will be applied to the cluster and the license status will reflect NKP Pro in the top right corner of the dashboard</li> </ol>"},{"location":"nai/infra_nkp_nai/#enable-nke-operators","title":"Enable NKE Operators","text":"<p>Enable these NKE Operators from NKP GUI.</p> <p>Note</p> <p>In this lab, we will be using the Management Cluster Workspace to deploy our Nutanix Enterprise AI (NAI)</p> <p>However, in a customer environment, it is recommended to use a separate workload NKP cluster.</p> <ol> <li>In the NKP GUI, Go to Clusters</li> <li>Click on Management Cluster Workspace</li> <li>Go to Applications </li> <li> <p>Search and enable the following applications: follow this order to install dependencies for NAI application</p> <ul> <li>Prometheus Monitoring: version <code>69.1.2</code> or later</li> <li>Prometheus Adapter: version <code>v4.11.0</code> or later</li> <li>Istio Service Mesh: version<code>1.20.8</code> or later</li> </ul> </li> <li> <p>The next application to enable is</p> <ul> <li> <p>Knative: version <code>v1.17.0</code> or later</p> </li> <li> <p>Search for Knative in the Applications</p> </li> <li> <p>Use the following configuration parameters in Workspace Configuration:</p> </li> </ul> <pre><code>serving:\n  config:\n    features:\n      kubernetes.podspec-nodeselector: enabled\n    autoscaler:\n      enable-scale-to-zero: false\n  knativeIngressGateway:\n    spec:\n      selector:\n        istio: ingressgateway\n      servers:\n      - hosts:\n        - '*'\n        port:\n          name: https\n          number: 443\n          protocol: HTTPS\n        tls:\n          mode: SIMPLE\n          credentialName: nai-cert # (1)\n</code></pre> <ol> <li>We will create this credential in the next section of the lab</li> </ol> </li> </ol> <p>Note</p> <p>It may take a few minutes for each application to be up and running. Monitor the deployment to make sure that these applications are running before moving on to the next section.</p>"},{"location":"nai/infra_nkp_nai/#gpu-operator","title":"GPU Operator","text":"<p>We will need to enable GPU operator for deploying NAI application. </p> <ol> <li>In the NKP GUI, Go to Clusters</li> <li>Click on Management Cluster Workspace</li> <li>Go to Applications </li> <li>Search for NVIDIA GPU Operator</li> <li>Click on Enable</li> <li>Click on Configuration tab</li> <li> <p>Click on Workspace Application Configuration Override and paste the following yaml content</p> <pre><code>driver:\n  enabled: true\n</code></pre> <p>As shown here:</p> <p></p> </li> <li> <p>Click on Enable on the top right-hand corner to enable GPU driver on the Ubuntu GPU nodes</p> </li> <li> <p>Check GPU operator resources and make sure they are running</p> CommandCommand output <pre><code>kubectl get po -A | grep -i nvidia\n</code></pre> <pre><code>kubectl get po -A | grep -i nvidia\n\nnvidia-container-toolkit-daemonset-fjzbt                          1/1     Running     0          28m\nnvidia-cuda-validator-f5dpt                                       0/1     Completed   0          26m\nnvidia-dcgm-exporter-9f77d                                        1/1     Running     0          28m\nnvidia-dcgm-szqnx                                                 1/1     Running     0          28m\nnvidia-device-plugin-daemonset-gzpdq                              1/1     Running     0          28m\nnvidia-driver-daemonset-dzf55                                     1/1     Running     0          28m\nnvidia-operator-validator-w48ms                                   1/1     Running     0          28m\n</code></pre> </li> <li> <p>Run a sample GPU workload to confirm GPU operations</p> CommandCommand output <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre> <pre><code>pod/cuda-vector-add created\n</code></pre> </li> <li> <p>Follow the logs to check if the GPU operations are successful</p> CommandSample CommandCommand output <pre><code>kubectl logs _gpu_worload_pod_name\n</code></pre> <pre><code>kubectl logs cuda-vector-add-xxx\n</code></pre> <pre><code>kubectl logs cuda-vector-add\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\n</code></pre> </li> </ol>"},{"location":"nai/infra_nkp_nai/#_1","title":"Deploy NKP Clusters","text":"<p>Now we are ready to deploy our AI workloads.</p>"},{"location":"nai/pre_reqs_nai/","title":"Pre-requisites for Deploying NAI","text":"<p>In this part of the lab we will prepare pre-requisites for LLM application on GPU nodes.</p> <p>The following is the flow of the applications lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state PreRequisites {\n        [*] --&gt; CreateFilesShare  \n        CreateFilesShare --&gt; PrepareHuggingFace\n        PrepareHuggingFace --&gt; [*]\n    }\n\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI : next section\n    DeployNAI --&gt; TestNAI\n    TestNAI --&gt; [*]</code></pre> <p>Prepare the following pre-requisites needed to deploy NAI on target kubernetes cluster.</p>"},{"location":"nai/pre_reqs_nai/#create-nutanix-files-storage-class","title":"Create Nutanix Files Storage Class","text":"<p>We will create Nutanix Files storage class which will be used to create a pvc that will store the <code>LLama-3-8B</code> model files.</p> <ol> <li> <p>Run the following command to check K8S status of the <code>nkpdev</code> cluster</p> <pre><code>kubectl get nodes\n</code></pre> </li> <li> <p>Add (append) the following environment variable to  <code>$HOME/nkp/.env</code> file</p> Template .envSample .env <pre><code>export FILES_CREDENTIALS_STRING='_prism_element_ip_addres:9440:admin:_your_password'\n</code></pre> <pre><code>export FILES_CREDENTIALS_STRING='10.x.x.37:9440:admin:password'\n</code></pre> </li> <li> <p>Create a secret for Nutanix Files CSI Driver</p> <pre><code>kubectl create secret generic nutanix-csi-credentials-files \\\n-n ntnx-system --from-literal=key=${FILES_CREDENTIALS_STRING} \\\n--dry-run -o yaml | kubectl apply -f -\n</code></pre> </li> <li> <p>In VSC Explorer, click on New File  and create a config file with the following name:</p> <pre><code>nai-nfs-storage.yaml\n</code></pre> <p>Add the following content and replace the <code>nfsServerName</code> with the name of the Nutanix Files server name .</p> <p></p> Template YAMLSample YAML <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n    name: nai-nfs-storage\nprovisioner: csi.nutanix.com\nparameters:\n  dynamicProv: ENABLED\n  nfsServerName: _your_nutanix_files_server_name\n  nfsServer: _your_nutanix_files_server_fqdn\n  csi.storage.k8s.io/provisioner-secret-name: nutanix-csi-credentials-files\n  csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system\n  csi.storage.k8s.io/node-publish-secret-name: nutanix-csi-credentials-files\n  csi.storage.k8s.io/node-publish-secret-namespace: ntnx-system\n  csi.storage.k8s.io/controller-expand-secret-name: nutanix-csi-credentials-files\n  csi.storage.k8s.io/controller-expand-secret-namespace: ntnx-system\n  storageType: NutanixFiles\nallowVolumeExpansion: true\n</code></pre> <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n    name: nai-nfs-storage\nprovisioner: csi.nutanix.com\nparameters:\n  dynamicProv: ENABLED\n  nfsServerName: labFS\n  nfsServer: labFS.ntnxlab.local\n  csi.storage.k8s.io/provisioner-secret-name: nutanix-csi-credentials-files\n  csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system\n  csi.storage.k8s.io/node-publish-secret-name: nutanix-csi-credentials-files\n  csi.storage.k8s.io/node-publish-secret-namespace: ntnx-system\n  csi.storage.k8s.io/controller-expand-secret-name: nutanix-csi-credentials-files\n  csi.storage.k8s.io/controller-expand-secret-namespace: ntnx-system\n  storageType: NutanixFiles\nallowVolumeExpansion: true\n</code></pre> </li> <li> <p>Create the storage class</p> <pre><code>kubectl apply -f nai-nfs-storage.yaml\n</code></pre> </li> <li> <p>Check storage classes in the cluster for the Nutanix Files storage class</p> CommandCommand output <pre><code>kubectl get storageclass\n</code></pre> <pre><code>kubectl get storageclass\n\nNAME                       PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ndkp-object-store           kommander.ceph.rook.io/bucket   Delete          Immediate              false                  28h\nnai-nfs-storage            csi.nutanix.com                 Delete          Immediate              true                   24h\nnutanix-volume (default)   csi.nutanix.com                 Delete          WaitForFirstConsumer   false                  28h\n</code></pre> </li> </ol>"},{"location":"nai/pre_reqs_nai/#request-access-to-model-on-hugging-face","title":"Request Access to Model on Hugging Face","text":"<p>Follow these steps to request access to the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model:</p> <p>LLM Recommendation</p> <p>From testing <code>google/gemma-2-2b-it</code> model is quicker to download and obtain download rights, than <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model.</p> <p>Feel free to use the google/gemma-2-2b-it model if necessary. The procedure to request access to the model is the same.</p> <ol> <li> <p>Sign in to your Hugging Face account:  </p> <ul> <li>Visit Hugging Face and log in to your account.</li> </ul> </li> <li> <p>Navigate to the model page:  </p> <ul> <li>Go to the Meta-Llama-3.1-8B-Instruct model page.</li> </ul> </li> <li> <p>Request access:</p> <ul> <li>On the model page, you will see a section or button labeled Request Access (this is usually near the top of the page or near the \"Files and versions\" section).</li> <li>Click Request Access.</li> </ul> </li> <li> <p>Complete the form:</p> <ul> <li>You may be prompted to fill out a form or provide additional details about your intended use of the model.</li> <li>Complete the required fields and submit the request.</li> </ul> </li> <li> <p>Wait for approval:</p> <ul> <li>After submitting your request, you will receive a notification or email once your access is granted.</li> <li>This process can take some time depending on the approval workflow.</li> </ul> </li> </ol> <p>Once access is granted, there will be an email notification.</p> <p>Note</p> <p>Email from Hugging Face can take a few minutes or hours before it arrives.</p>"},{"location":"nai/pre_reqs_nai/#create-a-hugging-face-token-with-read-permissions","title":"Create a Hugging Face Token with Read Permissions","text":"<p>Follow these steps to create a Hugging Face token with read permissions:</p> <ol> <li> <p>Sign in to your Hugging Face account:  </p> <ul> <li>Visit Hugging Face and log in to your account.</li> </ul> </li> <li> <p>Access your account settings:</p> <ul> <li>Click on your profile picture in the top-right corner.</li> <li>From the dropdown, select Settings.</li> </ul> </li> <li> <p>Navigate to the \"Access Tokens\" section:</p> <ul> <li>In the sidebar, click on Access Tokens.</li> <li>You will see a page where you can create and manage tokens.</li> </ul> </li> <li> <p>Create a new token:</p> <ul> <li>Click the New token button.</li> <li>Enter a name for your token (i.e., <code>read-only-token</code>).</li> </ul> </li> <li> <p>Set token permissions:</p> <ul> <li>Under the permissions dropdown, select Read. For Example:     </li> </ul> </li> <li> <p>Create and copy the token:</p> <ul> <li>After selecting the permissions, click Create.</li> <li>Your token will be generated and displayed only once, so make sure to copy it and store it securely.</li> </ul> </li> </ol> <p>Use this token for accessing Hugging Face resources with read-only permissions.</p>"},{"location":"nai/test_nai/","title":"Deploying GPT-in-a-Box NVD Reference Application using GitOps (FluxCD)","text":"<pre><code>stateDiagram-v2\n    direction LR\n\n    state TestNAI {\n        [*] --&gt; CheckInferencingService\n        CheckInferencingService --&gt;  TestChatApp\n        TestChatApp --&gt; [*]\n    }\n\n    [*] --&gt; PreRequisites\n    PreRequisites --&gt; DeployNAI \n    DeployNAI --&gt; TestNAI : previous section\n    TestNAI --&gt; [*]</code></pre>"},{"location":"nai/test_nai/#test-querying-inference-service-api","title":"Test Querying Inference Service API","text":"<ol> <li> <p>Prepare the API key that was created in the previous section</p> Template commandSample command <pre><code>export API_KEY=_your_endpoint_api_key\n</code></pre> <pre><code>export API_KEY=5840a693-254d-41ef-a2d3-1xxxxxxxxxx\n</code></pre> </li> <li> <p>Construct your <code>curl</code> command using the API key obtained above, and run it on the terminal</p> CommandCommand output <pre><code>curl -k -X 'POST' 'https://nai.10.x.x.216.nip.io/api/v1/chat/completions' \\\n-H \"Authorization: Bearer $API_KEY\" \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model\": \"llama-8b\",\n    \"messages\": [\n        {\n        \"role\": \"user\",\n        \"content\": \"What is the capital of France?\"\n        }\n    ],\n    \"max_tokens\": 256,\n    \"stream\": false\n}'\n</code></pre> <pre><code>{\n    \"id\": \"9e55abd1-2c91-4dfc-bd04-5db78f65c8b2\",\n    \"object\": \"chat.completion\",\n    \"created\": 1728966493,\n    \"model\": \"llama-8b\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"The capital of France is Paris. It is a historic city on the Seine River in the north-central part of the country. Paris is also the political, cultural, and economic center of France.\"\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 17,\n        \"completion_tokens\": 41,\n        \"total_tokens\": 58\n    },\n    \"system_fingerprint\": \"\"\n}\n</code></pre> </li> </ol> <p>We have a successful NAI deployment.</p>"},{"location":"nai/test_nai/#accessing-llm-frontend-ui","title":"Accessing LLM Frontend UI","text":"<ol> <li> <p>In the NAI GUI, under Endpoints, click on the llama8b</p> </li> <li> <p>Click on Test</p> </li> <li> <p>Provide a sample prompt and check the output</p> <p></p> </li> </ol>"},{"location":"nai/test_nai/#sample-chat-application","title":"Sample Chat Application","text":"<p>Nutanix also provides a sample chat application that uses NAI to provide chatbot capabilities. We will install and use the chat application in this section. </p> <ol> <li> <p>Run the following command to deploy the chat application.</p> <pre><code>code $HOME/sol-cnai-infra/scripts/nai/chat.yaml\n</code></pre> </li> <li> <p>Change this line to point to the IP address of your NAI cluster for the <code>VirtualService</code> resource</p> </li> <li> <p>Insert <code>chat</code> as the subdomain in the <code>nai.10.x.x.216.nip.io</code> main domain.</p> <p>Example: complete URL</p> <pre><code>chat.nai.10.x.x.216.nip.io\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nai-chat\nspec:\n  gateways:\n  - knative-serving/knative-ingress-gateway\n  hosts:\n  - chat.nai.10.x.x.216.nip.io\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nai-chatapp\n        port:\n        number: 8502\n</code></pre> </li> <li> <p>We should be able to see the chat application running on the NAI cluster.</p> <p></p> </li> <li> <p>Input the endpoint URL and API key to start chatting with the LLM.</p> </li> </ol> <p>We have successfully deployed the following:</p> <ul> <li>Inferencing endpoint</li> <li>A sample chat application that uses NAI to provide chatbot capabilities</li> </ul>"},{"location":"nkp_ndk/","title":"Nutanix Data Services for Kubernetes (NDK) Lab Guide","text":""},{"location":"nkp_ndk/#introduction","title":"Introduction","text":"<p>Nutanix Data Services for Kubernetes (NDK) simplifies the management of stateful applications on Kubernetes by providing robust data protection, replication, and recovery capabilities. This lab guide is designed to help you explore NDK's features through hands-on exercises, covering setup, snapshot management, cross-namespace operations, and multi-cluster replication. The labs progress from single-cluster to multi-cluster workflows, including new features introduced in NDK <code>1.3</code>, such as support for Read-Write-Many (RWX) file storage.</p>"},{"location":"nkp_ndk/#lab-content","title":"Lab Content","text":"<ol> <li> <p>Set up source NKP in Primary PC    Configure the Nutanix Kubernetes Platform (NKP) on the primary Prism Central (PC) to serve as the source environment for NDK operations.</p> </li> <li> <p>Set up destination NKP in Secondary PC    Configure NKP on the secondary Prism Central to act as the destination environment for replication and recovery workflows.</p> </li> <li> <p>Install NDK on the Primary PC    Install NDK on the primary PC, disabling TLS for simplified setup.</p> </li> <li> <p>Install NDK on the Secondary PC    Install NDK on the secondary PC, disabling TLS to match the primary setup.</p> </li> </ol> <p>Single PC/PE/K8s Workflows </p> <ul> <li>Workflow 1: Application Snapshot &amp; Restore      Create and restore application snapshots within a single Prism Central/Prism Element/Kubernetes environment.  </li> <li>Workflow 2: Application Cross-Namespace Restore using ReferenceGrant      Perform application restoration across namespaces using ReferenceGrant to manage access between source and target namespaces.  </li> <li>Workflow 3: Schedule Protection Policy/Plan      Configure and schedule a protection policy to automate application backups.  </li> <li>New (NDK 1.3) Workflow 4: Application Snapshot &amp; Restore with RWX/Files      Explore snapshot and restore functionality for applications using Read-Write-Many (RWX) file storage, a new feature in NDK 1.3.</li> </ul> <p>Multi PC/PE/K8s Workflows</p> <ul> <li>Workflow 1: Snapshot Asynchronous Replication &amp; Recovery      Set up and test asynchronous replication of snapshots across multiple clusters and recover applications.  </li> <li>Workflow 2: Multi-Cluster (3 PE/3 K8s, 2 PC) Asynchronous Replication      Configure asynchronous replication across three Prism Elements and Kubernetes clusters managed by two Prism Central instances.  </li> <li>New (NDK 1.3) Workflow 3: Multi-Cluster (3 PE/3 K8s, 2 PC) Asynchronous + Synchronous Replication      Implement and test both asynchronous and synchronous replication for applications across multiple clusters, leveraging NDK 1.3 enhancements.</li> </ul>"},{"location":"nkp_ndk/nkp_ndk/","title":"Preparing NDK","text":"<p>In this section we will use just one Prism Central (PC)/Prism Element (PE)/K8s cluster to test the data recovery capabilities of NDK.</p>"},{"location":"nkp_ndk/nkp_ndk/#prerequisites","title":"Prerequisites","text":"<ul> <li>NDK <code>v1.2</code> or later deployed on a Nutanix cluster</li> <li>Nutanix Kubernetes Platform (NKP) cluster <code>v1.15</code> or later deployed, accessible via <code>kubectl</code>. See NKP Deployment for NKP install instructions.</li> <li>Internal Harbor container registry. See Harbor Installation</li> <li>Nutanix CSI driver installed for storage integration. [pre-configured with NKP install]</li> <li>Networking configured to allow communication between the Kubernetes cluster, PC and PE.</li> <li>Traefik Ingress controller installed for external access. [pre-configured with NKP install]</li> <li>K8s Load Balancer installed to facilitate replication workflows. [ Metallb pre-configured with NKP install]</li> <li>Linux Tools VM or equivalent environment with <code>kubectl</code>, <code>helm</code>, <code>curl</code>, <code>docker</code> and <code>jq</code> installed. See Jumphost VM for details.</li> <li>PC, PE and NKP access credentials</li> </ul>"},{"location":"nkp_ndk/nkp_ndk/#high-level-process","title":"High-Level Process","text":"<p>Warning</p> <p>NKP only supports NDK <code>v1.2</code> at the time of writing this lab.</p> <p>We will use NDK <code>v1.2</code> with NKP <code>v2.15</code> for this lab.</p> <p>This lab will be updated as NKP supports NDK <code>v1.3</code> in the near future.</p> <ol> <li>Download NDK <code>v1.2</code> binaries that are available in Nutanix Support Nutanix Portal</li> <li>Upload NDK containers to an internal Harbor registry</li> <li>Enable NDK to trust internal Harbor registry. See here</li> <li>Install NDK <code>v1.2</code></li> </ol>"},{"location":"nkp_ndk/nkp_ndk/#setup-source-nkp-in-primary-pcpek8s","title":"Setup source NKP in Primary PC/PE/K8s","text":"<p>Make sure to name your NKP cluster appropriately so it is easy to identify</p> <p>For the purposes of this lab, we will call the source NKP cluster as <code>nkpprimary</code></p> <p>Follow instructions in NKP Deployment to setup source/primary NKP K8s cluster.</p>"},{"location":"nkp_ndk/nkp_ndk/#setup-harbor-internal-registry","title":"Setup Harbor Internal Registry","text":"<p>Follow instructions in Harbor Installation to setup internal Harbor registry for storing NDK <code>v1.2</code> containers.</p> <ol> <li>Login to Harbor</li> <li>Create a project called <code>nkp</code> in Harbor</li> </ol>"},{"location":"nkp_ndk/nkp_ndk/#prepare-for-ndk-installation","title":"Prepare for NDK Installation","text":""},{"location":"nkp_ndk/nkp_ndk/#download-ndk-binaries","title":"Download NDK Binaries","text":"<ol> <li> <p>Open new <code>VSCode</code> window on your jumphost VM</p> </li> <li> <p>In <code>VSCode</code> Explorer pane, click on existing <code>$HOME</code> folder</p> </li> <li> <p>Click on New Folder  name it: <code>ndk</code></p> </li> <li> <p>On <code>VSCode</code> Explorer plane, click the <code>$HOME/ndk</code> folder</p> </li> <li> <p>On <code>VSCode</code> menu, select <code>Terminal</code> &gt; <code>New Terminal</code></p> </li> <li> <p>Browse to <code>ndk</code> directory</p>  Command <pre><code>cd $HOME/ndk\n</code></pre> </li> <li> <p>In <code>VSC</code>, under the newly created <code>ndk</code> folder, click on New File  and create file with the following name:</p>  File <pre><code>.env\n</code></pre> </li> <li> <p>Add (append) the following environment variables and save it</p>  Template .env Sample .env <pre><code>export NDK_VERSION=_your_ndk_version\nexport JUMPBOX=_your_jumpboxvm_ip\nexport SSH_USER=ubuntu\nexport KUBE_RBAC_PROXY_VERSION=_vX.XX.X\nexport KUBECTL_VERSION=_X.XX.X\nexport IMAGE_REGISTRY=_your_harbor_registy_url/nkp\n</code></pre> <pre><code>export NDK_VERSION=1.2.0\nexport JUMPBOX=10.x.x.124\nexport SSH_USER=ubuntu\nexport KUBE_RBAC_PROXY_VERSION=v0.17.0\nexport KUBECTL_VERSION=1.30.3\nexport IMAGE_REGISTRY=harbor.example.com/nkp\n</code></pre> </li> <li> <p>Source the <code>.env</code> file to import environment variables</p>  Command <pre><code>source $HOME/ndk/.env\n</code></pre> </li> <li> <p>Login to Nutanix Portal using your credentials</p> </li> <li> <p>Go to Downloads &gt; Nutanix Data Services for Kubernetes (NDK) </p> </li> <li> <p>Scroll and choose Nutanix Data Services for Kubernetes ( Version: 1.2.0 )</p> </li> <li> <p>Download the NDK binaries bundle from the link you copied earlier</p>  Command  Sample Command Paste the download URL within double quotes<pre><code>curl -o ndk-1.2.0.tar \"_paste_download_URL_here\"\n</code></pre> <pre><code>$HOME/ndk $ curl -o ndk-1.2.0.tar \"https://download.nutanix.com/downloads/ndk/1.2.0/ndk-1.2.0.tar?Expires=XXXXXXXXX__\"\n</code></pre> </li> <li> <p>Extract the NDK binaries</p>  Command  Sample Command <pre><code>tar -xvf ndk-${NDK_VERSION}.tar\ncd ndk-${NDK_VERSION}\n</code></pre> <pre><code>tar -xvf ndk-1.2.0.tar\ncd ndk-1.2.0\n</code></pre> NDK Binaries Directory Contents <pre><code>~/ndk/ndk-1.2.0$ tree\n\n\u251c\u2500\u2500 ndk-1.2.0\n\u2502   \u251c\u2500\u2500 chart\n\u2502   \u2502   \u251c\u2500\u2500 Chart.yaml                                      # NDK chart\n\n\u2502   \u2502   \u251c\u2500\u2500 crds\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 scheduler.nutanix.com_jobschedulers.yaml    # NDK CRDs\n\n\u2502   \u2502   \u251c\u2500\u2500 templates\n\n        \u2502   \u2502   \u2514\u2500\u2500 values.yaml                             # NDK chart values\n\n\u2502   \u2514\u2500\u2500 ndk-1.2.0.tar                                       # NDK container images\n</code></pre> </li> </ol>"},{"location":"nkp_ndk/nkp_ndk/#upload-ndk-binaries-to-internal-registry","title":"Upload NDK Binaries to Internal Registry","text":"<ol> <li> <p>Load NDK container images and upload to internal Harbor registry</p>  Command  Sample Command <pre><code>docker load -i ndk-${NDK_VERSION}.tar\ndocker login ${IMAGE_REGISTRY} \n\nfor img in ndk/manager:${NDK_VERSION} ndk/infra-manager:${NDK_VERSION} ndk/job-scheduler:${NDK_VERSION} ndk/kube-rbac-proxy:${KUBE_RBAC_PROXY_VERSION} ndk/bitnami-kubectl:${KUBECTL_VERSION}; do docker tag $img ${IMAGE_REGISTRY}/${img}; docker push ${IMAGE_REGISTRY}/${img};done\n</code></pre> <pre><code>docker load -i ndk-1.2.0.tar\ndocker login harbor.example.com/nkp\n\nfor img in ndk/manager:1.2.0 ndk/infra-manager:1.2.0 ndk/job-scheduler:1.2.0 ndk/kube-rbac-proxy:v0.17.0 ndk/bitnami-kubectl:1.30.3; do docker tag ndk/bitnami-kubectl:1.30.3 harbor.example.com/nkp/ndk/bitnami-kubectl:1.30.3; docker push harbor.example.com/nkp/ndk/bitnami-kubectl:1.30.3;done\n</code></pre> </li> </ol>"},{"location":"nkp_ndk/nkp_ndk/#install-ndk-on-primary-nkp-cluster","title":"Install NDK on Primary NKP Cluster","text":"<ol> <li>Login to VSCode Terminal</li> <li> <p>Set you NKP cluster KUBECONFIG</p>  Command Sample Command <pre><code>export KUBECONFIG=$HOME/nkp/_nkp_primary_cluster_name.conf\n</code></pre> <pre><code>export KUBECONFIG=$HOME/nkp/nkpprimary.conf\n</code></pre> </li> <li> <p>Test connection to <code>nkpprimary</code> cluster </p>  Command Sample Command <pre><code>kubectl get nodes -owide\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                                STATUS   ROLES           AGE    VERSION\nnkpprimary-md-0-vd5kr-ff8r8-hq764   Ready    &lt;none&gt;          3d4h   v1.32.3\nnkpprimary-md-0-vd5kr-ff8r8-jjpvx   Ready    &lt;none&gt;          3d4h   v1.32.3\nnkpprimary-md-0-vd5kr-ff8r8-md28h   Ready    &lt;none&gt;          3d4h   v1.32.3\nnkpprimary-md-0-vd5kr-ff8r8-xvmf6   Ready    &lt;none&gt;          3d4h   v1.32.3\nnkpprimary-xnnk5-6pnr8              Ready    control-plane   3d4h   v1.32.3\nnkpprimary-xnnk5-87slh              Ready    control-plane   3d4h   v1.32.3\nnkpprimary-xnnk5-fjdd4              Ready    control-plane   3d4h   v1.32.3\n</code></pre> </li> <li> <p>Install NDK</p>  Command  Sample Command Command output <pre><code>helm upgrade -n ntnx-system --install ndk chart/ \\\n--set manager.repository=\"$IMAGE_REGISTRY/ndk/manager\" \\\n--set manager.tag=${NDK_VERSION} \\\n--set infraManager.repository=\"$IMAGE_REGISTRY/ndk/infra-manager\" \\\n--set infraManager.tag=${NDK_VERSION} \\\n--set kubeRbacProxy.repository=\"$IMAGE_REGISTRY/ndk/kube-rbac-proxy\" \\\n--set kubeRbacProxy.tag=${KUBE_RBAC_PROXY_VERSION} \\\n--set bitnamiKubectl.repository=\"$IMAGE_REGISTRY/ndk/bitnami-kubectl\" \\\n--set bitnamiKubectl.tag=${KUBECTL_VERSION} \\\n--set jobScheduler.repository=\"$IMAGE_REGISTRY/ndk/job-scheduler\" \\\n--set jobScheduler.tag=${NDK_VERSION} \\\n--set config.secret.name=nutanix-csi-credentials \\\n--set tls.server.enable=false\n</code></pre> <pre><code>helm upgrade -n ntnx-system --install ndk chart/ \\\n--set manager.repository=\"harbor.example.com/nkp/ndk/manager\" \\\n--set manager.tag=1.2.0 \\\n--set infraManager.repository=\"harbor.example.com/nkp/ndk/infra-manager\" \\\n--set infraManager.tag=1.2.0 \\\n--set kubeRbacProxy.repository=\"harbor.example.com/nkp/ndk/kube-rbac-proxy\" \\\n--set kubeRbacProxy.tag=v0.17.0 \\\n--set bitnamiKubectl.repository=\"harbor.example.com/nkp/ndk/bitnami-kubectl\" \\\n--set bitnamiKubectl.tag=1.30.3 \\\n--set jobScheduler.repository=\"harbor.example.com/nkp/ndk/job-scheduler\" \\\n--set jobScheduler.tag=1.2.0 \\\n--set config.secret.name=nutanix-csi-credentials \\\n--set tls.server.enable=false\n</code></pre> <pre><code>Release \"ndk\" does not exist. Installing it now.\nNAME: ndk\nLAST DEPLOYED: Mon Jul  7 06:33:28 2025\nNAMESPACE: ntnx-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> </li> <li> <p>Check if all NDK custom resources are running (4 of 4 containers should be running inside the <code>ndk-controller-manger</code> pod)</p>  Command  Sample Command <pre><code>kubens ntnx-system\nk get all -l app.kubernetes.io/name=ndk\n</code></pre> <pre><code>Active namespace is \"ntnx-system\".\n\n$ k get all -l app.kubernetes.io/name=ndk\n\nNAME                                          READY   STATUS    RESTARTS   AGE\npod/ndk-controller-manager-57fd7fc56b-gg5nl   4/4     Running   0          19m\n\nNAME                                             TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)          AGE\nservice/ndk-controller-manager-metrics-service   ClusterIP      10.109.134.126   &lt;none&gt;         8443/TCP         19m\nservice/ndk-intercom-service                     LoadBalancer   10.99.216.62     10.122.7.212   2021:30258/TCP   19m\nservice/ndk-scheduler-webhook-service            ClusterIP      10.96.174.148    &lt;none&gt;         9444/TCP         19m\nservice/ndk-webhook-service                      ClusterIP      10.107.189.171   &lt;none&gt;         443/TCP          19m\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ndk-controller-manager   1/1     1            1           19m\n\nNAME                                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ndk-controller-manager-57fd7fc56b   1         1         1       19m\n</code></pre> </li> </ol>"},{"location":"nkp_ndk/nkp_ndk/#ndk-custom-resources-for-k8s","title":"NDK Custom Resources for K8s","text":"<p>To begin protecting applications with NDK, it is good to become familiar with the NDK custom resources and how they are used to manage data protection. The following table provides a brief overview of the NDK custom resources and their purposes.</p> <p>For more information about the NDK custom resources, see the NDK Custom Resources section of the NDK documentation.</p> <p>Tip</p> <p>We will be using NDK custom resources throughout the lab for accomplising data protection tasks and show the relationship between these custom resources as well.</p> Custom Resource Purpose <code>StorageCluster</code> Defines the Nutanix storage fabric and UUIDs for PE and PC. <code>Application</code> Defines a logical group of K8s resources for data protection. <code>ApplicationSnapshotContent</code> Stores infrastructure-level data of an application snapshot. <code>ApplicationSnapshot</code> Takes a snapshot of an application and its volumes. <code>ApplicationSnapshotRestore</code> Restores an application snapshot. <code>Remote</code> Defines a target Kubernetes cluster for replication. <code>ReplicationTarget</code> Specifies where to replicate an application snapshot. <code>ApplicationSnapshotReplication</code> Triggers snapshot replication to another cluster. <code>JobScheduler</code> Defines schedules for data protection jobs. <code>ProtectionPlan</code> Defines snapshot and replication rules and retention. <code>AppProtectionPlan</code> Applies one or more ProtectionPlans to an application."},{"location":"nkp_ndk/nkp_ndk/#configure-ndk","title":"Configure NDK","text":"<p>The first component we would configure in NDK is <code>StorageCluster</code>. This is used to represent the Nutanix Cluster components including the following:</p> <ul> <li>Prism Central (PC)</li> <li>Prism Element (PE)</li> </ul> <p>By configuring <code>StorageCluster</code> custom resource with NDK, we are providing Nutanix infrastructure information to NDK.</p> <ol> <li> <p>Logon to Jumphost VM Terminal in <code>VSCode</code></p>  Command <pre><code>cd $HOME/ndk\n</code></pre> </li> <li> <p>Get uuid of PC and PE using the following command</p>   Template Command  Sample .command  Command output <pre><code>kubectl get node _any_nkp_node_name -o jsonpath='{.metadata.labels}' | grep -o 'csi\\.nutanix\\.com/[^,]*' \n</code></pre> <pre><code>kubectl get node nkprimary-md-0-vd5kr-ff8r8-hq764 -o jsonpath='{.metadata.labels}' | grep -o 'csi\\.nutanix\\.com/[^,]*' \n</code></pre> <pre><code>$ kubectl get node nkprimary-md-0-vd5kr-ff8r8-hq764 -o jsonpath='{.metadata.labels}' | grep -o 'csi\\.nutanix\\.com/[^,]*' \n\ncsi.nutanix.com/prism-central-uuid\":\"d0f1eb56-9ee6-4469-b21f-xxxxxxxxxxxx\"\ncsi.nutanix.com/prism-element-uuid\":\"00062f20-b2e0-fa8e-4b04-xxxxxxxxxxxx\"\n</code></pre> </li> <li> <p>Add (append) the following environment variables <code>$HOME/ndk/.env</code> and save it</p>  Template .env Sample .env <pre><code>export PRISM_CENTRAL_UUID=_pc_uuid_from_previous_commands\nexport PRISM_ELEMENT_UUID=_pe_uuid_from_previous_commands\nexport SC_NAME=_storage_cluster_name\nexport KUBECONFIG=$HOME/nkp/_nkp_primary_cluster_name.conf\n</code></pre> <pre><code>export PRISM_CENTRAL_UUID=ad0f1eb56-9ee6-4469-b21f-xxxxxxxxxx\nexport PRISM_ELEMENT_UUID=00062f20-b2e0-fa8e-4b04-xxxxxxxxxx\nexport SC_NAME=primary-storage-cluster\nexport KUBECONFIG=$HOME/nkp/nkpprimary.conf\n</code></pre> </li> <li> <p>Note and export the external  IP assigned to the NDK intercom service on the Primary Cluster</p> <pre><code>export PRIMARY_NDK_IP=$(k get svc -n ntnx-system ndk-intercom-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\necho $PRIMARY_NDK_IP\n</code></pre> </li> <li> <p>Add (append) the following environment variables file <code>$HOME/ndk/.env</code> and save it</p>  Template .env <pre><code>export PRIMARY_NDK_PORT=2021\nexport PRIMARY_NDK_IP=$(k get svc -n ntnx-system ndk-intercom-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre> </li> <li> <p>Source the <code>$HOME/ndk/.env</code> file</p>  Command <pre><code>source $HOME/ndk/.env\n</code></pre> </li> <li> <p>Create the StorageCluster custom resource</p>  Command Command Output <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: StorageCluster\nmetadata:\n name: $SC_NAME\nspec:\n storageServerUuid: $PRISM_ELEMENT_UUID\n managementServerUuid: $PRISM_CENTRAL_UUID\nEOF\n</code></pre> <pre><code>storagecluster.dataservices.nutanix.com/primary-storage-cluster created\n</code></pre> </li> </ol> <p>Now we are ready to create local cluster snapshots and snapshot restores using the following NDK custom resources:</p> <ul> <li><code>ApplicationSnapshot</code> and</li> <li><code>ApplicationSnapshotRestore</code></li> </ul>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/","title":"Replicating and Recovering Application to a Different K8S Cluster","text":"<p>In this section we wil snapshot the Application components, replicate it to a second K8S cluster and recover.</p>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#design","title":"Design","text":"<p>In this section there are two PC and individual PE connected. Each PE has a NKP cluster. </p> <p>We will be replicating application from one NKP cluster in one PC to another NKP cluster in a different PC.</p> # PC PE NKP Cluster Source PC-1 PE-1 <code>nkpprimary</code> Destination PC-2 PE-2 <code>nkpsecondary</code> <p>The following is the flow of the application recovery. </p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state PC1 {\n        [*] --&gt; PE1\n        PE1 --&gt; nkpprimary\n        nkpprimary --&gt; SourceApp\n    }\n\n    state PC2 {\n        [*] --&gt; PE2\n        PE2 --&gt; nkpsecondary\n        nkpsecondary --&gt; DestinationApp\n    }\n\n    [*] --&gt; PC1\n    PC1 --&gt; PC2\n    PC2 --&gt; [*]</code></pre>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#setup-destination-nkp-in-primary-pcpek8s","title":"Setup Destination NKP in Primary PC/PE/K8s","text":"<p>Make sure to name your NKP cluster appropriately so it is easy to identify</p> <p>For the purposes of this lab, we will call the source NKP cluster as <code>nkpsecondary</code></p> <p>Follow instructions in NKP Deployment to setup destination/secondary NKP K8s cluster.</p>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#replication-custom-resources","title":"Replication Custom Resources","text":"<p>The steps include configuring the following NDK custom resources:</p> Custom Resource Purpose <code>StorageCluster</code> Defines the Nutanix storage fabric and UUIDs for secondary NKP cluster <code>Remote</code> Defines a target Kubernetes cluster for replication on the target NKP cluster <code>ReplicationTarget</code> Specifies where to replicate an application snapshot. <code>ApplicationSnapshotReplication</code> Triggers snapshot replication to another cluster. <code>ApplicationSnapshotRestore</code> Restores an application snapshot."},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#configure-availability-zones-on-pcs","title":"Configure Availability Zones on PCs","text":"<p>To enable replication between two PC and underlying PE, we will need to configure Availability Zone bi-directionally.</p> <ol> <li>Logon to Primary PC and go to Administration &gt; Availability Zones</li> <li>Click on Connect to Availability Zone</li> <li> <p>Choose Physical Location and enter the Secondary PC details</p> <ul> <li>IP Address for Remote PC - 10.x.x.x</li> <li>Username - admin</li> <li>Password - xxxxxxxxxxx</li> </ul> </li> <li> <p>Click on Connect</p> </li> <li>Confirm addition of remote PC</li> <li>Repeat steps 1 - 5 on the remote PC to configure access to Primary PC</li> </ol>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#install-ndk-on-secondary-nkp-cluster","title":"Install NDK on Secondary NKP Cluster","text":"<ol> <li>Login to VSCode Terminal</li> <li> <p>Set you NKP cluster KUBECONFIG</p>  Command Sample Command <pre><code>export KUBECONFIG=$HOME/nkp/_nkp_secondary_cluster_name.conf\n</code></pre> <pre><code>export KUBECONFIG=$HOME/nkp/nkpsecondary.conf\n</code></pre> </li> <li> <p>Test connection to <code>nkpsecondary</code> cluster </p>  Command Sample Command <pre><code>kubectl get nodes -owide\n</code></pre> <pre><code>$ kubectl get nodes\n\nNAME                            STATUS   ROLES           AGE   VERSION\nnkpsec-md-0-fdrzg-clvf9-2gnqc   Ready    &lt;none&gt;          24h   v1.32.3\nnkpsec-md-0-fdrzg-clvf9-9msmd   Ready    &lt;none&gt;          24h   v1.32.3\nnkpsec-md-0-fdrzg-clvf9-hnjlm   Ready    &lt;none&gt;          24h   v1.32.3\nnkpsec-md-0-fdrzg-clvf9-t8t4l   Ready    &lt;none&gt;          24h   v1.32.3\nnkpsec-rhdh8-2xs7z              Ready    control-plane   24h   v1.32.3\nnkpsec-rhdh8-srm6h              Ready    control-plane   24h   v1.32.3\nnkpsec-rhdh8-wxbd9              Ready    control-plane   24h   v1.32.3\n</code></pre> </li> <li> <p>Install NDK</p>  Command  Sample Command Command output <pre><code>helm upgrade -n ntnx-system --install ndk chart/ \\\n--set manager.repository=\"$IMAGE_REGISTRY/ndk/manager\" \\\n--set manager.tag=${NDK_VERSION} \\\n--set infraManager.repository=\"$IMAGE_REGISTRY/ndk/infra-manager\" \\\n--set infraManager.tag=${NDK_VERSION} \\\n--set kubeRbacProxy.repository=\"$IMAGE_REGISTRY/ndk/kube-rbac-proxy\" \\\n--set kubeRbacProxy.tag=${KUBE_RBAC_PROXY_VERSION} \\\n--set bitnamiKubectl.repository=\"$IMAGE_REGISTRY/ndk/bitnami-kubectl\" \\\n--set bitnamiKubectl.tag=${KUBECTL_VERSION} \\\n--set jobScheduler.repository=\"$IMAGE_REGISTRY/ndk/job-scheduler\" \\\n--set jobScheduler.tag=${NDK_VERSION} \\\n--set config.secret.name=nutanix-csi-credentials \\\n--set tls.server.enable=false\n</code></pre> <pre><code>helm upgrade -n ntnx-system --install ndk chart/ \\\n--set manager.repository=\"harbor.example.com/nkp/ndk/manager\" \\\n--set manager.tag=1.2.0 \\\n--set infraManager.repository=\"harbor.example.com/nkp/ndk/infra-manager\" \\\n--set infraManager.tag=1.2.0 \\\n--set kubeRbacProxy.repository=\"harbor.example.com/nkp/ndk/kube-rbac-proxy\" \\\n--set kubeRbacProxy.tag=v0.17.0 \\\n--set bitnamiKubectl.repository=\"harbor.example.com/nkp/ndk/bitnami-kubectl\" \\\n--set bitnamiKubectl.tag=1.30.3 \\\n--set jobScheduler.repository=\"harbor.example.com/nkp/ndk/job-scheduler\" \\\n--set jobScheduler.tag=1.2.0 \\\n--set config.secret.name=nutanix-csi-credentials \\\n--set tls.server.enable=false\n</code></pre> <pre><code>Release \"ndk\" does not exist. Installing it now.\nNAME: ndk\nLAST DEPLOYED: Mon Jul  7 06:33:28 2025\nNAMESPACE: ntnx-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> </li> <li> <p>Check if all NDK custom resources are running (4 of 4 containers should be running inside the <code>ndk-controller-manger</code> pod)</p>  Command  Sample Command <pre><code>kubens ntnx-system\nk get all -l app.kubernetes.io/name=ndk\n</code></pre> <pre><code>Active namespace is \"ntnx-system\".\n\n$ k get all -l app.kubernetes.io/name=ndk\n\nNAME                                          READY   STATUS    RESTARTS   AGE\npod/ndk-controller-manager-57fd7fc56b-gg5nl   4/4     Running   0          19m\n\nNAME                                             TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)          AGE\nservice/ndk-controller-manager-metrics-service   ClusterIP      10.109.134.126   &lt;none&gt;         8443/TCP         19m\nservice/ndk-intercom-service                     LoadBalancer   10.99.216.62     10.122.7.212   2021:30258/TCP   19m\nservice/ndk-scheduler-webhook-service            ClusterIP      10.96.174.148    &lt;none&gt;         9444/TCP         19m\nservice/ndk-webhook-service                      ClusterIP      10.107.189.171   &lt;none&gt;         443/TCP          19m\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ndk-controller-manager   1/1     1            1           19m\n\nNAME                                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ndk-controller-manager-57fd7fc56b   1         1         1       19m\n</code></pre> </li> </ol>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#configure-ndk","title":"Configure NDK","text":"<p>The first component we would configure in NDK is <code>StorageCluster</code>. This is used to represent the Nutanix Cluster components including the following:</p> <ul> <li>Prism Central (PC)</li> <li>Prism Element (PE)</li> </ul> <p>By configuring <code>StorageCluster</code> custom resource with NDK, we are providing Nutanix infrastructure information to NDK.</p> <ol> <li> <p>Logon to Jumphost VM Terminal in <code>VSCode</code></p>  Command <pre><code>cd $HOME/ndk\n</code></pre> </li> <li> <p>Get <code>uuid</code> of secondary PC and PE using the following command</p>   Template Command  Sample .command  Command output <pre><code>kubectl get node _any_nkp_node_name -o jsonpath='{.metadata.labels}' | grep -o 'csi\\.nutanix\\.com/[^,]*' \n</code></pre> <pre><code>kubectl get node nkpsecondary-md-0-fdrzg-clvf9-t8t4l -o jsonpath='{.metadata.labels}' | grep -o 'csi\\.nutanix\\.com/[^,]*' \n</code></pre> <pre><code>$ kubectl get node nkpsecondary-md-0-fdrzg-clvf9-t8t4l -o jsonpath='{.metadata.labels}' | grep -o 'csi\\.nutanix\\.com/[^,]*' \n\ncsi.nutanix.com/prism-central-uuid\":\"cb5ca4e1-29d4-4a6f-91c7-xxxxxxxxxxxx\"\ncsi.nutanix.com/prism-element-uuid\":\"000639fd-8cfa-9bf4-3d70-xxxxxxxxxxxx\"\n</code></pre> </li> <li> <p>Add (append) the following environment variables and save it</p>  Template .env Sample .env <pre><code>export SECONDARY_PRISM_CENTRAL_UUID=_pc_uuid_from_previous_commands\nexport SECONDARY_PRISM_ELEMENT_UUID=_pe_uuid_from_previous_commands\nexport SECONDARY_SC_NAME=_storage_cluster_name\nexport NDK_REPLICATION_CLUSTER_NAME=_secondary_cluster_name\nexport KUBECONFIG=$HOME/nkp/_nkp_secondary_cluster_name.conf\n</code></pre> <pre><code>export SECONDARY_PRISM_CENTRAL_UUID=ad0f1eb56-9ee6-4469-b21f-xxxxxxxxxx\nexport SECONDARY_PRISM_ELEMENT_UUID=00062f20-b2e0-fa8e-4b04-xxxxxxxxxx\nexport SECONDARY_SC_NAME=secondary-storage-cluster\nexport NDK_REPLICATION_CLUSTER_NAME=nkpsecondary\nexport KUBECONFIG=$HOME/nkp/nkpsecondary.conf\n</code></pre> </li> <li> <p>Note and export the external  IP assigned to the NDK intercom service on the Primary Cluster</p> <pre><code>export SECONDARY_NDK_IP=$(k get svc -n ntnx-system ndk-intercom-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\necho $SECONDARY_NDK_IP\n</code></pre> </li> <li> <p>Add (append) the following environment variables file <code>$HOME/ndk/.env</code> and save it</p>  Template .env <pre><code>export SECONDARY_NDK_PORT=2021\nexport SECONDARY_NDK_IP=$(k get svc -n ntnx-system ndk-intercom-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre> </li> <li> <p>Source the <code>.env</code> file</p>  Command <pre><code>source $HOME/ndk/.env\n</code></pre> </li> <li> <p>Create the StorageCluster custom resource</p>  Command Command Output <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: StorageCluster\nmetadata:\n name: $SECONDARY_SC_NAME\nspec:\n storageServerUuid: $SECONDARY_PRISM_ELEMENT_UUID\n managementServerUuid: $SECONDARY_PRISM_CENTRAL_UUID\nEOF\n</code></pre> <pre><code>storagecluster.dataservices.nutanix.com/secondary-storage-cluster created\n</code></pre> </li> <li> <p>Find and configure secondary NDK IP and port number </p> </li> </ol>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#ndk-recover-to-the-secondary-nkp-cluster","title":"NDK Recover to the Secondary NKP Cluster","text":"<p>Since we have a sample workload configured on the primary NKP cluster, we will:</p> <ol> <li>Configure remote NKP cluster on the primary NKP cluster (using <code>Remote</code> <code>ReplicationTarget</code> custom resources)</li> <li>Replicate the snapshot of the sample workload from the primary NKP to secondary NKP  (using <code>ApplicationSnapshotReplication</code> custom resource)</li> <li>Restore the replicated snapshot on the secondary NKP to get the workloads (using <code>ApplicationSnapshotRestore</code> custom resource)</li> </ol>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#create-remote-cluster-on-primary-nkp-cluster","title":"Create Remote Cluster on Primary NKP Cluster","text":"<ol> <li> <p>Switch context to primary NKP cluster <code>nkpprimary</code></p>  Command Sample Command <pre><code>export KUBECONFIG=$HOME/nkp/_nkp_primary_cluster_name.conf\n</code></pre> <pre><code>export KUBECONFIG=$HOME/nkp/nkpprimary.conf\n</code></pre> </li> <li> <p>Create the Remote resource on the primary NKP cluster</p>  Command  Sample Command Command output <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: Remote\nmetadata:\n  name: ${NDK_REPLICATION_CLUSTER_NAME}\nspec:\n  ndkServiceIp: ${SECONDARY_NDK_IP}\n  ndkServicePort: ${SECONDARY_NDK_PORT}\nEOF\n</code></pre> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: Remote\nmetadata:\n  name: nkpsecondary\nspec:\n  ndkServiceIp: 10.x.x.32\n  ndkServicePort: 2021\nEOF\n</code></pre> <pre><code>remote.dataservices.nutanix.com/nkpsecondary created\n</code></pre> </li> <li> <p>Make sure the <code>Remote</code> cluster is healthy</p>   Sample Command Command output <pre><code>kubectl describe remote.dataservices.nutanix.com/nkpsecondary\n</code></pre> <pre><code>kubectl describe remote.dataservices.nutanix.com/nkpsecondary\n\nStatus:\nConditions:\n    Last Transition Time:  2025-07-16T21:29:38Z\n    Message:               \n    Observed Generation:   1\n    Reason:                Healthy\n    Status:                True\n    Type:                  Available\nEvents:                    &lt;none&gt;\n</code></pre> </li> <li> <p>Create the replication target on the primary NKP cluster</p>  Command  Sample Command Command output <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: ReplicationTarget\nmetadata:\n  name: ${NDK_REPLICATION_CLUSTER_NAME}\n  namespace: default\nspec:\n  remoteName: ${NDK_REPLICATION_CLUSTER_NAME}\nEOF\n</code></pre> <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: ReplicationTarget\nmetadata:\n  name: nkpsecondary\n  namespace: default\nspec:\n  remoteName: nkpsecondary\nEOF\n</code></pre> <pre><code>replicationtarget.dataservices.nutanix.com/nkpsecondary created\n</code></pre> </li> <li> <p>Make sure the <code>ReplicationTarget</code> is healthy</p>   Sample Command Command output <pre><code>kubectl describe replicationtarget.dataservices.nutanix.com/nkpsecondary\n</code></pre> <pre><code>kubectl describe replicationtarget.dataservices.nutanix.com/nkpsecondary\n\nstatus:\n  conditions:\n  - lastTransitionTime: \"2025-07-16T21:31:06Z\"\n    message: \"\"\n    observedGeneration: 1\n    reason: Healthy\n    status: \"True\"\n    type: Available\n</code></pre> </li> <li> <p>Replicate the Snapshot to the Replication Cluster</p>  Command Command output <pre><code>k apply -f -&lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: ApplicationSnapshotReplication\nmetadata:\n  name: replication-1\n  namespace: default\nspec:\n  applicationSnapshotName: app1-snap\n  replicationTargetName: ${NDK_REPLICATION_CLUSTER_NAME}\nEOF\n</code></pre> <pre><code>applicationsnapshotreplication.dataservices.nutanix.com/replication-1 created\n</code></pre> </li> <li> <p>Monitor the progress of the replication and make sure to complete it</p>  Command Command output <pre><code>kubectl describe ApplicationSnapshotReplication replication-1\n</code></pre> <pre><code>Status:\n  Conditions:\n    Last Transition Time:          2025-07-16T21:51:32Z\n    Message:                       \n    Observed Generation:           1\n    Reason:                        ReplicationComplete\n    Status:                        True\n    Type:                          Available\n    Last Transition Time:          2025-07-16T21:51:32Z\n    Message:                       \n    Observed Generation:           1\n    Reason:                        ReplicationComplete\n    Status:                        False\n    Type:                          Progressing\n  Replication Completion Percent:  100\n</code></pre> </li> </ol>"},{"location":"nkp_ndk/nkp_ndk_k8s_replication/#recover-application-in-remote-nkp-cluster","title":"Recover Application in Remote NKP Cluster","text":"<ol> <li> <p>Switch context to secondary NKP cluster <code>nkpsecondary</code></p>  Command Sample Command <pre><code>export KUBECONFIG=$HOME/nkp/_nkp_secondary_cluster_name.conf\n</code></pre> <pre><code>export KUBECONFIG=$HOME/nkp/nkpsecondary.conf\n</code></pre> </li> <li> <p>Confirm if the <code>ApplicationSnapshot</code> has been replicated</p>  Command Command output <pre><code>kubectl get ApplicationSnapshot -n default\n</code></pre> <pre><code>k get applicationsnapshot -n default\n\nNAMESPACE   NAME        AGE   READY-TO-USE   BOUND-SNAPSHOTCONTENT                                  SNAPSHOT-AGE\ndefault     app1-snap   8m   true           asc-aee3f794-190c-403b-a245-bcac8859bb88-19815381630   8m\n</code></pre> </li> <li> <p>Restore the replicated <code>ApplicationSnapshot</code></p>  Command Command output <pre><code># Restore\nkubectl apply -f - &lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: ApplicationSnapshotRestore\nmetadata:\n  name: app1-restore\nspec:\n  applicationSnapshotName: app1-snap\nEOF\n</code></pre> <pre><code>applicationsnapshotrestore.dataservices.nutanix.com/app1-restore created\n</code></pre> </li> <li> <p>Monitor the restore </p>  Command Command output <pre><code>k get applicationsnapshotrestore.dataservices.nutanix.com/app1-restore -w\n</code></pre> <pre><code>NAME           SNAPSHOT-NAME   COMPLETED\napp1-restore   app1-snap       false\napp1-restore   app1-snap       false\napp1-restore   app1-snap       true\n</code></pre> </li> <li> <p>Monitor the restore steps to understand the flow</p>  Command Command output <pre><code>k describe applicationsnapshotrestore.dataservices.nutanix.com/app1-restore\n</code></pre> <pre><code>NAME           SNAPSHOT-NAME   COMPLETED\nstatus:\ncompleted: true\nconditions:\n- lastTransitionTime: \"2025-07-16T21:57:53Z\"\n  message: All prechecks passed and finalizers on dependent resources set\n  observedGeneration: 1\n  reason: PrechecksPassed\n  status: \"True\"\n  type: PrechecksPassed\n- lastTransitionTime: \"2025-07-16T21:59:00Z\"\n  message: All eligible application configs restored\n  observedGeneration: 1\n  reason: ApplicationConfigRestored\n  status: \"True\"\n  type: ApplicationConfigRestored\n- lastTransitionTime: \"2025-07-16T21:59:15Z\"\n  message: All eligible volumes restored\n  observedGeneration: 1\n  reason: VolumesRestored\n  status: \"True\"\n  type: VolumesRestored\n- lastTransitionTime: \"2025-07-16T21:59:15Z\"\n  message: Application restore successfully finalised\n  observedGeneration: 1\n  reason: ApplicationRestoreFinalised\n  status: \"True\"\n  type: ApplicationRestoreFinalised\nfinishTime: \"2025-07-16 21:59:15\"\nstartTime: \"2025-07-16 21:57:52\"\n</code></pre> </li> <li> <p>Verify if app1 pvc and pod are restored</p>  Command Command Output <pre><code>kubectl get po,pvc -l app=app1\n</code></pre> <pre><code>$  kubectl get po,pvc -l app=app1\n\nNAME        READY   STATUS    RESTARTS   AGE\npod/app-1   1/1     Running   0          4m53s\n\nNAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS     VOLUMEATTRIBUTESCLASS   AGE\npersistentvolumeclaim/az-claim-1   Bound    pvc-55a6b812-35de-4db2-b0e1-8a55e1b4e41f   4Gi        RWO            nutanix-volume   &lt;unset&gt;                 5m54s\n</code></pre> </li> <li> <p>Check if data is present within the data mount <code>/data</code> inside the pod</p>  Command Command Output <pre><code>kubectl exec -it app-1 -- /bin/sh -c \"wc -l /data/abc.txt\"\n</code></pre> <pre><code>kubectl exec -it app-1 -- /bin/sh -c \"wc -l /data/abc.txt\"\n10000 /data/abc.txt\n</code></pre> </li> </ol> <p>We have successfully replicated application data to a secondary NKP cluster and recovered it using NDK. </p>"},{"location":"nkp_ndk/nkp_ndk_singlek8s/","title":"Recovering Application within the same K8S Cluster","text":"<p>In this section we will deploy a sample workload, snapshot the Application components and recover.</p> <ol> <li>Same namespace recovery</li> <li>Cross namespace recovery</li> </ol>"},{"location":"nkp_ndk/nkp_ndk_singlek8s/#deploying-a-sample-application","title":"Deploying a Sample Application","text":"<ol> <li> <p>On <code>VSCode</code> menu, select <code>Terminal</code> &gt; <code>New Terminal</code></p> </li> <li> <p>Browse to <code>ndk</code> directory</p>  Command <pre><code>cd $HOME/ndk\n</code></pre> </li> <li> <p>Source the .env file</p>  Command <pre><code>source .env\n</code></pre> </li> <li> <p>Change to default namespace</p>  Command <pre><code>kubens default\n</code></pre> </li> <li> <p>Create a stateful workload</p>  Command <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  labels:\n    app: app1\n  name: az-claim-1\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\nEOF\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app-1\n  labels:\n    app: app1\nspec:\n  containers:\n  - name: app\n    image: docker.io/library/busybox:1.36.1\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo $(date -u) ; sleep 5; done\"]\n    volumeMounts:\n    - name: persistent-storage\n      mountPath: /data\n  volumes:\n  - name: persistent-storage\n    persistentVolumeClaim:\n      claimName: az-claim-1\nEOF\n</code></pre> </li> <li> <p>Watch the workload components until they are running</p>  Command Sample command <pre><code>kubectl get po,pvc -l app=app1\n</code></pre> <pre><code>NAME        READY   STATUS    RESTARTS   AGE\npod/app-1   1/1     Running   0          2m\n\nNAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS     VOLUMEATTRIBUTESCLASS   AGE\npersistentvolumeclaim/az-claim-1   Bound    pvc-f2d77c96-d2e4-42b3-b3d6-aa828f9e45e6   1Gi        RWO            nutanix-volume   &lt;unset&gt;                 2m\n</code></pre> </li> <li> <p>Once the pvc and pod are running, inject some data into the persistent storage</p>  Command <pre><code>kubectl exec -it app-1 -- /bin/sh -c 'for i in $(seq 1 10000); do echo \"foobar\" &gt;&gt; /data/abc.txt;done'\n</code></pre> </li> </ol> <p>Now we have a stateful workload which we can replicate and recover using NDK</p>"},{"location":"nkp_ndk/nkp_ndk_singlek8s/#ndk-recover-to-the-same-namespace","title":"NDK Recover to the Same Namespace","text":"<p>Tip</p> <p>NDK uses labels to select kubernetes resources to act upon. </p> <ol> <li> <p>Define a NDK <code>Application</code> custom resource to replicate our deployed application with label <code>app1</code></p>  Command Command output <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: Application\nmetadata:\n  name: app-1\nspec:\n  applicationSelector:                      # Note if left blank, it will include all resources in the namespace\n    resourceLabelSelectors:\n    - labelSelector:\n        matchLabels:\n          app: app1\nEOF\n</code></pre> <pre><code>application.dataservices.nutanix.com/app-1 created\n</code></pre> </li> <li> <p>Take a local cluster snapshot of the <code>app1</code> application</p>  Command Command output <pre><code>k apply -f - &lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: ApplicationSnapshot\nmetadata:\n  name: app1-snap\nspec:\n  source:\n    applicationRef:\n      name: app-1                           # This is Application reference for the snapshot operation\n  expiresAfter:  48h\nEOF\n</code></pre> <pre><code>applicationsnapshot.dataservices.nutanix.com/app1-snap created\n</code></pre> </li> <li> <p>View the progress</p>  Command Command output <p><pre><code>kubectl get applicationsnapshot -w\n</code></pre> <pre><code>kubectl describe applicationsnapshot.dataservices.nutanix.com/app1-snap\n</code></pre></p> <p>Wait until the status of snapshot becomes true<pre><code>$ k get applicationsnapshot -w\n\nNAME        AGE   READY-TO-USE   BOUND-SNAPSHOTCONTENT                      SNAPSHOT-AGE\napp1-snap   11s   false          asc-8af85cca-3da7-468c-96fb-db54ec2cf940   \napp1-snap   37s   true           asc-8af85cca-3da7-468c-96fb-db54ec2cf940   36s\n</code></pre> Observe the Application components that are included and status<pre><code>Name:         app1-snap\nNamespace:    default\n\nAPI Version:  dataservices.nutanix.com/v1alpha1\nKind:         ApplicationSnapshot\nMetadata:\n  Creation Timestamp:  2025-07-08T01:34:41Z\n\nSpec:\n  Expires After:  48h0m0s\n  Source:\n    Application Ref:\n      Name:  app-1\nStatus:\n  Bound Application Snapshot Content Name:  asc-3c1e253a-266d-46fd-8559-d8aa189fea78\n  Creation Time:                            2025-07-08T01:35:13Z\n  Expiration Time:                          2025-07-10T01:35:13Z\n  Ready To Use:                             true\n  Summary:\n    Snapshot Artifacts:\n      cilium.io/v2/CiliumEndpoint:\n        Name:  app-1\n      v1/PersistentVolumeClaim:\n        Name:  az-claim-1\n      v1/Pod:\n        Name:  app-1\nEvents:\n  Type    Reason                   Age    From  Message\n  ----    ------                   ----   ----  -------\n  Normal  AcquireAppConfigWaiting  2m47s  NDK   ApplicationSnapshotContent's app config acquisition is waiting to be processed\n  Normal  VolumeSnapshotWaiting    2m45s  NDK   ApplicationSnapshotContent's volume snapshot phase is waiting to be processed\n  Normal  FinalizeSnapshotWaiting  2m16s  NDK   ApplicationSnapshotContent's finalize phase is waiting to be processed\n</code></pre></p> Relationship between NDK custom resources <p>We can observe that the <code>ApplicationSnapshot</code> and <code>ApplicationSnapshotContent</code> NDK custom resources are related.</p> <p><code>ApplicationSnapshotContent</code> also shows the Nutanix infrastructure components of the <code>ApplicationSnapshot</code> custom resources such as Nutanix volumes.</p> <p>Refer to the highlighted parts in the following command output.</p> <pre><code>kubectl get ApplicationSnapshot\n\nNAMESPACE   NAME        AGE     READY-TO-USE   BOUND-SNAPSHOTCONTENT                      SNAPSHOT-AGE\ndefault     app1-snap   4h33m   true           asc-8af85cca-3da7-468c-96fb-db54ec2cf940   4h33m\n</code></pre> <pre><code>kubectl get ApplicationSnapshotContent -oyaml\n\napiVersion: v1\nitems:\n- apiVersion: dataservices.nutanix.com/v1alpha1\n  kind: ApplicationSnapshotContent\n  metadata:\n    creationTimestamp: \"2025-07-08T02:00:06Z\"\n    finalizers:\n    - dataservices.nutanix.com/app-snap\n    - dataservices.nutanix.com/app-snap-content\n    generation: 1\n    name: asc-8af85cca-3da7-468c-96fb-db54ec2cf940\n    resourceVersion: \"7484635\"\n    uid: 9ed65107-b400-4d75-9acd-c0a7a0aede81\n  spec:\n    applicationSnapshotRef:\n      name: app1-snap\n      namespace: default\n    source:\n      applicationRef:\n        name: app-1\n        namespace: default\n  status:\n    applicationSnapshotSummary:\n      applicationSnapshotHandle:\n        name: asc-8af85cca-3da7-468c-96fb-db54ec2cf940\n      volumeClaimHandleMap:\n        az-claim-1: NutanixVolumes-0d77026f-f513-4b20-4c4f-822d31e0c4d4\n      volumeSnapshotHandleMap:\n        NutanixVolumes-0d77026f-f513-4b20-4c4f-822d31e0c4d4: 5acbcd9e-fde3-4fe5-aabd-11a4b080a044:ea8518bb-3453-4748-bbd0-713eb9358c5b\n</code></pre> </li> <li> <p>The NDK controller manager will also have logs of the snapshot operation. This will be useful for troubleshooting purposes</p>  Command Command output <pre><code>kubectl logs -f -n ntnx-system deploy/ndk-controller-manager\n</code></pre> <pre><code>$ kubectl logs -f -n ntnx-system deploy/ndk-controller-manager\n\n{\"level\":\"info\",\"timestamp\":\"2025-07-08T01:35:12.909Z\",\"caller\":\"applicationsnapshotcontent/asc_finalize.go:38\",\"msg\":\"resource regulated: ApplicationSnapshotContent's finalize phase is waiting to be processed\",\"controller\":\"applicationsnapshotcontent\",\"controllerGroup\":\"dataservices.nutanix.com\",\"controllerKind\":\"ApplicationSnapshotContent\",\"ApplicationSnapshotContent\":{\"name\":\"asc-3c1e253a-266d-46fd-8559-d8aa189fea78\"},\"namespace\":\"\",\"name\":\"asc-3c1e253a-266d-46fd-8559-d8aa189fea78\",\"reconcileID\":\"a0cf1641-40d7-4d51-b948-10cf9bae84e0\",\"requestId\":\"7566b4fa-0eda-4ffd-b34e-76daf2311148\"}\n\n{\"level\":\"info\",\"timestamp\":\"2025-07-08T01:35:13.381Z\",\"caller\":\"utils/controller.go:36\",\"msg\":\"Ensuring finalizer on objects\",\"controller\":\"applicationsnapshot\",\"controllerGroup\":\"dataservices.nutanix.com\",\"controllerKind\":\"ApplicationSnapshot\",\"ApplicationSnapshot\":{\"name\":\"app1-snap\",\"namespace\":\"default\"},\"namespace\":\"default\",\"name\":\"app1-snap\",\"reconcileID\":\"5af23527-5d34-4992-ab02-99ba4af2a401\",\"finalizer\":\"dataservices.nutanix.com/app-snap-2a003355dcfd33a75426095a4a71154b\"}\n</code></pre> </li> <li> <p>Observe the event <code>Create Recovery Point</code> operation in Prism Central </p> <p></p> </li> <li> <p>Delete the app to simulate a failure</p>  Command <pre><code>kubectl delete po,pvc -l app=app1 --force\n</code></pre> </li> <li> <p>Restore the app from <code>applicationSnapshot</code> custom resource</p>  Command Command Output <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: ApplicationSnapshotRestore\nmetadata:\n  name: app1-restore\nspec:\n  applicationSnapshotName: app1-snap\nEOF\n</code></pre> <pre><code>kubectl get applicationsnapshotrestore.dataservices.nutanix.com/app1-restore -w\n</code></pre></p> <p><pre><code>applicationsnapshotrestore.dataservices.nutanix.com/app1-restore created\n</code></pre> Wait until the status of snapshot becomes true<pre><code>kubectl get applicationsnapshotrestore.dataservices.nutanix.com/app1-restore -w\n\nNAME           SNAPSHOT-NAME   COMPLETED\napp1-restore   app1-snap       false\napp1-restore   app1-snap       false\napp1-restore   app1-snap       false\napp1-restore   app1-snap       true\n</code></pre></p> </li> <li> <p>Monitor the progress of <code>ApplicationSnapshotRestore</code> custom resource</p>  Command Command Output <pre><code>kubectl get applicationsnapshotrestore.dataservices.nutanix.com/app1-restore -w\n</code></pre> Wait until the status of restore becomes true<pre><code>kubectl get applicationsnapshotrestore.dataservices.nutanix.com/app1-restore -w\n\nNAME           SNAPSHOT-NAME   COMPLETED\napp1-restore   app1-snap       false\napp1-restore   app1-snap       false\napp1-restore   app1-snap       false\napp1-restore   app1-snap       true\n</code></pre> </li> <li> <p>Verify if app1 pvc and pod are restored</p>  Command Command Output <pre><code>kubectl get po,pvc -l app=app1\n</code></pre> <pre><code>$  kubectl get po,pvc -l app=app1\n\nNAME        READY   STATUS    RESTARTS   AGE\npod/app-1   1/1     Running   0          4m53s\n\nNAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS     VOLUMEATTRIBUTESCLASS   AGE\npersistentvolumeclaim/az-claim-1   Bound    pvc-55a6b812-35de-4db2-b0e1-8a55e1b4e41f   4Gi        RWO            nutanix-volume   &lt;unset&gt;                 5m54s\n</code></pre> </li> <li> <p>Check if data is present within the data mount <code>/data</code> inside the pod</p>  Command Command Output <pre><code>kubectl exec -it app-1 -- /bin/sh -c \"wc -l /data/abc.txt\"\n</code></pre> <pre><code>kubectl exec -it app-1 -- /bin/sh -c \"wc -l /data/abc.txt\"\n10000 /data/abc.txt\n</code></pre> </li> <li> <p>Observe the event <code>Restore Recovery Point</code> operation in Prism Central </p> <p></p> </li> </ol>"},{"location":"nkp_ndk/nkp_ndk_singlek8s/#cross-namespace-recovery","title":"Cross Namespace Recovery","text":"<p>NDK offers cross-namespace recovery capabilites. With this NKP or any supported K8s platform administrator can recover <code>applicationSnapshot</code> custom resource to a different namespace within the same K8s cluster.</p> <ol> <li> <p>Create a referenceGrant resouce to grant permission to restore specific application snapshots from one namespace to another.</p>  Template Command Sample Command Command output <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: _reference_grant_name\n  namespace: _source_namespace\nspec:\n  from:\n  - group: dataservices.nutanix.com\n    kind: ApplicationSnapshotRestore\n    namespace: _target_namespace\n  to:\n  - group: dataservices.nutanix.com\n    kind: ApplicationSnapshot\n    name: _appplication_snapshot_name\nEOF\n</code></pre> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: cross-ns-rg\n  namespace: default\nspec:\n  from:\n  - group: dataservices.nutanix.com\n    kind: ApplicationSnapshotRestore\n    namespace: restore\n  to:\n  - group: dataservices.nutanix.com\n    kind: ApplicationSnapshot\n    name: app1-snap\nEOF\n</code></pre> <pre><code>referencegrant.gateway.networking.k8s.io/cross-ns-rg created\n</code></pre> </li> <li> <p>Create a target namespace</p>  Template Command Sample Command <pre><code>kubectl create namespace restore\nkubens restore\n</code></pre> <pre><code>namespace/restore created\n\nContext \"nkplb-admin@nkplb\" modified.\nActive namespace is \"restore\".\n</code></pre> </li> <li> <p>Create and <code>applicationSnapshotRestore</code> custom resource to restore to target <code>restore</code> namespace</p>  Template Command Sample Command <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: ApplicationSnapshotRestore\nmetadata:\n name: _restore-snapshot-name\n namespace: _target_namespace\nspec:\n applicationSnapshotName: _snapshot_name\n applicationSnapshotNamespace: _snapshot_source_namespace\nEOF\n</code></pre> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: dataservices.nutanix.com/v1alpha1\nkind: ApplicationSnapshotRestore\nmetadata:\n name: cross-ns-asr\n namespace: restore\nspec:\n applicationSnapshotName: app1-snap\n applicationSnapshotNamespace: default\nEOF\n</code></pre> </li> <li> <p>Monitor the progress of <code>ApplicationSnapshotRestore</code> custom resource</p>  Command Command Output <p><pre><code>kubectl describe applicationsnapshotrestore.dataservices.nutanix.com/cross-ns-asr\n</code></pre> <pre><code>kubectl get applicationsnapshotrestore.dataservices.nutanix.com/app1-restore -w\n</code></pre></p> <p>Wait until the status of ApplicationSnapshotRestore custom resource is true<pre><code>Name:         cross-ns-asr\nNamespace:    restore\n\nAPI Version:  dataservices.nutanix.com/v1alpha1\nKind:         ApplicationSnapshotRestore\nMetadata:\nCreation Timestamp:  2025-07-08T05:40:31Z\nFinalizers:\n    dataservices.nutanix.com/application-restore\nGeneration:        1\nResource Version:  7484636\n\nSpec:\nApplication Snapshot Name:       app1-snap\nApplication Snapshot Namespace:  default\nStatus:\nCompleted:  true\n\nConditions:\n    Last Transition Time:  2025-07-08T05:40:31Z\n    Message:               \n    Observed Generation:   1\n    Reason:                RequestCompleted\n    Status:                False\n    Type:                  Progressing\n    Last Transition Time:  2025-07-08T05:40:31Z\n    Message:               All prechecks passed and finalizers on dependent resources set\n    Observed Generation:   1\n    Reason:                PrechecksPassed\n    Status:                True\n    Type:                  PrechecksPassed\n    Last Transition Time:  2025-07-08T05:40:31Z\n    Message:               Restore requests for all eligible volumes submitted\n    Observed Generation:   1\n    Reason:                VolumeRestoreRequestsSubmitted\n    Status:                True\n    Type:                  VolumeRestoreRequestsSubmitted\n    Last Transition Time:  2025-07-08T05:41:32Z\n    Message:               All eligible application configs restored\n    Observed Generation:   1\n    Reason:                ApplicationConfigRestored\n    Status:                True\n    Type:                  ApplicationConfigRestored\n    Last Transition Time:  2025-07-08T05:41:47Z\n    Message:               All eligible volumes restored\n    Observed Generation:   1\n    Reason:                VolumesRestored\n    Status:                True\n    Type:                  VolumesRestored\n    Last Transition Time:  2025-07-08T05:41:47Z\n    Message:               Application restore successfully finalised\n    Observed Generation:   1\n    Reason:                ApplicationRestoreFinalised\n    Status:                True\n    Type:                  ApplicationRestoreFinalised\nFinish Time:             2025-07-08 05:41:47\nStart Time:              2025-07-08 05:40:31\n</code></pre> <pre><code>kubectl get applicationsnapshotrestore/cross-ns-asr -w\nNAME           SNAPSHOT-NAME   COMPLETED\ncross-ns-asr   app1-snap       true\n</code></pre></p> </li> <li> <p>Verify if app1 pvc and pod are restored</p>  Command Command Output <pre><code>kubectl get po,pvc -l app=app1\n</code></pre> <pre><code>$  kubectl get po,pvc -l app=app1\n\nNAME        READY   STATUS    RESTARTS   AGE\npod/app-1   1/1     Running   0          4m53s\n\nNAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS     VOLUMEATTRIBUTESCLASS   AGE\naz-claim-1   Bound    pvc-9471ed41-64cb-4d57-bc62-fe94d9a3f672   1Gi        RWO            nutanix-volume   &lt;unset&gt;                 119s\n</code></pre> </li> <li> <p>Check if data is present within the data mount <code>/data</code> inside the pod</p>  Command Command Output <pre><code>kubectl exec -it app-1 -- /bin/sh -c \"wc -l /data/abc.txt\"\n</code></pre> <pre><code>kubectl exec -it app-1 -- /bin/sh -c \"wc -l /data/abc.txt\"\n10000 /data/abc.txt\n</code></pre> </li> </ol> <p>We have now successfully restored our application accross namespaces.</p>"},{"location":"nkp_tutorials/","title":"Getting Started","text":"<p>In this part of the lab we will deploy a workload on NKP cluster which stores its database in a NDB deployed Postgres server.</p> <p>We will also deploy a Kubernetes cluster so far as per the NVD design requirements.</p> <p>Deploy the NKP kubernetes cluster with the following components:</p> <ul> <li>3 x Control plane nodes</li> <li>3 x Worker nodes</li> </ul> <p>The following is the flow of the NAI lab:</p> <pre><code>stateDiagram-v2\n    direction LR\n\n    state PrepEnvironment {\n        [*] --&gt; PrepVM\n        PrepVM --&gt; [*]\n    }\n\n    state InstallNDBOperator {\n        [*] --&gt; InstallNDBOp\n        InstallNDBOp --&gt; [*]\n    }\n\n    state ConfigureSecrets {\n        [*] --&gt; CreateSecrets\n        CreateSecrets --&gt; [*]\n    }\n\n    state SetupNDB {\n        [*] --&gt; GetUUID\n        GetUUID --&gt; CreateProfile\n        CreateProfile --&gt; DeployDB\n        DeployDB --&gt; [*]\n    }\n\n    state VerifyDB {\n        [*] --&gt; CheckDB\n        CheckDB --&gt; [*]\n    }\n\n    state DeployApplication {\n        [*] --&gt; DeployApp\n        DeployApp --&gt; [*]\n    }\n\n    state ConfigureIngress {\n        [*] --&gt; CreateIngress\n        CreateIngress --&gt; [*]\n    }\n\n    state TestApplication {\n        [*] --&gt; TestFrontend\n        TestFrontend --&gt; TestBackend\n        TestBackend --&gt; CheckDBData\n        CheckDBData --&gt; [*]\n    }\n\n    state CleanupResources {\n        [*] --&gt; Cleanup\n        Cleanup --&gt; [*]\n    }\n\n    [*] --&gt; PrepEnvironment\n    PrepEnvironment --&gt; InstallNDBOperator\n    InstallNDBOperator --&gt; ConfigureSecrets\n    ConfigureSecrets --&gt; SetupNDB\n    SetupNDB --&gt; VerifyDB\n    VerifyDB --&gt; DeployApplication\n    DeployApplication --&gt; ConfigureIngress\n    ConfigureIngress --&gt; TestApplication\n    TestApplication --&gt; CleanupResources\n    CleanupResources --&gt; [*]</code></pre>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/","title":"Nutanix MCP Server","text":"<p>This lab walks through a Nutanix Model Context Protocol (MCP) server to get details from a Nutanix Prism cluster using MCP implementation.</p> <p>This lab is aimed to illustrate implementing MCP servers on Nutanix using Nutanix solutions (NKP and NAI).</p> <p>Warning</p> <p>MCP Nutanix is an experimental project at this time. See Nutanix MCP gitrepo for details. </p> <p>This project was created as a personal project to explore the capabilities of the Model Context Protocol frameworks in Go. It is:</p> <ul> <li>NOT an official Nutanix product or tool</li> <li>NOT supported, endorsed, or maintained by Nutanix</li> <li>NOT suitable for production environments</li> <li>PROVIDED AS-IS with no warranties or guarantees</li> </ul> <p>Lab Duration</p> <p>Estimated time to complete this lab is 30 minutes provided NKP and NAI are ready to use</p>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Nutanix Kubernetes Platform NKP cluster <code>v1.16</code> or later deployed, accessible via <code>kubectl</code>. See NKP Deployment for NKP install instructions.</li> <li>Nutanix CSI driver installed for storage integration. (pre-installed with NKP)</li> <li>Networking configured to allow communication between the Kubernetes cluster and NAI.</li> <li>Envoy Gateway controller installed for external access (pre-installed during NAI install)</li> <li>Optional - Traefik Ingress controller installed for external access (pre-installed during NAI install)</li> <li>Linux Tools VM or equivalent environment with <code>kubectl</code>, <code>helm</code>, <code>curl</code>, and <code>jq</code> installed.</li> <li>NAI <code>v2.4.0</code> or later server credentials and hosted LLM (gemini 2B as an example)</li> <li>Optional - access to publicly available LLM</li> </ul>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/#high-level-overview","title":"High-Level Overview","text":"<ol> <li>Deploy Toolhive Operator in NKP (kubernetes) cluster</li> <li>Deploy MCP server for Nutanix in NKP cluster</li> <li>Configure Ingress for Nutanix MCP Server</li> <li>Install VSCode Cline Client with MCP Server </li> <li>Install n8n AI workflow automation tool</li> <li>Configure n8n MCP Client to connect to Nutanix MCP Server and chat (ask questions)</li> </ol>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/#deploy-toolhive-operator-in-nkp-kubernetes-cluster","title":"Deploy Toolhive Operator in NKP (kubernetes) cluster","text":""},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/#install-thv","title":"Install THV","text":"<p>Toolhive is used to simplify deployment and management of MCP servers. </p> <p>We will install THV on the jumpbox to build our MCP server. </p> <ol> <li> <p>Log in to your Linux Tools VM (e.g., via SSH as <code>ubuntu</code>).</p> </li> <li> <p>Download and install stable <code>thv</code> binaries</p> <pre><code>mkdir $HOME/mcp\ncd $HOME/mcp\ncurl -o toolhive_0.3.7_linux_amd64.tar.gz \"https://github.com/stacklok/toolhive/releases/download/v0.3.7/toolhive_0.3.7_linux_amd64.tar.gz\"\n</code></pre> <pre><code>tar -xzf toolhive_0.3.7__amd64.tar.gz\nsudo mv thv /usr/local/bin/\nsudo chmod +x /usr/local/bin/thv\n</code></pre> </li> <li> <p>Verify <code>thv</code> version</p> CommandCommand output <pre><code>thv version\n</code></pre> <pre><code>ToolHive v0.3.7\nCommit:.  abb22d80a5321342530583bbc82aeadef718e943\nBuilt: 2025-10-06 08:45:05 UTC\nGo version: go1.24.4\nPlatform: linux/amd64\n</code></pre> </li> </ol>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/#prepare-mcp-server-image","title":"Prepare MCP Server Image","text":"<ol> <li> <p>Login to VSC on the jumphost VM, append the following environment variables to the <code>$HOME\\mcp\\.env</code> file and save it</p> Template .envSample .env <pre><code>export REGISTRY_URL=_your_registry_url\nexport REGISTRY_HOST=_your_registry_host\nexport REGISTRY_USERNAME=_your_registry_username\nexport REGISTRY_PASSWORD=_your_registry_password\nexport REGISTRY_CACERT=_path_to_ca_cert_of_registry  # (1)!\n</code></pre> <ol> <li>File must contain CA server and Harbor server's public certificate in one file</li> </ol> <pre><code>export REGISTRY_URL=https://harbor.10.x.x.111.nip.io/nkp\nexport REGISTRY_URL=harbor.10.x.x.111.nip.io/nkp\nexport REGISTRY_USERNAME=admin\nexport REGISTRY_PASSWORD=xxxxxxxx\nexport REGISTRY_CACERT=$HOME/harbor/certs/full_chain.pem\n</code></pre> </li> <li> <p>Create Docker file to build MCP server image for Nutanix</p> <pre><code>thv build --tag mcp-nutanix:latest go://github.com/thunderboltsid/mcp-nutanix@latest -o Dockerfile\n</code></pre> </li> <li> <p>Create container image</p> <pre><code>thv build --tag mcp-nutanix:latest go://.\n</code></pre> </li> <li> <p>Validate that docker architecture matches linux/amd64</p> CommandCommand output <pre><code>docker image inspect mcp-nutanix:latest --format '{{.Architecture}}{{.Os}}'\n</code></pre> <pre><code>amd64linux\n</code></pre> </li> <li> <p>Upload to a private container registry to be able to deploy in Kubernetes</p> CommandCommand output <pre><code>docker login ${REGISTRY_HOST} -u ${REGISTRY_USERNAME} -p ${REGISTRY_PASSWORD}\ndocker tag mcp-nutanix:latest ${REGISTRY_HOST}/mcp-nutanix:latest\ndocker push ${REGISTRY_HOST}/mcp-nutanix:latest\n</code></pre> <pre><code>Login Succeeded\n\nThe push refers to repository [harbor.10.x.x.124/nkp/mcp-nutanix]\n667e3f0433e8: Pushed \nd03eeff57712: Pushed \n73e1ce48ffe2: Pushed \n06fb002250ae: Pushed \n256f393e029f: Pushed \nlatest: digest: sha256:852ca2e96f5a288e10cd058c38dc5a22ffb7003b41cda5faa28ba794d36154e6 size: 1363\n</code></pre> </li> <li> <p>Configure <code>kubectl</code> to access your NKP Kubernetes cluster:</p> CommandSample command <pre><code>export KUBECONFIG=$HOME/_nkp_install_dir/nkpclustername.conf\n</code></pre> <pre><code>export KUBECONFIG=$HOME/nkp/nkpdev.conf\n</code></pre> </li> <li> <p>Install MCP server CRD in Kubernetes cluster</p> CommandCommand output <pre><code>helm upgrade -i toolhive-operator-crds \\\noci://ghcr.io/stacklok/toolhive/toolhive-operator-crds\n</code></pre> <pre><code>oci://ghcr.io/stacklok/toolhive/toolhive-operator-crds\nRelease \"toolhive-operator-crds\" does not exist. Installing it now.\nPulled: ghcr.io/stacklok/toolhive/toolhive-operator-crds:0.0.33\nDigest: sha256:aa24f2ddcd93b0582a5f9edf3465bdab835d62e6dcbf969f63666604815f502e\nNAME: toolhive-operator-crds\nLAST DEPLOYED: Fri Oct 10 02:30:44 2025\nNAMESPACE: n8n\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> </li> <li> <p>Install Toolhive operator</p> CommandCommand output <pre><code>helm upgrade -i toolhive-operator \\\noci://ghcr.io/stacklok/toolhive/toolhive-operator \\\n-n toolhive-system --create-namespace\n</code></pre> <pre><code>Release \"toolhive-operator\" does not exist. Installing it now.\nPulled: ghcr.io/stacklok/toolhive/toolhive-operator:0.2.20\nDigest: sha256:ad78fd2ca7c8eac82ac3d071b6a110853095b66c22e0cc4d6f2e6a9d03aea69a\nNAME: toolhive-operator\nLAST DEPLOYED: Fri Oct 10 02:34:29 2025\nNAMESPACE: toolhive-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> </li> <li> <p>Verify Toolhive installation</p> CommandCommand output <pre><code>kubens toolhive-system\nkubectl get pods\n</code></pre> <pre><code>NAME                                 READY   STATUS    RESTARTS   AGE\ntoolhive-operator-5bcb8dbfd4-2zmzw   1/1     Running   0          53s\n</code></pre> </li> </ol>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/#deploy-mcp-server-for-nutanix-in-nkp-cluster","title":"Deploy MCP server for Nutanix in NKP cluster","text":"<ol> <li> <p>Append the following variables to .env file for Nutanix Prism information</p> Template .envSample .env <pre><code>export NUTANIX_USERNAME=_your_nutanix_username\nexport NUTANIX_PASSWORD=_your_nutanix_password\nexport NUTANIX_ENDPOINT=_your_prism_central_fqdn\n</code></pre> <pre><code>export NUTANIX_USERNAME=admin\nexport NUTANIX_PASSWORD=xxxxxxxx\nexport NUTANIX_ENDPOINT=pc.example.com\n</code></pre> </li> <li> <p>Create docker registry secret to pull mcp-nutanix image from private harbor repository</p> <pre><code>kubectl create secret docker-registry regcred \\\n--docker-server=${REGISTRY_URL} \\\n--docker-username=${REGISTRY_USERNAME} \\\n--docker-password=${REGISTRY_PASSWORD} \\\n-n toolhive-system\n</code></pre> </li> <li> <p>Deploy MCP server for Nutanix using the pushed image from private harbor repository</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: toolhive.stacklok.dev/v1alpha1\nkind: MCPServer\nmetadata:\n  name: nutanix\n  namespace: toolhive-system\nspec:\n  image: ${REGISTRY_HOST}/mcp-nutanix:latest\n  transport: stdio\n  port: 8080\n  permissionProfile:\n    type: builtin\n    name: network\n  env:\n  - name: NUTANIX_ENDPOINT\n    value: \"${NUTANIX_ENDPOINT}\"\n  - name: NUTANIX_USERNAME\n    value: \"${NUTANIX_USERNAME}\"\n  - name: NUTANIX_PASSWORD\n    value: \"${NUTANIX_PASSWORD}\"\n  - name: NUTANIX_INSECURE\n    value: \"true\"\n  podTemplateSpec:\n    spec:\n      imagePullSecrets:\n      - name: regcred\n      containers:\n        - name: mcp\n          resources:\n            limits:\n              cpu: \"500m\"\n              memory: \"512Mi\"\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n  resources:\n    limits:\n      cpu: \"100m\"\n      memory: \"128Mi\"\n    requests:\n      cpu: \"50m\"\n      memory: \"64Mi\"\nEOF\n</code></pre> </li> <li> <p>Check the health of the <code>MCPServer</code> resource and make sure it is running</p> Template commandCommand output <pre><code>kubectl get mcpserver\n</code></pre> <pre><code>NAME      STATUS    URL                                                               AGE\nnutanix   Running   http://mcp-nutanix-proxy.toolhive-system.svc.cluster.local:8080   78s\n</code></pre> </li> <li> <p>Expose MCP Server <code>Ingress</code> for external access. We will be using the pre-deployed <code>Traefik</code> ingress that comes with NKP and expose MCP server as a subdomain.</p> </li> <li> <p>Get the IP address of the NKP pre-deployed Traefik Ingress</p> <pre><code>INGRESS_IP=$(kubectl get ingress kommander-kommander-ui -n kommander -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre> <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mcp-nutanix-proxy-ingress\n  namespace: toolhive-system\n  annotations:\n    kubernetes.io/ingress.class: kommander-traefik\n    traefik.ingress.kubernetes.io/router.tls: \"false\"\nspec:\n  rules:\n  - host: mcp-nutanix.${INGRESS_IP}.io\n    http:\n      paths:\n      - backend:\n          service:\n            name: mcp-nutanix-proxy\n            port:\n              number: 8080\n        path: /\n        pathType: Prefix\nEOF\n</code></pre> </li> <li> <p>Check the health of <code>MCPServer</code> resource</p> Template commandCommand output <pre><code>curl -k https://mcp-nutanix.10.x.x.215.nip.io/health\n</code></pre> <pre><code>{\"status\":\"healthy\",\"timestamp\":\"2025-10-10T02:57:16.51932436Z\",\"version\":{\"version\":\"v0.3.7\",\"commit\":\"abb22d80a5321342530583bbc82aeadef718e943\",\"build_date\":\"2025-10-06 11:31:07 +0300\",\"go_version\":\"go1.24.4\",\"platform\":\"linux/amd64\"},\"transport\":\"stdio\",\"mcp\":{\"available\":true,\"response_time_ms\":0,\"last_checked\":\"2025-10-10T02:57:16.519338081Z\"}}\n</code></pre> </li> <li> <p>Test the health of SSE endpoints</p> Template commandCommand output <pre><code>curl -k -N -H \"Accept: text/event-stream\" https://mcp-nutanix.10.x.x.215.nip.io/sse#mcp-nutanix\n</code></pre> <pre><code>event: message\ndata: {\"jsonrpc\":\"2.0\",\"id\":\"ping_1760065217411075629\",\"result\":{}}\n\nevent: message\ndata: {\"jsonrpc\":\"2.0\",\"id\":\"ping_1760065222410782180\",\"result\":{}}\n</code></pre> </li> </ol>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/#testing-mcp-server","title":"Testing MCP Server","text":""},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_mcp/#using-cline","title":"Using Cline","text":"<p>Tip</p> <p>Cline Documentation for MCP server setup instructions.</p> <ol> <li>In VSCode &gt; Go to Settings &gt; Extensions</li> <li>Search for Cline and Install (from the official developer Cline Inc.)</li> <li>Sign up for Cline (for free LLM usage)</li> <li>Use your favourtie LLM</li> <li>Click the MCP Servers icon in the top navigation bar of the Cline extension</li> <li>Select the Configure tab, and then Click the Configure MCP Servers link at the bottom of that pane.</li> <li> <p>Cline will open a new settings config file in VSCode. Paste the following after changing your MCP server and Nutanix PC connection details</p> Template fileSample file <pre><code>{\n\"mcpServers\": {\n    \"mcp-server-name\": {\n      \"disabled\": true,\n      \"timeout\": 60,\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-remote\",\n        \"https://mcp-nutanix.${INGRESS_IP}.nip.io/sse#mcp-nutanix\",\n        \"--allow-http\",\n        \"--transport sse-only\"\n      ],\n      \"env\": {\n        \"NUTANIX_ENDPOINT\": \"$NUTANIX_ENDPOINT\",\n        \"NUTANIX_USERNAME\": \"$NUTANIX_USERNAME\",\n        \"NUTANIX_PASSWORD\": \"$NUTANIX_PASSWORD\",\n        \"NUTANIX_INSECURE\": \"true\"\n      }\n    }\n}\n}\n</code></pre> <pre><code>{\n  \"mcpServers\": {\n    \"nutanix-pc\": {\n      \"disabled\": true,\n      \"timeout\": 60,\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-remote\",\n        \"https://mcp-nutanix.10.x.x.215.nip.io/sse#mcp-nutanix\",\n        \"--allow-http\",\n        \"--transport sse-only\"\n      ],\n      \"env\": {\n        \"NUTANIX_ENDPOINT\": \"pc.example.com\",\n        \"NUTANIX_USERNAME\": \"admin\",\n        \"NUTANIX_PASSWORD\": \"XXXXXXXXXXX\",\n        \"NUTANIX_INSECURE\": \"true\"\n      }\n    }\n  }\n}\n</code></pre> </li> <li> <p>Save the Cline extension config file</p> </li> <li> <p>Go to Cline chat and ask questions about the Nutanix PC environment</p> <p></p> </li> </ol>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_n8n/","title":"Deploy Nutanix MCP Server N8N Client","text":"<p>This lab is a continuation of the MCP lab with N8N server as an interfacing tool. </p>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_n8n/#high-level-overview","title":"High-Level Overview","text":"<ol> <li>Install n8n AI workflow automation tool</li> <li>Configure n8n MCP Client to connect to Nutanix MCP Server and chat (ask questions)</li> </ol>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_n8n/#install-n8n","title":"Install N8N","text":"<p>N8N is a a fair-code licensed workflow automation tool that combines AI capabilities with business process automation.</p> <p>We will install and self-host N8N on the NKP Server and use it to automate a chat AI Agent node using the Nutanix MCP Server which we deployed in the previous section. </p> <ol> <li> <p>Log in to your Linux Tools VM (e.g., via VSCode as <code>ubuntu</code>). and open Terminal</p> </li> <li> <p>Download N8N Manifests to deploy N8N in NKP cluster</p> <pre><code>mkdir $HOME/n8n\ncd $HOME/n8n\ngit clone https://github.com/n8n-io/n8n-hosting.git\ncd n8n-hosting/\n</code></pre> </li> <li> <p>Add the following n8n community node for use as tools with our AI Agent Node. We will do this by modifying the manifest file before deploying n8n.</p> <pre><code>cd n8n-hosting/kubernetes/\ncode n8n-deployment.yaml  # (1)  \n</code></pre> <ol> <li>Use <code>vim</code> as an alternate editor</li> </ol> </li> <li> <p>Add the following highlighted lines to the <code>n8n-deployment.yaml</code> file under the <code>env</code> section</p> <pre><code>   env:\n     &lt; Snipped for brevity &gt;\n     - name: N8N_PROTOCOL\n       value: http\n     - name: N8N_PORT\n       value: \"5678\"\n     - name: NODE_TLS_REJECT_UNAUTHORIZED\n       value: \"false\"\n     - name: N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE\n       value: \"true\"\n</code></pre> </li> <li> <p>Deploy <code>n8n</code></p> CommandCommand output <pre><code>export KUBECONFIG=$HOME/_nkp_install_dir/_nkpclustername.conf\nkubectl apply -f . \n</code></pre> <pre><code>export KUBECONFIG=$HOME/nkp/nkpdev.conf\n#\nhttproute.gateway.networking.k8s.io/n8n-chatapp-route created\npersistentvolumeclaim/n8n-claim0 created\ndeployment.apps/n8n created\nservice/n8n created\nnamespace/n8n created\npersistentvolumeclaim/postgresql-pv created\nconfigmap/init-data created\ndeployment.apps/postgres created\nsecret/postgres-secret created\nservice/postgres-service created\n</code></pre> </li> <li> <p>Check <code>n8n</code> services </p> CommandCommand output <pre><code>kubens n8n \nkubectl get all\n</code></pre> <pre><code>Active namespace is \"n8n\".\n#\nNAME                            READY   STATUS    RESTARTS        AGE\npod/n8n-cf6b5b8db-ql5db         1/1     Running   0               2m\npod/postgres-57dbbcbb58-mpnq8   1/1     Running   1 (2m ago)      2m\n\nNAME                       TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE\nservice/n8n                LoadBalancer   10.99.29.36   &lt;pending&gt;     5678:32488/TCP   2m\nservice/postgres-service   ClusterIP      None          &lt;none&gt;        5432/TCP         2m\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/n8n        1/1     1            1           2m\ndeployment.apps/postgres   1/1     1            1           2m\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/n8n-bc8878667         0         0         0       2m\nreplicaset.apps/n8n-cf6b5b8db         1         1         1       2m\nreplicaset.apps/postgres-57dbbcbb58   1         1         1       2m\n</code></pre> </li> <li> <p>If Envoy Gateway is installed as a part of NAI install use the following to create a <code>HTTPRoute</code> resource to access n8n      <pre><code>kubectl apply -f -&lt;&lt;EOF\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: n8n-chatapp-route\n  namespace: n8n                    # Same namespace as your chat app service\nspec:\n  parentRefs:\n  - name: nai-ingress-gateway\n    namespace: nai-system           # Namespace of the Gateway\n  hostnames:\n  - \"n8n.nai.10.x.x.216.nip.io\"     # Input Gateway IP address\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /\n    backendRefs:\n    - name: n8n\n      kind: Service\n      port: 5678\nEOF\n</code></pre></p> </li> <li> <p>Now n8n will be available at the following URL <code>n8n.nai.10.x.x.216.nip.io</code></p> </li> <li>Register for n8n with an email</li> <li>An activation key will be sent to email address</li> </ol>"},{"location":"nkp_tutorials/nkp_mcp_lab/nkp_nai_n8n/#setup-n8n-workflow-with-nutanix-mcp-server","title":"Setup N8N Workflow with Nutanix MCP Server","text":"<p>This workflow is a simple example of how to use the MCP server for Nutanix in an N8N workflow. It uses the AI Agent node to interact with the MCP server via the MCP Client node. In the following workflow, we'll be using the <code>n8n-nodes-mcp</code> community node to interact with the Nutanix MCP server.</p> <ol> <li> <p>Add Community Node <code>n8n-node-mcp</code> to n8n instance under Settings</p> <p></p> </li> <li> <p>Add AI Agent node to workflow and Configure OpenAI Chat Model, Simple Memory and other settings as needed.</p> <p>For the prompt, use the following:</p> <pre><code>You are an AI agent that can interact with a Nutanix cluster using the MCP server.\nYou can list VMs, get VM details, and perform actions like start, stop, and reboot VMs.\nUse the MCP Client node to execute commands on the Nutanix cluster.\n</code></pre> <p></p> </li> <li> <p>Add the NAI LLM Model deployed here and set up with credentials</p> <ul> <li>API Key: Use the one generated in NAI </li> <li>Base URL: <code>https://nai.10.x.x.216.nip.io/api/v1</code></li> </ul> <p></p> </li> <li> <p>Choose Simple Memory for storing in-flight data</p> </li> <li> <p>Search and add <code>MCP Client Tool</code> (via npm) tool to workflow and name it Nutanix MCP List</p> </li> <li> <p>For Credentials - create New Credential and Configure MCP Client node to use the Nutanix MCP server SSE endpoint</p> <p>SSE URL: </p> <pre><code>https://mcp-nutanix.${INGRESS_IP}.nip.io/sse#mcp-nutanix\n</code></pre> <p>Additional Headers: Copy and paste the following (with actual values for Nutanix Prism connectivity)</p> <pre><code>NUTANIX_ENDPOINT=\"$NUTANIX_ENDPOINT\",NUTANIX_USERNAME=\"$NUTANIX_USERNAME\",NUTANIX_PASSWORD=\"$NUTANIX_PASSWORD\",NUTANIX_INSECURE=\"true\"\n</code></pre> <p></p> </li> <li> <p>Run Execute Step to test connection and view available commands</p> <p></p> </li> <li> <p>Add another MCP Client (via npm) tool node to workflow and name it something meaningful like Nutanix MCP Execute</p> </li> <li>For credential use the Nutanix MCP server SSE endpoint (previously configured)</li> <li> <p>For Tool Name, select Expression tab and provide following</p> <pre><code>{{ $fromAI(\"tool\",\"the tool selected\") }}\n</code></pre> </li> <li> <p>Click on Execute Step</p> </li> <li> <p>Type <code>vm_list</code>  in the input text box and click on Execute Step</p> <p></p> <p></p> </li> <li> <p>Run the workflow to verify configuration</p> <p></p> </li> <li> <p>In the AI Chat window, ask questions about the Nutanix cluster</p> <p></p> </li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/","title":"NKP Workload with NDB Database","text":""},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#install-custom-three-layer-application","title":"Install Custom Three-Layer Application","text":"<p>This section deploys a three-layer application (React frontend, Django backend, Postgres database) based on this blog, adapted for Kubernetes and NDB.</p>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#create-database-schema-and-data","title":"Create Database Schema and Data","text":"<ol> <li>Apply application secrets:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/app-secrets.yaml -n ndb\n</code></pre></li> <li>Download and edit the ConfigMap:    <pre><code>curl -LO https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/app-variables.yaml\n</code></pre>    Update <code>DB_HOST</code> and <code>DB_PORT</code> in <code>app-variables.yaml</code>:    <pre><code>DB_HOST: dbforflower-svc\nDB_PORT: \"80\"\n</code></pre></li> <li>Apply the ConfigMap:    <pre><code>kubectl apply -f app-variables.yaml -n ndb\n</code></pre></li> <li>Run the Django job to populate the database:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/django-job.yaml -n ndb\n</code></pre></li> <li>Monitor the job:    <pre><code>kubectl get job django-job -n ndb -w\n</code></pre>    Example output:    <pre><code>NAME         COMPLETIONS   DURATION   AGE\ndjango-job   1/1           19s        20s\n</code></pre></li> <li>Check job logs:    <pre><code>kubectl logs -n ndb $(kubectl get pod -n ndb -l job-name=django-job -o jsonpath='{.items[0].metadata.name}')\n</code></pre></li> <li>Verify new tables:    <pre><code>kubectl exec -it psql -n ndb -- psql -h dbforflower-svc -p 80 -U postgres -d predictiondb\n</code></pre> <pre><code>\\dt\n</code></pre>    Expected output: ~11 tables (e.g., <code>auth_user</code>, <code>django_migrations</code>).</li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#install-frontend-and-backend","title":"Install Frontend and Backend","text":"<ol> <li>Deploy Django and React applications:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/django-deployment.yaml -n ndb\nkubectl apply -f https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/react-deployment.yaml -n ndb\n</code></pre></li> <li>Verify pods:    <pre><code>kubectl get pods -n ndb\n</code></pre></li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#create-ingress-for-access","title":"Create Ingress for Access","text":"<ol> <li> <p>Create an Ingress resource to replace OpenShift Routes:</p> <pre><code>cat &lt;&lt; EOF &gt; ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: flower-app\n  namespace: ndb\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\n    traefik.ingress.kubernetes.io/router.pathmatcher: PathPrefix\nspec:\n  rules:\n  - host: flower.apps.k8suserXX.ntnxlab.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: react-cluster-ip-service\n            port:\n              number: 80\n      - path: /admin\n        pathType: Prefix\n        backend:\n          service:\n            name: django-cluster-ip-service\n            port:\n              number: 80\n      - path: /api\n        pathType: Prefix\n        backend:\n          service:\n            name: django-cluster-ip-service\n            port:\n              number: 80\n      - path: /static/admin/\n        pathType: Prefix\n        backend:\n          service:\n            name: django-cluster-ip-service\n            port:\n              number: 80\n      - path: /static/rest_framework/\n        pathType: Prefix\n        backend:\n          service:\n            name: django-cluster-ip-service\n            port:\n              number: 80\n      - path: /static/\n        pathType: Prefix\n        backend:\n          service:\n            name: django-cluster-ip-service\n            port:\n              number: 80\n      - path: /media/\n        pathType: Prefix\n        backend:\n          service:\n            name: django-cluster-ip-service\n            port:\n              number: 80\nEOF\n</code></pre> </li> <li> <p>Replace <code>k8suserXX</code> with your user ID:</p> <pre><code>sed -i 's/k8suserXX/your_user_id/g' ingress.yaml\n</code></pre> </li> <li> <p>Apply the Ingress:</p> <pre><code>kubectl apply -f ingress.yaml -n ndb\n</code></pre> </li> <li> <p>Verify if Ingress resources are created</p> <pre><code>kubectl get ingress -n ndb\n</code></pre> </li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#test-frontend-react-application","title":"Test Frontend React Application","text":"<ol> <li>Update your local hosts file:    <pre><code>10.38.18.220 flower.apps.k8suserXX.ntnxlab.local\n</code></pre></li> <li>Access the React app:    <pre><code>http://flower.apps.k8suserXX.ntnxlab.local/\n</code></pre></li> <li>Log in with:<ul> <li>Username: admin</li> <li>Password: admin_password</li> </ul> </li> <li>Use the sliders to predict flower names.</li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#test-backend-django-application","title":"Test Backend Django Application","text":"<ol> <li>Access the Django admin:    <pre><code>http://flower.apps.k8suserXX.ntnxlab.local/admin\n</code></pre></li> <li>Log in with:<ul> <li>Username: admin</li> <li>Password: admin_password</li> </ul> </li> <li>Add a new user:<ul> <li>Username: xyz-user</li> <li>Password: your_password</li> </ul> </li> <li>Save and test the new user in the React app.</li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#check-postgres-database-data","title":"Check Postgres Database Data","text":"<ol> <li>Connect to the database:    <pre><code>kubectl exec -it psql -n ndb -- psql -h dbforflower-svc -p 80 -U postgres -d predictiondb\n</code></pre></li> <li>Query users:    <pre><code>SELECT username, last_login FROM auth_user;\n</code></pre>    Expected output:    <pre><code>username  | last_login\n----------+-------------------------------\nadmin     | 2022-12-14 01:38:41.480801+00\nxyz-user  | 2022-12-14 01:38:53.474404+00\n</code></pre></li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#takeaways","title":"Takeaways","text":"<ul> <li>The NDB Operator simplifies VM-based database deployment in Kubernetes.</li> <li>Databases are provisioned via YAML manifests and exposed as Kubernetes Services.</li> <li>Security is managed with <code>securityContext</code> instead of SCCs, ensuring non-root execution.</li> </ul>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_install_app/#cleanup-optional","title":"Cleanup (optional)","text":"<pre><code>kubectl delete -f ingress.yaml -n ndb\nkubectl delete -f https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/react-deployment.yaml -n ndb\nkubectl delete -f https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/django-deployment.yaml -n ndb\nkubectl delete -f https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/django-job.yaml -n ndb\nkubectl delete -f app-variables.yaml -n ndb\nkubectl delete -f https://raw.githubusercontent.com/nutanix-japan/ocp-gitp/main/docs/ocp_ndb/k8s/app-secrets.yaml -n ndb\nkubectl delete -f database.yaml -n ndb\nkubectl delete -f ndbserver.yaml -n ndb\nkubectl delete -f your-db-secret.yaml -n ndb\nkubectl delete -f your-ndb-secret.yaml -n ndb\nkubectl delete pod psql -n ndb\nkubectl delete namespace ndb\nhelm uninstall ndb-operator -n ndb-operator\nkubectl delete namespace ndb-operator\n</code></pre> <p>Decommission the Postgres database in the NDB UI.</p>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/","title":"Nutanix NKP and Nutanix NDB Integration","text":"<p>This lab sets up a custom microservices-based application with a VM-based Nutanix Database Service (NDB). It demonstrates integrating NDB-provisioned databases with a vanilla Kubernetes cluster, replacing OpenShift-specific features like Routes with Ingress and Security Context Constraints (SCCs) with Kubernetes security contexts.</p> <p>NDB provides Database-as-a-Service for Microsoft SQL Server, Oracle, PostgreSQL, MongoDB, and MySQL, enabling efficient management of databases in hybrid multicloud environments. Customers often use VM-based databases due to existing expertise, ease of deployment, and robust high availability, disaster recovery, and security practices.</p> <p>Lab Duration</p> <p>Estimated time to complete this lab is 60 minutes.</p> Fun Fact <p>The NDB Operator was developed by Nutanix Japan's Solution Engineers (SE) team during a 2022 Hackathon, addressing customer needs for Kubernetes integration. The team won the Hackathon, and the NDB Operator is now available for customers, showcasing Nutanix's commitment to customer value.</p>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>NDB <code>v2.5</code> or later deployed on a Nutanix cluster </li> <li>Nutanix Kubernetes Platform NKP cluster <code>v1.15</code> or later deployed, accessible via <code>kubectl</code>. See NKP Deployment for NKP install instructions.</li> <li>Nutanix CSI driver installed for storage integration.</li> <li>Networking configured to allow communication between the Kubernetes cluster and NDB.</li> <li>Traefik Ingress controller installed for external access.</li> <li>Linux Tools VM or equivalent environment with <code>kubectl</code>, <code>helm</code>, <code>curl</code>, and <code>jq</code> installed.</li> <li>NDB server credentials and SSH key pair for database provisioning.</li> </ul> <p>Note</p> <p>Currently, only Postgres databases are supported by the NDB Operator. Support for other databases (MSSQL, MySQL, Oracle, etc.) will be added incrementally. Check Nutanix release announcements for updates. Nutanix provides 24/7/365 support for Postgres with Postgres Professional. See the solution brief for more details.</p>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#high-level-overview","title":"High-Level Overview","text":"<ol> <li>Install the NDB Operator on the Kubernetes cluster.</li> <li>Deploy a Postgres database using NDB.</li> <li>Install a custom three-layer application (React frontend, Django backend, Postgres database).</li> <li>Connect the application to the NDB-provisioned database.</li> <li>Create database schema and populate data.</li> <li>Test the application and verify data in the database.</li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#install-ndb-operator-on-kubernetes","title":"Install NDB Operator on Kubernetes","text":""},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#prepare-the-linux-tools-vm","title":"Prepare the Linux Tools VM","text":"<ol> <li>Log in to your Linux Tools VM (e.g., via SSH as <code>ubuntu</code>).</li> <li>Create a working directory:     <pre><code>mkdir -p $HOME/k8suserXX/ndb\ncd $HOME/k8suserXX/ndb\n</code></pre></li> <li> <p>Configure <code>kubectl</code> to access your NKP Kubernetes cluster:</p> CommandSample command <pre><code>export KUBECONFIG=$HOME/_nkp_install_dir/nkpclustername.conf\nkubectl cluster-info\nkubectl get nodes\n</code></pre> <pre><code>export KUBECONFIG=$HOME/nkp/nkpdev.conf\nkubectl cluster-info\nkubectl get nodes\n</code></pre> </li> <li> <p>Install the latest Cert-Manager as a prerequisite:</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.18.0/cert-manager.yaml\n</code></pre> </li> <li> <p>Verify Cert-Manager is running:</p> <pre><code>kubectl get pods -n cert-manager\n</code></pre> </li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#install-the-ndb-operator","title":"Install the NDB Operator","text":"<ol> <li>Add Nutanix\u2019s Helm repository:    <pre><code>helm repo add nutanix https://nutanix.github.io/helm/\n</code></pre></li> <li> <p>Install the NDB Operator</p> CommandOutput <pre><code>helm install ndb-operator nutanix/ndb-operator --version 0.5.3 -n ndb-operator --create-namespace\n</code></pre> <pre><code>NAME: ndb-operator\nLAST DEPLOYED: [Timestamp]\nNAMESPACE: ndb-operator\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> </li> <li> <p>Verify the NDB Operator is running:</p> CommandOutput <pre><code>kubectl get all -n ndb-operator\n</code></pre> <pre><code>NAME                                                   READY   STATUS    RESTARTS   AGE\npod/ndb-operator-controller-manager-77fcb496d5-7qcfc   2/2     Running   0          2m16s\n</code></pre> </li> <li> <p>Optionally, view operator logs:</p> <pre><code>kubectl logs -f deployment/ndb-operator-controller-manager -n ndb-operator\n</code></pre> </li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#create-ndb-postgres-database","title":"Create NDB Postgres Database","text":""},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#high-level-steps","title":"High-Level Steps","text":"<ol> <li>The NDB Operator sends a database creation request to the NDB server.</li> <li>The NDB server provisions a Postgres database VM and database.</li> <li>The NDB server returns the operation result to the NDB Operator.</li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#prepare-secrets","title":"Prepare Secrets","text":"<ol> <li> <p>Create a Kubernetes namespace:    <pre><code>kubectl create namespace ndb\n</code></pre></p> </li> <li> <p>Create a Secret for NDB server credentials:</p> <pre><code>cat &lt;&lt; EOF &gt; your-ndb-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: your-ndb-secret\n  namespace: ndb\ntype: Opaque\nstringData:\n  username: admin\n  password: _ndb_admin_password\nEOF\n</code></pre> </li> <li> <p>Edit <code>your-ndb-secret.yaml</code> with your NDB server credentials and apply:</p> <pre><code>vi your-ndb-secret.yaml\nkubectl apply -f your-ndb-secret.yaml\n</code></pre> </li> <li> <p>Create a Secret for the Postgres VM credentials, including an SSH public key:</p> Create SSH Key Pair Commands <pre><code>ssh-keygen -t rsa -b 2048 -f ~/.ssh/for_ndb\n</code></pre> </li> <li> <p>Copy the public key from <code>~/.ssh/for_ndb.pub</code> into <code>your-secret.yaml</code>.</p> <pre><code>vi your-db-secret.yaml\nkubectl apply -f your-db-secret.yaml\n</code></pre> <pre><code>cat &lt;&lt; EOF &gt; your-db-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: your-db-secret\n  namespace: ndb\ntype: Opaque\nstringData:\n  password: postgres_password\n  ssh_public_key: ssh-rsa AAAAB3NzaC1yc2E... # &lt;&lt; Paste contents of `~/.ssh/for_ndb.pub` \nEOF\n</code></pre> </li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#get-ndb-cluster-uuid","title":"Get NDB Cluster UUID","text":"<ol> <li>Set the NDB server IP:    <pre><code>NDB_IP=your_ndb_vm_ip\necho $NDB_IP\n</code></pre>    Example:    <pre><code>NDB_IP=10.42.12.18\n</code></pre></li> <li>Retrieve the NDB cluster UUID:    <pre><code>NDB_UUID=\"$(curl -X GET -u admin -k https://$NDB_IP/era/v0.9/clusters | jq '.[0].id')\"\necho $NDB_UUID\n</code></pre>    Example output:    <pre><code>\"eafdb83c-e512-46ce-8d7d-6859dc170272\"\n</code></pre>    Note the UUID for the next step.</li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#create-ndb-compute-profile","title":"Create NDB Compute Profile","text":"<ol> <li>In the NDB UI, navigate to Profiles &gt; Compute Profile.</li> <li>Create a new compute profile:</li> <li>Name: DEFAULT_OOB_SMALL_COMPUTE</li> <li>CPUs: 4</li> <li>Cores per CPU: 2</li> <li>Memory: 8GB</li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#create-postgres-database","title":"Create Postgres Database","text":"<ol> <li>Create an <code>NDBServer</code> resource:    <pre><code>cat &lt;&lt; EOF &gt; ndbserver.yaml\napiVersion: ndb.nutanix.com/v1alpha1\nkind: NDBServer\nmetadata:\n  name: ndb\n  namespace: ndb\nspec:\n  credentialSecret: your-ndb-secret\n  server: https://$NDB_IP:8443/era/v0.9\n  skipCertificateVerification: true\nEOF\n</code></pre></li> <li>Apply the resource:    <pre><code>kubectl apply -f ndbserver.yaml\n</code></pre></li> <li>Set a database server name:    <pre><code>MY_DB_SERVER_NAME=k8suserXX\necho $MY_DB_SERVER_NAME\n</code></pre></li> <li>Create a <code>Database</code> resource:    <pre><code>cat &lt;&lt; EOF &gt; database.yaml\napiVersion: ndb.nutanix.com/v1alpha1\nkind: Database\nmetadata:\n  name: dbforflower\n  namespace: ndb\nspec:\n  ndbRef: ndb\n  isClone: false\n  databaseInstance:\n    clusterId: $NDB_UUID\n    name: \"$MY_DB_SERVER_NAME\"\n    databaseNames:\n      - predictiondb\n    credentialSecret: your-db-secret\n    size: 10\n    timezone: \"UTC\"\n    type: postgres\nEOF\n</code></pre></li> <li>Apply the resource:    <pre><code>kubectl apply -f database.yaml\n</code></pre></li> <li>Monitor the database provisioning:    <pre><code>kubectl get database -n ndb\n</code></pre>    Example output:    <pre><code>NAME          IP ADDRESS   STATUS     TYPE\ndbforflower                CREATING   postgres\n</code></pre></li> <li>Check logs for progress:    <pre><code>kubectl logs -f deployment/ndb-operator-controller-manager -n ndb-operator\n</code></pre></li> <li>Optionally, monitor progress in the NDB UI under Operations. Provisioning takes about ~20 minutes.</li> </ol>"},{"location":"nkp_tutorials/nkp_ndb_lab/nkp_ndb_lab/#check-database-connectivity","title":"Check Database Connectivity","text":"<ol> <li>Verify the database status:    <pre><code>kubectl get database -n ndb\n</code></pre>    Example output:    <pre><code>NAME          IP ADDRESS    STATUS   TYPE\ndbforflower   10.38.13.45   READY    postgres\n</code></pre></li> <li>Check the Service and Endpoint:    <pre><code>kubectl get service,ep -n ndb\n</code></pre>    Example output:    <pre><code>NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\nservice/dbforflower-svc   ClusterIP   172.30.188.239   &lt;none&gt;        80/TCP    73s\nendpoints/dbforflower-svc   10.38.13.45:5432   73s\n</code></pre></li> <li>Deploy a test Postgres pod:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: psql\n  namespace: ndb\nspec:\n  restartPolicy: Never\n  containers:\n  - name: psql\n    image: postgres:15\n    command: [\"/bin/sh\", \"-c\", \"echo 'Pod is running' &amp;&amp; sleep 7200\"]\n    env:\n    - name: POSTGRES_PASSWORD\n      value: postgres_password\n    securityContext:\n      runAsUser: 1000\n      runAsGroup: 1000\n      fsGroup: 1000\nEOF\n</code></pre></li> <li>Connect to the database:    <pre><code>kubectl exec -it psql -n ndb -- psql -h dbforflower-svc -p 80 -U postgres -d predictiondb\n</code></pre>    Enter <code>postgres_password</code> when prompted. Run:    <pre><code>\\du\n</code></pre>    Example output:    <pre><code>List of roles\nRole name | Attributes | Member of\n----------+------------+-----------\npostgres  | Superuser  | {}\n...\n</code></pre></li> </ol>"},{"location":"sizing/sizing/","title":"Sizing for AI Applications on Nutanix","text":""},{"location":"sizing/sizing/#sizing-llm-infrastructures-key-considerations","title":"Sizing LLM Infrastructures: Key Considerations","text":"<p>As large language models (LLMs) continue to grow in size and complexity, ensuring adequate infrastructure is crucial for efficient deployment and performance. This section explores key considerations when sizing LLM infrastructures, including model storage, GPU requirements, networking throughput, and optimization techniques.</p>"},{"location":"sizing/sizing/#common-application-and-generative-ai-use-cases","title":"Common Application and Generative AI Use Cases","text":"<p>LLMs are versatile and can be employed in a wide range of AI tasks and applications, including:</p> <ol> <li> <p>Natural Language Processing (NLP): Language translation, text summarization, sentiment analysis, question answering, and text generation.</p> </li> <li> <p>Conversational AI: Building chatbots, virtual assistants, and dialogue systems for customer service, e-commerce, and personal assistance.</p> </li> <li> <p>Content Creation: Generating creative content such as articles, stories, scripts, and marketing copy.</p> </li> <li> <p>Code Generation: Assisting developers by generating code snippets, suggesting improvements, and explaining code functionality.</p> </li> <li> <p>Multimodal AI: Combining language models with computer vision and speech recognition for tasks like image captioning, video understanding, and audio transcription.</p> </li> <li> <p>Scientific Research: Analyzing and summarizing large volumes of scientific literature, assisting in hypothesis generation, and supporting drug discovery efforts.</p> </li> </ol>"},{"location":"sizing/sizing/#llm-model-sizes","title":"LLM Model Sizes","text":"<p>Recent LLM models like Llama2, Llama3, Mixtral-7B, and Mixtral-8x7B have pushed the boundaries of model size and complexity. These models can range from several gigabytes to hundreds of gigabytes in size, posing significant challenges for storage and computational resources.</p> <p>For a list of currently supported validated models with Nutanix GPT-in-a-Box (0.2) on Kubernetes, see https://opendocs.nutanix.com/gpt-in-a-box/kubernetes/v0.2/validated_models/</p>"},{"location":"sizing/sizing/#model-storage","title":"Model Storage","text":"<p>Storing these massive models requires substantial storage infrastructure. For example, the Llama2 model is approximately 7GB, while the Mixtral-8x7B model can reach up to 200GB or more.</p> <p>Solid-state (SSDs) or Non-Volatile Memory (NVMe) drives are highly recommended for optimal performance and low latency when accessing the underlying model storage.</p> <p>As with any workload running on Nutanix Infrastructure, sizing the underlying storage infrastructure should consider the following factors:</p> <ul> <li>Total model size</li> <li>Number of models to be stored</li> <li>Redundancy and backup requirements</li> <li>Future growth and scalability needs</li> </ul> <p>Additionally, for LLM deployments and AI applications leveraging Retrieval-Augmented Generation (RAG) pipelines should take into consideration additional storage capacity that may be required to handle scenarios that incorporate:</p> <ul> <li>Vector Databases: Deploying a vector database like Milvus to store and retrieve relevant information from unstructured data sources should be sized accordingly to the vendor's hardware recommendations when running on Kubernetes. </li> <li>Data Ingestion: Workloads for importing and preprocessing various unstructured data sources into solutions such as Nutanix Objects, such as documents, PDFs, web pages, or misc. multimedia content.</li> </ul> <p>Additionally, the choice of the underlying storage technology being leveraged for model storage plays a crucial role in performance and accessibility. </p> <p>Here are some options to consider:</p> <ol> <li>Network File System (NFS) (i.e., Nutanix Files): NFS is a traditional file sharing protocol that allows multiple systems to access the same storage over a network. It provides low latency and high throughput but may require additional infrastructure and management overhead.</li> </ol> <p>When deploying Nutanix GPT-in-a-Box, the getting started guide leverages Nutanix Files, but alternative approaches can be considered for advanced use cases.</p> <ol> <li> <p>S3 Object Storage (i.e., Nutanix Objects): Nutanix Objects is a software-defined object storage solution that offers scalable and cost-effective storage for large datasets.</p> </li> <li> <p>OCI Container Image Registry (i.e., Harbor): Any preferred OCI compliant container registry can be used to store and distribute LLM models as container images. This approach can simplify model deployment and management, especially in containerized environments.</p> </li> </ol>"},{"location":"sizing/sizing/#gpu-requirements","title":"GPU Requirements","text":"<p>LLMs are computationally intensive and benefit greatly from GPU acceleration. The choice of GPU depends on the target deployment environment, whether it's an edge device or a data center.</p>"},{"location":"sizing/sizing/#edge-deployments","title":"Edge Deployments","text":"<p>For edge deployments, smaller and more power-efficient GPUs like the NVIDIA T4, A10 or L4 series can be suitable options. These GPUs offer a balance between performance and power consumption, making them ideal for environments with infrastructure constraints.</p>"},{"location":"sizing/sizing/#data-center-deployments","title":"Data Center Deployments","text":"<p>In data center environments, high-performance GPUs like the NVIDIA L40S and H100 are better suited for handling larger LLM models. These GPUs provide massive parallelism and high memory bandwidth, enabling efficient inference and training.</p> <p>When sizing GPU infrastructure, consider the following factors:</p> <ul> <li>Model size and computational requirements</li> <li>Batch size and throughput requirements</li> <li>Scalability and future growth needs</li> <li>Power and cooling requirements</li> </ul>"},{"location":"sizing/sizing/#gpu-models-for-llm-infrastructures","title":"GPU Models for LLM Infrastructures","text":"<p>When sizing LLM infrastructures, the choice of GPU plays a crucial role in determining performance and efficiency. Here, we'll focus on two GPU models from NVIDIA: the L40S and the L4, which are well-suited for both edge deployments and data center environments.</p>"},{"location":"sizing/sizing/#nvidia-l4","title":"NVIDIA L4","text":"<p>The NVIDIA L4 (GPU Memory: 16GB) is a low-power, high-performance GPU designed for edge computing and embedded applications.</p> <p>The L40S is an excellent choice for edge deployments where power efficiency and compact form factors are essential. Its low power consumption and high performance make it suitable for tasks like natural language processing, speech recognition, and computer vision.</p>"},{"location":"sizing/sizing/#nvidia-l40s","title":"NVIDIA L40S","text":"<p>The NVIDIA L40S (GPU Memory: 48GB) is a data center-grade GPU designed for high-performance computing and AI workloads.</p> <p>With its powerful compute capabilities and ample memory, the L4 is well-suited for deploying larger LLM models in data center environments. It can handle computationally intensive tasks such as language model training, inference, and multi-modal AI applications.</p>"},{"location":"sizing/sizing/#compute-considerations","title":"Compute Considerations","text":"<p>In addition to GPU resources, the compute infrastructure for LLM deployments should also be carefully planned. Here are some considerations:</p>"},{"location":"sizing/sizing/#kubernetes-workloads","title":"Kubernetes Workloads","text":"<p>For containerized LLM deployments, Kubernetes can provide a scalable and flexible platform for managing and orchestrating workloads. When sizing Kubernetes clusters, consider factors such as the number of nodes, CPU and memory requirements, and the ability to scale up or down based on demand.</p> <p>Additionally, for LLM deployments involving Retrieval-Augmented Generation (RAG) pipelines, you may need to provision additional workloads to handle tasks such as:</p> <ul> <li>Vector Database: Deploying a vector database like Milvus to store and retrieve relevant information from unstructured data sources.</li> <li>Data Ingestion: Workloads for importing and preprocessing various unstructured data sources, such as documents, web pages, or multimedia content.</li> </ul> <p>These additional workloads can help enhance the performance and capabilities of your LLM deployment by providing access to relevant external knowledge sources.</p>"},{"location":"sizing/sizing/#virtualization-specifications","title":"Virtualization Specifications","text":"<p>If deploying LLM models on virtual machines (VMs), ensure that the virtualization platform (i.e., Nutanix AHV) can provide the necessary CPU, memory, and GPU resources. Additionally, consider the overhead introduced by virtualization and the potential impact on performance.</p>"},{"location":"sizing/sizing/#system-cpu-and-memory-considerations","title":"System CPU and Memory Considerations","text":"<p>In addition to GPUs, the CPU and system memory also play a crucial role in LLM infrastructures. Here are some considerations:</p>"},{"location":"sizing/sizing/#cpu","title":"CPU","text":"<p>While LLMs primarily rely on GPU acceleration, the CPU is still responsible for tasks like data preprocessing, input/output operations, and orchestrating the overall workflow. High-performance CPUs, such as the latest Intel Xeon or AMD EPYC processors, can ensure efficient data handling and minimize bottlenecks.</p>"},{"location":"sizing/sizing/#system-memory","title":"System Memory","text":"<p>LLMs often require large amounts of system memory to store intermediate data and facilitate efficient data transfer between the CPU and GPU. Sufficient RAM, ideally DDR4 or DDR5, should be provisioned to avoid performance degradation due to excessive paging or swapping.</p>"},{"location":"sizing/sizing/#networking-throughput","title":"Networking Throughput","text":"<p>Efficient data transfer is crucial for LLM deployments, especially in distributed or multi-node setups. High-speed networking infrastructure, such as 100Gbps Ethernet, can ensure low latency and high throughput for model transfers and inference requests.</p> <p>When sizing networking infrastructure, consider the following factors:</p> <ul> <li>Number of nodes and their interconnectivity</li> <li>Data transfer requirements (model updates, inference requests)</li> <li>Bandwidth and latency requirements</li> <li>Scalability and future growth needs</li> </ul>"},{"location":"sizing/sizing/#best-practices-for-sizing-and-optimizing-modern-llm-models","title":"Best Practices for Sizing and Optimizing Modern LLM Models","text":"<p>When sizing and optimizing modern large language models (LLMs), there are several best practices to consider:</p> <ol> <li> <p>Rightsizing Resources: Accurately estimate the resource requirements for your LLM deployment based on factors such as model size, batch size, and throughput requirements. Overprovisioning resources can lead to unnecessary costs, while underprovisioning can result in performance bottlenecks.</p> </li> <li> <p>Scalability and Elasticity: Design your infrastructure to be scalable and elastic, allowing you to easily scale resources up or down based on demand. This can be achieved through containerization, orchestration tools like Kubernetes, and cloud-native architectures.</p> </li> <li> <p>Distributed Training and Inference: For very large models that exceed the memory capacity of a single GPU, consider distributed training and inference techniques. This involves partitioning the model across multiple GPUs or nodes, enabling efficient processing of larger models.</p> </li> <li> <p>Model Quantization: Quantize your LLM models to lower precision (e.g., 16-bit or 8-bit) to reduce memory footprint and increase throughput, while maintaining acceptable accuracy levels. Tools like NVIDIA TensorRT and AMD ROCm can assist with quantization and optimization.</p> </li> <li> <p>Optimized Data Pipelines: Ensure efficient data ingestion, preprocessing, and transfer pipelines to minimize bottlenecks and latency. Leverage techniques like caching, prefetching, and parallel data processing.</p> </li> <li> <p>Hybrid Cloud Architectures: Consider a hybrid cloud approach, where you can leverage both on-premises and cloud resources for optimal performance and cost-efficiency. This can involve running inference on-premises, such as an edge deployment, while offloading training to a datacenter or the cloud (i.e., Nutanix NC2).</p> </li> <li> <p>Monitoring and Optimization: Continuously monitor your LLM infrastructure for performance bottlenecks, resource utilization, and cost optimization opportunities. Leverage monitoring tools and performance profiling to identify areas for improvement.</p> </li> <li> <p>Automated Scaling and Optimization: Implement automated scaling and optimization mechanisms to dynamically adjust resources based on workload demands. This can involve auto-scaling groups, load balancing, and automated model optimization techniques.</p> </li> <li> <p>Containerization and Reproducibility: Package your LLM models and dependencies into containers for consistent and reproducible deployments across different environments. This can simplify management, updates, and portability.</p> </li> <li> <p>Collaboration and Knowledge Sharing: Foster collaboration and knowledge sharing within your organization and the broader LLM community. Stay up-to-date with the latest best practices, tools, and techniques for optimizing LLM deployments.</p> </li> </ol> <p>By carefully considering these factors and employing appropriate optimization techniques, organizations can effectively size and optimize their LLM infrastructures, ensuring efficient performance, scalability, and cost-effectiveness while meeting the demanding requirements of modern LLM applications.</p>"},{"location":"static/","title":"Introduction","text":"<p>The simulator section of the site is to help visualise kubernetes node and pod deployment.</p> <p>Warning</p> <p>This is just a static HTML page with javascript. There is no backend logic to persist simulations.</p> <p>Remember to take screen shots are download them to your personal device for persistence.</p> <p>We are covering four scenarios in this section. We will add more as it becomes available. </p>"},{"location":"static/#simulators","title":"Simulators","text":"<ul> <li> <p> Kubernetes Pods and Nodepools</p> <p>Simulate and visualise how pods and nodepools are deployed with your own criteria.</p> <p> Simulation</p> </li> <li> <p> Kubernetes Deployment Strategies</p> <p>Visualise various Kubernetes Deployment Strategies</p> <ul> <li>Rolling updates</li> <li>Recreate</li> <li>Blue/Green</li> </ul> <p> Simulation</p> </li> </ul> <ul> <li> <p> LLM Model Selector</p> <p>This Simulation helps you visualize and choose LLMs for your use case. </p> <p> Simulation</p> </li> </ul> <ul> <li> <p> LLM Scheduling on Kubernetes Clusters</p> <p>This is a visual simulator that demonstrates how Kubernetes  schedules Large Language Model (LLM) workloads across different types of compute nodes. It helps users understand how resource management, scheduling decisions, and pod placement works in a Kubernetes environment, specifically for AI/ML workloads that often have specialized resource requirements like GPUs.</p> <p> Simulation</p> <p>Interactive Model Configuration </p> <p>The ability to select different LLM models with varying resource requirements, configure deployment parameters &gt; like inference engines, and see how these affect resource needs.</p> <p>Visual Node Pool Management</p> <p>The feature to create and customize different node pools with various hardware specifications  (CPU, memory, GPU types/counts), simulating real-world Kubernetes cluster architecture.</p> <p>Realistic Scheduling Simulation</p> <p>Shows the actual scheduling process, demonstrating how Kubernetes evaluates node resources, makes placement  decisions, and handles resource constraints.</p> <p>GPU Compatibility &amp; Resource Constraints </p> <p>Demonstrates how specific models might require particular GPU types or hardware configurations, and how resource constraints affect deployment.</p> <p>Test Scenarios  Predefined test configurations that showcase different scheduling scenarios, from CPU-only deployments to mixed workloads and resource constraints.</p> </li> </ul>"},{"location":"supportgpt/supportgpt/","title":"Under constuction","text":"Template fileExample file .env.sample.yaml<pre><code>k8s_cluster:\n\n  ## kubernetes distribution - supported \"nke\" \"kind\"\n  distribution: nke\n  ## kubernetes cluster name\n  name: _required\n  ## cluster_profile_type - anything under clusters/_profiles (e.g., llm-management, llm-workloads, etc.)\n  profile: _required\n  ## environment name - based on profile selected under clusters/_profiles/&lt;profile&gt;/&lt;environment&gt; (e.g., prod, non-prod, etc.)\n  environment: _required\n\n  ## docker hub registry configs\n  registry:\n    docker_hub:\n      user: _required\n      password: _required\n\n  ## nvidia gpu specific configs\n  gpu_operator:\n    enabled: false\n    version: v23.9.0\n    cuda_toolkit_version: v1.14.3-centos7\n    ## time slicing typically only configured on dev scenarios. \n    ## ideal for jupyter notebooks\n    time_slicing:\n      enabled: false\n      replica_count: 2\n\nflux:\n  ## flux specific configs for github repo\n  github:\n    repo_url: _required\n    repo_user: _required\n    repo_api_token: _required\n\ninfra:\n  ## Global nutanix configs\n  nutanix:\n    ## Nutanix Prism Creds, required to download NKE creds\n    prism_central:\n      enabled: false\n      # endpoint: _required_if_enabled\n      # user: _required_if_enabled\n      # password: _required_if_enabled\n\n    ## Nutanix Objects Store Configs\n    objects:\n      enabled: false\n      # host: _required_if_enabled\n      # port: _required_if_enabled\n      # region: _required_if_enabled\n      # use_ssl: _required_if_enabled\n      # access_key: _required_if_enabled\n      # secret_key: _required_if_enabled\n\nservices:\n  #####################################################\n  ## Required variables for kube-vip and depedent services\n  ## kube-vip specific configs required for any services needing to be configured with LoadBalancer Virtual IP Addresses\n  kube_vip:\n    enabled: false\n    ## Used to configure default global IPAM pool. A minimum of 2 ips should be provide in a range\n    ## For Example: ipam_range: 172.20.0.22-172.20.0.23\n    #ipam_range: _required_if_enabled\n\n  ## required for all platform services that are leveraging nginx-ingress\n  nginx_ingress:\n    enabled: false\n    version: 4.8.3\n    ## Virtual IP Address (VIP) dedicated for nginx-ingress controller. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## Example: vip: 172.20.0.20\n    #vip: _required_if_enabled\n\n    ## NGINX Wildcard Ingress Subdomain used for all default ingress objects created within cluster \n    ## For DEMO purposes, it is common to prefix subdomain with cluster-name as each cluster would require dedicated wildcard domain.\n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.example.com, then value is example.com\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: wildcard_ingress_subdomain:flux-kind-local.172.20.0.20.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n    ## Wildcard Ingress Subdomain for management cluster.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates\n    #management_cluster_ingress_subdomain: _required_if_enabled\n\n  istio:\n    enabled: false\n    version: 1.17.2\n    ## Virtual IP Address (VIP) dedicated for istio ingress gateway. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## This address should be mapped to wildcard_ingress_subdomain defined below. For Example: vip: 172.20.0.21\n    #vip: _required_if_enabled\n\n    ## Istio Ingress Gateway - Wildcard Subdomain used for all knative/kserve llm inference endpoints. \n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.llm.example.com, then value is llm.example.com\n    ## If leveraging AWS Route 53 DNS with Let's Encrypt (below), make sure to enable/configure AWS credentials needed to \n    ## support CertificateSigningRequests using ACME DNS Challenges.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: llm.flux-kind-local.172.20.0.21.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n  cert_manager:\n    ## if enabled - cluster issuer will be self-signed-issuer\n    enabled: false\n    version: v1.13.5\n    ## if aws_route53_acme_dns.enabled - the cluster issuer across all services will be set to \"letsencrypt-issuer\"\n    ## Following AWS Route53 Access Creds required for Lets Encrypt ACME DNS Challenge\n    ## For additional details, https://cert-manager.io/docs/configuration/acme/dns01/route53/\n    ## minimum supported cert-manager version is v1.9.1 https://cert-manager.io/docs/releases/release-notes/release-notes-1.9/#v191\n    aws_route53_acme_dns:\n      enabled: false\n      # email: _required_if_enabled\n      # zone: _required_if_enabled\n      # hosted_zone_id: _required_if_enabled\n      # region: _required_if_enabled\n      # key_id: _required_if_enabled\n      # key_secret: _required_if_enabled\n\n  ## do not disable kyverno unless you know what you're doing\n  ## this is needed to keep docker hub creds synchronized between namespaces.\n  kyverno:\n    enabled: true\n    version: 3.1.4\n\n  ## the following versions and dependencies kserve are aligned with GPT In A Box Opendocs\n  ## the only exception is with cert-manager due to usage of aws route 53\n  ## https://opendocs.nutanix.com/gpt-in-a-box/kubernetes/v0.2/getting_started/\n\n  kserve:\n    enabled: false\n    version: v0.11.2\n\n  knative_serving:\n    enabled: false\n    version: knative-v1.10.1\n\n  knative_istio:\n    enabled: false\n    version: knative-v1.10.0\n\n  ## The following components are leveraged to support Nutanix Validated Designs\n  ## The NVD for GPT in a Box leverages a RAG Pipeline with Serverless Functions \n  ## to demonstrate end to end workflow with Nutanix Integration\n\n  ## Milvus is vector database \n  milvus:\n    enabled: false\n    version: 4.1.13\n    milvus_bucket_name: milvus\n\n  ## Knative Eventing used to receive Event notifications from Nutanix Objects Document Bucket\n  knative_eventing:\n    enabled: false\n    version: knative-v1.10.1\n\n  ## Kafka is messaging broker used by both knative eventing Document Ingestion serverless function\n  ## and integrates with Nutanix Objects Events Notification Kafka Endpoints\n  ## Kafka is also leveraged by Milvus as a Messaging Broker for Milvus related events, as opposed to the default Apache Pulsar\n  kafka:\n    enabled: false\n    version: 26.8.5\n\n  ## OpenTelemetry Collector version is used for both the Deployment and Daemon is used to collect data for monitoring\n  opentelemetry_collector:\n    enabled: false\n    version: 0.80.1\n\n  ## OpenTelemetry Operator is used to deploy opentelemetry components\n  opentelemetry_operator:\n    enabled: false\n    version: 0.47.0\n\n  ## Uptrace is Observability / Monitoring UI\n  uptrace:\n    enabled: false\n    version: 1.5.7\n\n  ## Jupyterhub is deployed on non-prod workload clusters in NVD Reference\n  jupyterhub:\n    enabled: false\n    version: 3.1.0\n\n  redis:\n    enabled: false\n    version: 18.1.6\n\n  elasticsearch:\n    enabled: false\n    version: 19.13.10\n\n  kubernetes_dashboard:\n    enabled: false\n    version: 7.3.2\n\n  weave_gitops:\n    enabled: true\n    version: 4.0.36\n\napps:\n  ## Required GPT NVD Reference Application Helm Chart Configs\n  gptnvd_reference_app:\n    enabled: false\n    version: 0.2.7\n    #documents_bucket_name: documents01\n  ## Required NAI LLM Helm Chart Configs\n  ### huggingFaceToken required when useExistingNFS. This will download model when llm is initialized\n  nai_helm:\n    enabled: false\n    version: 0.1.1\n    #model: llama2_7b_chat\n    #revision: 94b07a6e30c3292b8265ed32ffdeccfdadf434a8\n    #maxTokens: 4000\n    #repPenalty: 1.2\n    #temperature: 0.2\n    #topP: 0.9\n    #useExistingNFS: false\n    #nfs_export: /llm-model-store\n    #nfs_server: _required\n    #huggingFaceToken: _required\n</code></pre> .env.mgmt-cluster.yaml<pre><code>k8s_cluster:\n\n  ## kubernetes distribution - supported \"nke\" \"kind\"\n  distribution: nke\n  ## kubernetes cluster name\n  name: _required\n  ## cluster_profile_type - anything under clusters/_profiles (e.g., llm-management, llm-workloads, etc.)\n  profile: _required\n  ## environment name - based on profile selected under clusters/_profiles/&lt;profile&gt;/&lt;environment&gt; (e.g., prod, non-prod, etc.)\n  environment: _required\n\n  ## docker hub registry configs\n  registry:\n    docker_hub:\n      user: _required\n      password: _required\n\n  ## nvidia gpu specific configs\n  gpu_operator:\n    enabled: false\n    version: v23.9.0\n    cuda_toolkit_version: v1.14.3-centos7\n    ## time slicing typically only configured on dev scenarios. \n    ## ideal for jupyter notebooks\n    time_slicing:\n      enabled: false\n      replica_count: 2\n\nflux:\n  ## flux specific configs for github repo\n  github:\n    repo_url: _required\n    repo_user: _required\n    repo_api_token: _required\n\ninfra:\n  ## Global nutanix configs\n  nutanix:\n    ## Nutanix Prism Creds, required to download NKE creds\n    prism_central:\n      enabled: false\n      # endpoint: _required_if_enabled\n      # user: _required_if_enabled\n      # password: _required_if_enabled\n\n    ## Nutanix Objects Store Configs\n    objects:\n      enabled: false\n      # host: _required_if_enabled\n      # port: _required_if_enabled\n      # region: _required_if_enabled\n      # use_ssl: _required_if_enabled\n      # access_key: _required_if_enabled\n      # secret_key: _required_if_enabled\n\nservices:\n  #####################################################\n  ## Required variables for kube-vip and depedent services\n  ## kube-vip specific configs required for any services needing to be configured with LoadBalancer Virtual IP Addresses\n  kube_vip:\n    enabled: false\n    ## Used to configure default global IPAM pool. A minimum of 2 ips should be provide in a range\n    ## For Example: ipam_range: 172.20.0.22-172.20.0.23\n    #ipam_range: _required_if_enabled\n\n  ## required for all platform services that are leveraging nginx-ingress\n  nginx_ingress:\n    enabled: false\n    version: 4.8.3\n    ## Virtual IP Address (VIP) dedicated for nginx-ingress controller. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## Example: vip: 172.20.0.20\n    #vip: _required_if_enabled\n\n    ## NGINX Wildcard Ingress Subdomain used for all default ingress objects created within cluster \n    ## For DEMO purposes, it is common to prefix subdomain with cluster-name as each cluster would require dedicated wildcard domain.\n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.example.com, then value is example.com\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: wildcard_ingress_subdomain:flux-kind-local.172.20.0.20.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n    ## Wildcard Ingress Subdomain for management cluster.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates\n    #management_cluster_ingress_subdomain: _required_if_enabled\n\n  istio:\n    enabled: false\n    version: 1.17.2\n    ## Virtual IP Address (VIP) dedicated for istio ingress gateway. \n    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer\n    ## This address should be mapped to wildcard_ingress_subdomain defined below. For Example: vip: 172.20.0.21\n    #vip: _required_if_enabled\n\n    ## Istio Ingress Gateway - Wildcard Subdomain used for all knative/kserve llm inference endpoints. \n    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.llm.example.com, then value is llm.example.com\n    ## If leveraging AWS Route 53 DNS with Let's Encrypt (below), make sure to enable/configure AWS credentials needed to \n    ## support CertificateSigningRequests using ACME DNS Challenges.\n    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. \n    ## For Example: llm.flux-kind-local.172.20.0.21.nip.io\n    #wildcard_ingress_subdomain: _required_if_enabled\n\n  cert_manager:\n    ## if enabled - cluster issuer will be self-signed-issuer\n    enabled: false\n    version: v1.13.5\n    ## if aws_route53_acme_dns.enabled - the cluster issuer across all services will be set to \"letsencrypt-issuer\"\n    ## Following AWS Route53 Access Creds required for Lets Encrypt ACME DNS Challenge\n    ## For additional details, https://cert-manager.io/docs/configuration/acme/dns01/route53/\n    ## minimum supported cert-manager version is v1.9.1 https://cert-manager.io/docs/releases/release-notes/release-notes-1.9/#v191\n    aws_route53_acme_dns:\n      enabled: false\n      # email: _required_if_enabled\n      # zone: _required_if_enabled\n      # hosted_zone_id: _required_if_enabled\n      # region: _required_if_enabled\n      # key_id: _required_if_enabled\n      # key_secret: _required_if_enabled\n\n  ## do not disable kyverno unless you know what you're doing\n  ## this is needed to keep docker hub creds synchronized between namespaces.\n  kyverno:\n    enabled: true\n    version: 3.1.4\n\n  ## the following versions and dependencies kserve are aligned with GPT In A Box Opendocs\n  ## the only exception is with cert-manager due to usage of aws route 53\n  ## https://opendocs.nutanix.com/gpt-in-a-box/kubernetes/v0.2/getting_started/\n\n  kserve:\n    enabled: false\n    version: v0.11.2\n\n  knative_serving:\n    enabled: false\n    version: knative-v1.10.1\n\n  knative_istio:\n    enabled: false\n    version: knative-v1.10.0\n\n  ## The following components are leveraged to support Nutanix Validated Designs\n  ## The NVD for GPT in a Box leverages a RAG Pipeline with Serverless Functions \n  ## to demonstrate end to end workflow with Nutanix Integration\n\n  ## Milvus is vector database \n  milvus:\n    enabled: false\n    version: 4.1.13\n    milvus_bucket_name: milvus\n\n  ## Knative Eventing used to receive Event notifications from Nutanix Objects Document Bucket\n  knative_eventing:\n    enabled: false\n    version: knative-v1.10.1\n\n  ## Kafka is messaging broker used by both knative eventing Document Ingestion serverless function\n  ## and integrates with Nutanix Objects Events Notification Kafka Endpoints\n  ## Kafka is also leveraged by Milvus as a Messaging Broker for Milvus related events, as opposed to the default Apache Pulsar\n  kafka:\n    enabled: false\n    version: 26.8.5\n\n  ## OpenTelemetry Collector version is used for both the Deployment and Daemon is used to collect data for monitoring\n  opentelemetry_collector:\n    enabled: false\n    version: 0.80.1\n\n  ## OpenTelemetry Operator is used to deploy opentelemetry components\n  opentelemetry_operator:\n    enabled: false\n    version: 0.47.0\n\n  ## Uptrace is Observability / Monitoring UI\n  uptrace:\n    enabled: false\n    version: 1.5.7\n\n  ## Jupyterhub is deployed on non-prod workload clusters in NVD Reference\n  jupyterhub:\n    enabled: false\n    version: 3.1.0\n\n  redis:\n    enabled: false\n    version: 18.1.6\n\n  elasticsearch:\n    enabled: false\n    version: 19.13.10\n\n  kubernetes_dashboard:\n    enabled: false\n    version: 7.3.2\n\n  weave_gitops:\n    enabled: true\n    version: 4.0.36\n\napps:\n  ## Required GPT NVD Reference Application Helm Chart Configs\n  gptnvd_reference_app:\n    enabled: false\n    version: 0.2.7\n    #documents_bucket_name: documents01\n  ## Required NAI LLM Helm Chart Configs\n  ### huggingFaceToken required when useExistingNFS. This will download model when llm is initialized\n  nai_helm:\n    enabled: false\n    version: 0.1.1\n    #model: llama2_7b_chat\n    #revision: 94b07a6e30c3292b8265ed32ffdeccfdadf434a8\n    #maxTokens: 4000\n    #repPenalty: 1.2\n    #temperature: 0.2\n    #topP: 0.9\n    #useExistingNFS: false\n    #nfs_export: /llm-model-store\n    #nfs_server: _required\n    #huggingFaceToken: _required\n</code></pre>"},{"location":"tools/tools/","title":"Tools Used","text":""},{"location":"tools/tools/#go-tasks","title":"Go Tasks","text":"<p>Go Tasks is used as an easy alternative instead of <code>make</code>. </p>"},{"location":"tools/tools/#opentofu","title":"Opentofu","text":"<p>Opentofu framework will be used to automate and lifecycle manage the Nutanix infrastructure components.</p>"}]}